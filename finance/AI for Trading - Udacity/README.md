# AI for Trading - Udacity

Course 1: [[1_Basic Quantitative Trading/README]]

Course 2: [[2_Advanced Quantitative Trading/README]]

Course 3: [[3_Stocks, Indices & ETFs/README]]

Course 4: [[4_Factor Investing & Alpha Research/README]]



715 [[Introduction to Kalman Filters]]

[[Linear Algebra]]

[[Introduction to Neural Networks]]

[[Indicators]]

extras

[[Reinforcement Learning]]

## Part 02-Module 01-Lesson 02_Intro to Natural Language Processing

### 02. Structured Languages-NsmqUIHlk6U.en

What makes it so hard for computers to understand us? One drawback of human languages, or feature depending on how you look at it, is the lack of a precisely defined structure. To understand how that makes things difficult let's first take a look at some languages that are more structured. Mathematics, for instance, uses a structured language. When I write y equals 2x plus 5 there is no ambiguity in what I want to convey. I'm saying that the variable y is related to the variable x as two times x plus five. Formal logic also uses a structure language. For example, consider the expression parent(x,y) and parent(x,z) implies sibling(y, z). This statement is asserting that if x is a parent of y and x is a parent of z, then y and z are siblings. A set of structure languages that may be more familiar to you are scripting and programming languages. Consider this SQL statement. SELECT name, email FROM users WHERE name LIKE A%. We are asking the database to return the names and e-mail addresses of all users whose names begin with an A. These languages are designed to be as unambiguous as possible and are suitable for computers to process.

### 03. Grammar-Jw3dA7xmoQ4.en

Structured languages are easy to parse and understand for computers because they are defined by a strict set of rules or grammar. There are standard forms of expressing such grammars and algorithms, that can parse properly formed statements to understand exactly what is meant. When a statement doesn't match the prescribed grammar, a typical computer doesn't try to guess the meaning, it simply gives up. Such violations of grammatical rules are reported as syntax errors.

### 04. Unstructured Text-OmwSdaec5vU.en

The languages we use to communicate with each other also have defined grammatical rules. And indeed, in some situations we use simple structured sentences but for the most part human discourse is complex and unstructured. Despite that, we seem to be really good at understanding each other and even ambiguities are welcome to a certain extent. So, what can computers do to make sense of unstructured text? Here are some preliminary ideas. Computers can do some level of processing with words and phrases, trying to identify key words, parts of speech, named entities, dates, quantities, etc. Using this information they can also try to parse sentences, at least ones that are relatively more structured. This can help extract the relevant parts of statements, questions, or instructions. At a higher level computers can analyze documents to find frequent and rare words, assess the overall tone or sentiment being expressed, and even cluster or group similar documents together. You can imagine that building on top of these ideas, computers can do a whole lot with unstructured text even if they cannot understand it like us.

### 06. Context-J-4pfu2w1C0.en

So what is stopping computers from becoming as capable as humans in understanding natural language? Part of the problem lies in the variability and complexity of our sentences. Consider this excerpt from a movie review. "I was lured to see this on the promise of a smart witty slice of old fashioned fun and intrigue. I was conned. " Although it starts with some potentially positive words it turns out to be a strongly negative review. Sentences like this might be somewhat entertaining for us but computers tend to make mistakes when trying to analyze them. But there is a bigger challenge that makes NLP harder than you think. Take a look at this sentence. "The sofa didn't fit through the door because it was too narrow." What does "it" refer to? Clearly "it" refers to the door. Now consider a slight variation of this sentence. "The sofa didn't fit through the door because it was too wide." What does "it" refer to in this case? Here it's the sofa. Think about it. To understand the proper meaning or semantics of the sentence you implicitly applied your knowledge about the physical world, that wide things don't fit through narrow things. You may have experienced a similar situation before. You can imagine that there are countless other scenarios in which some knowledge or context is indispensable for correctly understanding what is being said.

### 07. Natural Language Processing-UQBxJzoCp-I.en

Natural language processing is one of the fastest growing fields in the world. NLP Is making its way into a number of products and services that we use every day. Let's begin with an overview of how to design an end-to-end NLP pipeline. Not that kind of pipeline; a natural language processing pipeline, where you start with raw text, in whatever form it is available, process it, extract relevant features, and build models to accomplish various NLP tasks. Now that I think about it, that is kind of like refining crude oil. Anyways, you'll learn how these different stages in the pipeline depend on each other. You'll also learn how to make design decisions, how to choose existing libraries, and tools to perform each step.

### 08. NLP M1-L1 01 NLP Pipeline-vJx6oKlu_MM.en

Let's look at a common NLP pipeline. It consists of three stages, text processing, feature extraction and modeling. Each stage transforms text in some way and produces a result that the next stage needs. For example, the goal of text processing is to take raw input text, clean it, normalize it, and convert it into a form that is suitable for feature extraction. Similarly, the next stage needs to extract and produce feature representations that are appropriate for that type of model you're planning to use and the NLP task you're trying to accomplish. When you're building such a pipeline, your workflow may not be perfectly linear. Let's say, you spend some time implementing text processing functions, then make some simple feature extractors, and then design a baseline statistical model. But then, maybe you are not happy with the results. So you go back and rethink what features you need, and that in turn, can make you change your processing routines. Keep in mind that this is a very simplified view of natural language processing. Your application may require additional steps.

### 09. Text Processing-pqheVyctkNQ.en

Let's take a closer look at text processing. The first question that comes to mind is, why do we need to process text? Why can we not feed it in directly? To understand that, think about where we get this text to begin with. Websites are a common source of textual information. Here's a portion of a sample web page from Wikipedia and the corresponding HTML markup, which serves as our raw input. For the purpose of natural language processing, you would typically want to get rid of all or most of the HTML tags, and retain only plain text. You can also remove or set aside any URLs or other items not relevant to your task. The Web is probably the most common and fastest growing source of textual content. But you may also need to consume PDFs, Word documents or other file formats. Or your raw input may even come from a speech recognition system or from a book scan using OCR. Some knowledge of the source medium can help you properly handle the input. In the end, your goal is to extract plain text that is free of any source specific markers or constructs that are not relevant to your task. Once you have obtained plain text, some further processing may be necessary. For instance, capitalization doesn't usually change the meaning of a word. We can convert all the words to the same case so that they're not treated differently. Punctuation marks that we use to indicate pauses, etc. can also be removed. Some common words in a language often help provide structure, but don't add much meaning. For example, a, and, the, of, are, and so on. Sometimes it's best to remove them if that helps reduce the complexity of the procedures you want to apply later.

### 10. Feature Extraction-UgENzCmfFWE.en

Okay. We now have clean normalized text. Can we feed this into a statistical or a machine learning model? Not quite. Let's see why. Text data is represented on modern computers using an encoding such as ASCII or Unicode that maps every character to a number. Computer store and transmit these values as binary, zeros and ones. These numbers also have an implicit ordering. 65 is less than 66 which is less than 67. But does that mean A is less than B, and B is less and C? No. In fact, that would be an incorrect assumption to make and might mislead our natural language processing algorithms. Moreover, individual characters don't carry much meaning at all. It is words that we should be concerned with, but computers don't have a standard representation for words. Yes, internally they are just sequences of ASCII or Unicode values but they don't quite capture the meanings or relationships between words. Compare this with how an image is represented in computer memory. Each pixel value contains the relative intensity of light at that spot in the image. For a color image, we keep one value per primary color; red, green, and blue. These values carry relevant information. Two pixels with similar values are perceptually similar. Therefore, it makes sense to directly use pixel values in a numerical model. Yes, some feature engineering may be necessary such as edge detection or filtering, but pixels are a good starting point. So the question is, how do we come up with a similar representation for text data that we can use as features for modeling? The answer again depends on what kind of model you're using and what task you're trying to accomplish. If you want to use a graph based model to extract insights, you may want to represent your words as symbolic nodes with relationships between them like WordNet. For statistical models however, you need some sort of numerical representation. Even then, you have to think about the end goal. If you're trying to perform a document level task, such as spam detection or sentiment analysis, you may want to use a per document representations such as bag-of-words or doc2vec. If you want to work with individual words and phrases such as for text generation or machine translation, you'll need a word level representation such as word2vec or glove. There are many ways of representing textual information, and it is only through practice that you can learn what you need for each problem.

### 11. Modeling-P4w_2rkxBvE.en

The final stage in this process is what I like to call modeling. This includes designing a model, usually a statistical or a machine learning model, fitting its parameters to training data using an optimization procedure, and then using it to make predictions about unseen data. The nice thing about working with numerical features is that it allows you to utilize pretty much any machine learning model. This includes support vector machines, decision trees, neural networks, or any custom model of your choice. You could even combine multiple models to get better performance. How you utilize the model is up to you. You can deploy it as a web-based application, package it up into a handy mobile app, integrate it with other products, services, and so on. The possibilities are endless.

## Part 02-Module 01-Lesson 03_Text Processing

### 01. Text Processing-6LO6I5M18PQ.en

In this lesson, you'll learn how to read text data from different sources and prepare it for feature extraction. You'll begin by cleaning it to remove irrelevant items, such as HTML tags. You will then normalize text by converting it into all lowercase, removing punctuations and extra spaces. Next, you will split the text into words or tokens and remove words that are too common, also known as stop words. Finally, you will learn how to identify different parts of speech, named entities, and convert words into canonical forms using stemming and lemmatization. After going through all these processing steps, your text may look very different, but it captures the essence of what was being conveyed in a form that is easier to work with.

### 03. Capturing Text Data-Z4mnMN1ApG4.en

The processing stage begins with reading text data. Depending on your application, that can be from one of several sources. The simplest source is a plain text file on your local machine. We can read it in using Python's built in file input mechanism. Text data may also be included as part of a larger database or table. Here, we have a CSV file containing information about some news articles. We can read this in using pandas very easily. Pandas includes several useful string manipulation methods that can be applied to an entire column at once. For instance, converting all values to lowercase. Sometimes, you may have to fetch data from an online resource, such as a web service or API. In this example, we use the requests library in Python to obtain a quote of the day from a simple API, but you could also obtain tweets, reviews, comments, whatever you would like to analyze. Most APIs return JSON or XML data, so you need to be aware of the structure in order to pull out the fields that you need. Many data sets you will encounter have likely been fetched and prepared by someone else using a similar procedure.

### 04. Cleaning-qawXp9DPV6I.en

Text data, especially from online sources, is almost never clean. Let's look at the Udacity course catalog as an example. Say you want to extract the title and description of each course or Nanodegree. Sounds simple, right? Let's jump into Python and give it a shot. You can follow along by downloading and launching the text processing notebook. We can fetch the web page like any other online resource using the requests library. It looks like we got back the entire HTML source. This is what the browser needs to render the web page. But most of this is useless for our purposes. We need a way to extract all the plain text as visible on the website. How about using regular expressions? Let's define a pattern to match all HTML tags and remove them by replacing with a blank string. Okay, that did something. We can see that the page title has been extracted successfully, but there is a lot of JavaScript and a number of other items that we don't need. In fact, this regular expression somehow didn't match some tags. Maybe they were nested inside other tags. Maybe we need to account for tags spread across lines. Anyway, this doesn't seem like the best approach for this job. What we really need is a way to parse the HTML, just like a web browser, and pull out the relevant elements. Introducing BeautifulSoup. It's a nice Python library meant to do exactly that. You just pass in the raw web page text which in this case contains HTML to create a soup object, and then you can extract the plain text, leaving behind any HTML tags using a symbol called to the get-text method. This takes care of nested tags, tags that are broken across lines, and a multitude of other edge cases that make HTML parsing a pain. It also forgives some small errors in HTML just like browsers, making it more robust. Let's see. That's better. I don't see any HTML tags, but there are still a bunch of JavaScript and a lot of spaces. What else can we do? Let's take a look at how the HTML source is structured. The easiest way to do this is to right click on an element of your choice, here, this course title, and choose Inspect or View Page Source. Now look at where the title is placed and what is the most distinct way of finding it in the HTML documentary. Here, we have a parent div with a class of course summary card. That sounds promising. Let's use it. BeautifulSoup is actually very powerful. It enables you to walk the tree or dorm in many different ways. Here we are asking the library to find all divs with a class of course summary card. The result returned is a list of all such divs in the document. Let's store this in a variable and look at one of the divs. Okay. Scrolling through this, I see that the title is stored in this a-tag which is contained in this H3 tag. How do we extract this title? One way to get to it is using a CSS selector. And now we can fetch the plain text content just like we did before. Great. One last thing. Let's strip out the extra whitespace from both ends. There you go. Now, let's look back at the HTML to see how we can grab the description text. There it is. It's a div with an attribute called data-course-short-summary, but no value or any other attribute. Again, there is a way to select such tags using CSS. Specify the tag name, here, div, followed by the attribute name in square brackets. Looks good. Let's extract the text and clean it up. All right. We can now repeat this over all course summaries. To do this, we can use a simple for loop. Looks spot on to me. Let's store this data so we can use it later. Here, we are simply keeping the data in a list called courses. What we did just now is called scraping a web page. Although it sounds a little violent, trust me, it's not. In fact, scraping is very common. Google News is a prime example. It pulls out the title and first sentence or two from news articles and displays them. Google probably uses a combination of rules and machine learning to identify what portion of the HTML contains the title and the beginning of the article text that it can use as a preview. It works great most of the time, but sometimes, it does fail. Here, for this article on quantum entanglement, the preview doesn't seem to match the title at all. It looks more like a caption for this image. What likely happened is that the caption was the first piece of text on the web page and Google's algorithm picked that up as if it was part of the main article. This just goes to show how seemingly routine tasks and text processing are still not solved all the way. Okay, let's look back at what we just achieved. We started by fetching a single web page, the Udacity course catalog. Then we tried a couple of methods to remove HTML tags. We finally settled on using BeautifulSoup to parse the entire HTML source, find all course summaries, and extract the title and description for each course. And then we saved them all in a list. Depending on what you're planning to do next, you may continue to treat these chunks as part of a single document or consider each to be a separate document. The latter is useful, for instance, if you want to group related courses. The problem then reduces to document clustering.

### 05. Normalization-eOV2UUY8vtM.en

Plain text is great but it's still human language with all its variations and bells and whistles. Next, we'll try to reduce some of that complexity. In the English language, the starting letter of the first word in any sentence is usually capitalized. All caps are sometimes used for emphasis and for stylistic reasons. While this is convenient for a human reader from the standpoint of a machine learning algorithm, it does not make sense to differentiate between Car, car, and CAR, they all mean the same thing. Therefore, we usually convert every letter in our text to a common case, usually lowercase, so that each word is represented by a unique token. Here's some sample text, a review for the movie, The Second Renaissance, a story about intelligent robots that get into a fight with humans over their rights. Yup, the way we treat robots these days. Anyway, if we have the reviews stored in a variable called text, converting it to lowercase is a simple call to the lore method in Python. Here's what it looks like after a conversion. Note all the letters that were changed. Other languages may or may not have a case equivalent but similar principles may apply depending on your NLP task, you may want to remove special characters like periods, question marks, and exclamation points from the text and only keep letters of the alphabet and maybe numbers. This is especially useful when we are looking at text documents as a whole in applications like document classification and clustering where the low level details do not matter a lot. Here, we can use a regular expression that matches everything that is not a lowercase A to Z, uppercase A is Z, or digits zero to nine, and replaces them with a space. This approach avoids having to specify all punctuation characters, but you can use other regular expressions as well. Lowercase conversion and punctuation removal are the two most common text normalization steps. Whether you need to apply them and at what stage depends on your end goal and the way you design your pipeline.

### 06. Tokenization-4Ieotbeh4u8.en

Token is a fancy term for a symbol. Usually, one that holds some meaning and is not typically split up any further. In case of natural language processing, our tokens are usually individual words. So tokenization is simply splitting each sentence into a sequence of words. The simplest way to do this is using the split method which returns a list of words. Note that it splits on whitespace characters by default, which includes regular spaces but also tabs, new lines, et cetera. It's also smart about ignoring two or more whitespace characters in a sequence, so it doesn't return blank strings. But you can control all this using optional parameters. So far, we've only been using Python's built-in functionality, but some of these operations are much easier to perform using a library like NLTK, which stands for natural language toolkit. The most common approach for splitting up texting NLTK is to use the word tokenized function from nltk.tokenize. This performs the same task as split but is a little smarter. Try passing in some raw text that has not been normalized. You'll notice that the punctuations are treated differently based on their position. Here, the period after the title Doctor has been retained along with Dr as a single token. As you can imagine, NLTK is using some rules or patterns to decide what to do with each punctuation. Sometimes, you may need to split text into sentences. For instance, if you want to translate it. You can achieve this with NLTK using sent tokenize. Then you can split each sentence into words if needed. NLTK provide several other tokenizers, including a regular expression base tokenizer that you can use to remove punctuation and perform tokenization in a single step, and also a tweet tokenizer that is aware of twitter handles, hash tags, and emoticons. Check out the nltk.tokenize package for more details.

### 07. Stop Word Removal-WAU_Ij0GJbw.en

Stop words are uninformative words like, is, our, the, in, at, et cetera that do not add a lot of meaning to a sentence. They are typically very commonly occurring words, and we may want to remove them to reduce the vocabulary we have to deal with and hence the complexity of later procedures. Notice that even without our and the in the sentence above, we can still infer it's positive sentiment toward dogs. You can see for yourself which words NLTK considers to be stop words in English. Note that this is based on a specific corpus or collection of text. Different corpora may have different stop words. Also, a word maybe a stop word in one application, but a useful word in another. To remove stop words from a piece of text, you can use a Python list comprehension with a filtering condition. Here, we apply stop word removal to the movie review after normalizing and tokenizing it. The result is a little hard to read, but notice how it has helped reduce the size of the input, at the same time important words have been retained.

### 08. Part-of-Speech Tagging-WFEu8bXI5OA.en

Remember parts of speech from school? Nouns, pronouns, verbs, adverbs, et cetera. Identifying how words are being used in a sentence can help us better understand what is being said. It can also point out relationships between words and recognize cross references. NLTK, again, makes things pretty easy for us. You can pass in tokens or words to the POS tag function which returns a tag for each word identifying different parts of speech. Notice how it has correctly labelled the first utterance of "lie" as a verb, while marking the second one as a noun. Refer to the NLTK documentation for more details on what each tag means. One of the cool applications of part of speech tagging is parsing sentences. Here's an example from the NLTK book that uses a custom grammar to parse an ambiguous sentence. Notice how the parser returns both interpretations that are valid. It is much easier to see the difference when we visualize the parse trees. I shot an elephant in my pajamas, versus, I shot an elephant and the elephant was in my pajamas. How he got into my pajamas? I don't know.

### 09. Named Entity Recognition-QUQu2nsE7vE.en

Named entities are typically noun phrases that refer to some specific object, person, or place. You can use the ne_chunk function to label named entities in text. Note that you have to first tokenize and tag parts of speech. This is a very simple example, but notice how the different entity types are also recognized: person, organization, and GPE, which stands for geopolitical entity. Also note how it identified the two words, Udacity and Inc, together as a single entity. Out in the wild, performance is not always great but training on a large corpus definitely helps. Named entity recognition is often used to index and search for news articles, for example, on companies of interest.

### 10. Stemming And Lemmatization-7Gjf81u5hmw.en

In order to further simplify text data, let's look at some ways to normalize different variations and modifications of words. Stemming is the process of reducing a word to its stem or root form. For instance, branching, branched, branches et cetera, can all be reduced to branch. After all, they conveyed the idea of something separating into multiple paths or branches. Again, this helps reduce complexity while retaining the essence of meaning that is carried by words. Stemming is meant to be a fast and crude operation carried out by applying very simple search and replace style rules. For example, the suffixes 'ing' and 'ed' can be dropped off, 'ies' can be replaced by 'y' et cetera. This may result in stem words that are not complete words, but that's okay, as long as all forms of that word are reduced to the same stem. Thus, capturing the common underlying idea. NLTK has a few different stemmers for you to choose from, including PorterStemmer that we use here, SnowballStemmer, and other language-specific stemmers. You simply need to pass in one word at a time. Note that here, we have already removed stop words. Some of the conversions are actually pretty good, like started, reduced to start. Others, like people, losing the 'e' at the end are a result of applying very simplistic rules. Lemmatization is another technique used to reduce words to a normalized form, but in this case, the transformation actually uses a dictionary to map different variants of a word back to its root. With this approach, we are able to reduce non-trivial inflections such as is, was, were, back to the root 'be'. The default lemmatizer in NLTK uses the Wordnet database to reduce words to the root form. Let's try it out. Just like in stemming, you initialize an instance of WordNetLemmatizer and pass in individual words to its lemmatize method. What happened here? It seems that only the word ones got reduced to one, all the others are unchanged. If you read the words carefully, you'll see that ones is the only plural noun here. In fact, that's exactly why it got transformed. A lemmatizer needs to know or make an assumption about the part of speech for each word it's trying to transform. In this case, WordNetLemmatizer defaults to nouns, but we can override that by specifying the PoS parameter. Let's pass in 'v' for verbs. This time, the two verb forms 'boring' and 'started' got converted. Great. Note that there are other verbs, but they are already in the root form. Also, note how we passed in the output from the previous noun lemmatization step. This way of chaining procedures is very common. Let's recap. As we saw in the previous examples, stemming sometimes results in stems that are not complete words in English. Lemmatization is similar to stemming with one difference, the final form is also a meaningful word. That said, stemming does not need a dictionary like lemmatization does. So depending on the constraints you have, stemming maybe a less memory intensive option for you to consider.

### 11. Summary-zKYEvRd2XmI.en

We have covered a number of text processing steps. Let's summarize what a typical workflow looks like. Starting with a plain text sentence, you first normalize it by converting to lowercase and removing punctuation, and then you split it up into words using a tokenizer. Next, you can remove stop words to reduce the vocabulary you have to deal with. Depending on your application, you may then choose to apply a combination of stemming and lemmatization to reduce words to the root or stem form. It is common to apply both, lemmatization first, and then stemming. This procedure converts a natural language sentence into a sequence of normalized tokens which you can use for further analysis.

## Part 02-Module 01-Lesson 04_Feature Extraction

### 01. Feature Extraction-Bd6TJB8eVLQ.en

Once we have our text ready in a clean and normalized form, we need to transform it into features that can be used for modeling. For instance, treating each document like a bag of words allows us to compute some simple statistics that characterize it. These statistics can be improved by assigning appropriate weights towards using a TF-IDF Scheme. This enables a more accurate comparison between documents. For certain applications, we may need to find numerical representations of individual words, and for that, we can use word embeddings, which are a very efficient and powerful method. In this lesson, you will learn all these techniques for extracting relevant features from text data.

### 02. Bag Of Words-A7M1z8yLl0w.en

The first feature representation we'll look at is called Bag of Words. The Bag of Words model treats each document as an un-ordered collection or bag of words. Here, a document is the unit of text that you want to analyze. For instance, if you want to compare essays submitted by students to check for plagiarism, each essay would be a document. If you want to analyze the sentiment conveyed by tweets, then each tweet would be a document. To obtain a bag of words from a piece of raw text, you need to simply apply appropriate text processing steps: cleaning, normalizing, splitting into words, stemming, lemmatization, et cetera. And then treat the resulting tokens as an un-ordered collection or set. So, each document in your data set will produce a set of words. But keeping these as separate sets is very inefficient. They're of different sizes, may contain different words, and are hard to compare. Also, whatever word occurs multiple times in a document? Is there a better representation you can think of? A more useful approach is to turn each document into a vector of numbers, representing how many times each word occurs in a document. A set of documents is known as a corpus, and this gives the context for the vectors to be calculated. First, collect all the unique words present in your corpus to form your vocabulary. Arrange these words in some order, and let them form the vector element positions or columns of a table, and assume each document is a row. Then count the number of occurrences of each word in each document and enter the value in the respective column. At this stage, it is easier to think of this as a Document-Term Matrix, illustrating the relationship between documents in rows, and words or terms in columns. Each element can be interpreted as a term frequency. How frequently does that term occur in this document? Now, consider what you can do with this representation. One possibility is to compare two documents based on how many words they have in common or how similar their term frequencies are. A more mathematical way of expressing that is to compute the dot product between the two row vectors, which is the sum of the products of corresponding elements. Greater the dot product, more similar the two vectors are. The dot product has one flaw, it only captures the portions of overlap. It is not affected by other values that are not uncommon. So, pairs that are very different can end up with the same product as ones that are identical. A better measure is cosine similarity, where we divide the dot product of two vectors by the product of their magnitudes or Euclidean norms. If you think of these vectors as arrows in some n-dimensional space, then this is equal to the cosine of the angle theta between them. Identical vectors have cosine equals one. Orthogonal vectors have cosine equal zero. And for vectors that are exactly opposite, it is minus one. So, the values always range nicely between one for most similar, to minus one, most dissimilar.

### 03. TF-IDF-XZBiBIRcACE.en

One limitation of the bag-of-words approach is that it treats every word as being equally important, whereas intuitively, we know that some words occur frequently within a corpus. For example, when looking at financial documents, cost or price may be a pretty common term. We can compensate for this by counting the number of documents in which each word occurs, this can be called document frequency, and then dividing the term frequencies by the document frequency of that term. This gives us a metric that is proportional to the frequency of occurrence of a term in a document, but inversely proportional to the number of documents it appears in. It highlights the words that are more unique to a document, and thus better for characterizing it. You may have heard of, or used, the TF-IDF transform before. It's simply the product of two words, very similar to what we've seen so far, a term frequency and an inverse document frequency. The most commonly used form of TF-IDF defines term frequency as the raw count of a term, t, in a document, d, divided by the total number of terms in d, and inverse document frequency as the logarithm of the total number of documents in the collection, d, divided by the number of documents where t is present. Several variations exist that try to normalize, or smooth the resulting values, or prevent edge cases such as divide-by-zero errors. Overall, TF-IDF is an innovative approach to assigning weights to words that signify their relevance in documents.

### 04. One-Hot Encoding-a0j1CDXFYZI.en

So far, we've looked at representations that tried to characterize an entire document or collection of words as one unit. As a result, the kinds of inferences we can make are also typically at a document level, mixture of topics in the document, documents similarity, documents sentiment, et cetera. For a deeper analysis of text, we need to come up with a numerical representation for each word. If you've dealt with categorical variables for data analysis or tried to perform multi-class classification, you may have come across this term, One-Hot Encoding. That is one way of representing words, treat each word like a class, assign it a vector that has one in a single pre-determined position for that word and zero everywhere else. Looks familiar? Yeah, it's just like the bag of words idea, only that we keep a single word in each bag and build a vector for it.

### 05. Word Embeddings-4mM_S9L2_JQ.en

One-hot encoding usually works in some situations but breaks down when we have a large vocabulary to deal with, because the size of our ward representation grows with the number of words. What we need as a way to control the size of our word representation by limiting it to a fixed-size vector. In other words, we want to find an embedding for each word in some vector space and we wanted to exhibit some desired properties. For example, if two words are similar in meaning, they should be closer to each other compared to words that are not. And if two pairs of words have a similar difference in their meanings, they should be approximately equally separated in the embedded space. We could use such a representation for a variety of purposes like finding synonyms and analogies, identifying concepts around which words are clustered, classifying words as positive, negative, neutral, et cetera. By combining word vectors, we can come up with another way of representing documents as well.

### 06. Word2Vec-7jjappzGRe0.en

Word2Vec is perhaps one of the most popular examples of word embeddings used in practice. As the name Word2Vec indicates, it transforms words to vectors. But what the name doesn't give away is how that transformation is performed. The core idea behind Word2Vec is this, a model that is able to predict a given word, given neighboring words, or vice versa, predict neighboring words for a given word is likely to capture the contextual meanings of words very well. And these are, in fact, two flavors of Word2Vec models, one where you are given neighboring words called continuous bag of words, and the other where you are given the middle word called Skip-gram. In the Skip-gram model, you pick any word from a sentence, convert it into a one-hot encoded vector and feed it into a neural network or some other probabilistic model that is designed to predict a few surrounding words, its context. Using a suitable loss function, optimize the weights or parameters of the model and repeat this till it learns to predict context words as best as it can. Now, take an intermediate representation like a hidden layer in a neural network. The outputs of that layer for a given word become the corresponding word vector. The Continuous Bag of Words variation also uses a similar strategy. This yields a very robust representation of words because the meaning of each word is distributed throughout the vector. The size of the word vector is up to you, how you want to tune performance versus complexity. It remains constant no matter how many words you train on, unlike Bag of Words, for instance, where the size grows with the number of unique words. And once you pre-train a large set of word vectors, you can use them efficiently without having to transform again and again, just store them in a lookup table. Finally, it is ready to be used in deep learning architectures. For example, it can be used as the input vector for recurrent neural nets. It is also possible to use RNNs to learn even better word embeddings. Some other optimizations are possible that further reduce the model and training complexity such as representing the output words using Hierarchical Softmax, computing loss using Sparse Cross Entropy, et cetera.

### 07. GloVe-KK3PMIiIn8o.en

Word2vec is just one type of forward embedding. Recently, several other related approaches have been proposed that are really promising. GloVe or global vectors for word representation is one such approach that tries to directly optimize the vector representation of each word just using co- occurrence statistics, unlike word2vec which sets up an ancillary prediction task. First, the probably that word j appears in the context of word i is computed, pj given i for all word pairs ij in a given corpus. What do we mean by j appears in context of i? Simply that word j is present in the vicinity of word i, either right next to it, or a few words away. We count all such occurrences of i and j in our text collection, and then normalize account to get a probability. Then, a random vector is initialized for each word, actually two vectors. One for the word when it is acting as a context, and one when it is acting as the target. So far, so good. Now, for any pair of words, ij, we want the dot product of their word vectors, w_i times w_j, to be equal to their co-occurrence probability. Using this as our goal and a suitable last function, we can iteratively optimize these word vectors. The result should be a set of vectors that capture the similarities and differences between individual words. If you look at it from another point of view, we are essentially factorizing the co-occurrence probability matrix into two smaller matrices. This is the basic idea behind GloVe. All that sounds good, but why co-occurrence probabilities? Consider two context words, say ice and steam, and two target words, solid and water. You would come across solid more often in the context of ice than steam, right? But water could occur in either context with roughly equal probability. At least, that's what we would expect. Surprise. That's exactly what co-occurrence probabilities reflect. Given a large corpus, you'll find that the ratio of P solid given ice to P solid given steam is much greater than one, while the ratio of P water given ice and P water given steam is close to one. Thus, we see that co-occurrence probabilities already exhibit some of the properties we want to capture. In fact, one refinement over using raw probability values is to optimize for the ratio of probabilities. Now, there are a lot of subtleties here, not the least of which is the fact that the co-occurence probability matrix is huge. At the same time, co-occurrence probability values are typically very low, so it makes sense to work with the log of these values. I encourage you to read the original paper that introduced GloVe to get a better understanding of this technique.

### 08. Embeddings For Deep Learning-gj8u1KG0H2w.en

Where the embeddings are fast becoming the de facto choice for representing words, especially for use and deep neural networks. But why do these techniques work so well? Doesn't it seem almost magical that you can actually do arithmetic with words, like woman minus man plus king equals queen? The answer might lie in the distributional hypothesis, which states that words that occur in the same contexts tend to have similar meanings. For example, consider this sentence. Would you like to have a cup of blank? Okay. How about, I like my blank black. One more, I need my morning blank before I can do anything. What are you thinking? Tea? Coffee? What give you the hint? Cup? Black? Morning? But it could be either of the two, right? And that's the point. In these contexts, tea and coffee are actually similar. Therefore, when a large collection of sentences is used to learn in embedding, words with common context words tend to get pulled closer and closer together. Of course, there could also be contexts in which tea and coffee are dissimilar. For example, blank grounds are great for composting. Or, I prefer loose leaf blank. Here we are clearly talking about coffee grounds, and loose leaf tea. How do we capture these similarities and differences in the same embedding? By adding another dimension. Let's see how. Words can be close along one dimension. Here, tea and coffee are both beverages, but separated along some other dimension. Maybe this dimension captures all the variability among beverages. In a human language, there are many more dimensions along which word meanings can vary. And the more dimensions you can capture in your word vector, the more expressive that representation will be. But how many dimensions do you really need? Consider a typical neural network architecture designed for an NLP task, say word prediction. It's common to use a word embedding layer that produces a vector with a few hundred dimensions, but that's significantly small compared to using one heart encodings directly, which are as large as the size of the vocabulary, sometimes in tens of thousands of words. Also, if you learn the embedding as part of the model training process, you can obtain a representation that captures the dimensions that are most relevant for your task. This often adds complexity. So unless you're building a model for a very narrow application like one that deals with medical terminology, you can use a pre-trained embedding as a look-up. For example, work to veck or glove. Then you only need to train the layer specific to your task. Compare this with the network architecture for a computer vision task, say, image classification, the raw input here is also very high dimensional. For example, even 128 by 128 Image contains over 16 thousand pixels. We typically use convolutional layers to exploit the spatial relationships and image data and reduce this dimensionality. Early stages and visual processing are often transferable across tasks, so it is common to use some pre-trained layers from an existing network, like Alex nad or BTG 16 and only learn the later layers. Come to think of it, using an embedding look up for NLP is not on like using pre-treated layers for computer vision. Both are great examples of transfer learning.

### 09. T-SNE-xxcK8oZ6_WE.en

Word embeddings need to have high dimensionality in order to capture sufficient variations in natural language, which makes them super hard to visualize. T-SNE, which stands for t-Distributed Stochastic Neighbor Embedding, is a dimensionality reduction technique that can map high dimensional vectors to a lower dimensional space. It's kind of like PCA, Principle Component Analysis, but with one amazing property. When performing the transformation, it tries to maintain relative distances between objects, so that similar ones stay closer together while dissimilar objects stay further apart. This makes t-SNE a great choice for visualizing word embeddings. It effectively preserves the linear substructures and relationships that have been learned by the embedding model. If we look at the larger vector space, we can discover meaningful groups of related words. Sometimes, that takes a while to realize why certain clusters are formed, but most of the groupings are very intuitive. T-SNE also works on other kinds of data, such as images. Here, we see pictures from the Caltech 101 dataset organized into clusters that roughly correspond to class labels, including airplanes with blue sky being the common theme, sailboats of different shapes and sizes, and human faces. This is a very useful tool for better understanding the representation that a network learns and for identifying any bugs or other issues.

### 10. NLP Summary-B9ul8fsQYOA.en

Congratulations on completing the lesson. But remember, this is only the beginning of a long and exciting journey into a world with limitless possibilities. It was a pleasure helping you take your first few steps and I'm looking forward to seeing what systems you will build, what new problems you will solve, and how you'll advance the field of natural language processing. Good luck on your adventure.

## Part 02-Module 02-Lesson 01_Introduction to Neural Networks



### 05. Linear Boundaries-X-uMlsBi07k.en

So, first let's add some math. We're going to label the horizontal axis corresponding to the test by the variable x1, and the vertical axis corresponding to the grades by the variable x2. So this boundary line that separates the blue and the red points is going to have a linear equation. The one drawn has equation 2x1+x2-18=0. What does this mean? This means that our method for accepting or rejecting students simply says the following: take this equation as our score, the score is 2xtest+grades-18. Now when the student comes in, we check their score. If their score is a positive number, then we accept the student and if the score is a negative number then we reject the student. This is called a prediction. We can say by convention that if the score is 0, we'll accept a student although this won't matter much at the end. And that's it. That linear equation is our model. In the more general case, our boundary will be an equation of the following wx1+w2x2+b=0. We'll abbreviate this equation in vector notation as wx+b=0, where w is the vector w1w2 and x is the vector x1x2. And we simply take the product of the two vectors. We'll refer to x as the input, to w as the weights and b as the bias. Now, for a student coordinates x1x2, we'll denote a label as Y and the label is what we're trying to predict. So if the student gets accepted, namely the point is blue, then the label is Y+1. And if the student gets rejected, namely the point is red and then the label is Y=0. Thus, each point is in the form x1x2Y or Y is 1 for the blue points and 0 for the red points. And finally, our prediction is going to be called Y-hat and it will be what the algorithm predicts that the label will be. In this case, Y-hat is one of the algorithm predicts that the student gets accepted, which means the point lies over the line. And, Y-hat is 0 if the algorithm predicts that this didn't get rejected, which means the point is under the line. In math terms, this means that the prediction Y-hat is 1 if wx+b is greater than or equal to zero and 0 if wx+b is less than 0. So, to summarize, the points above the line have Y hat=1 and the points below the line have Y-hat=0. And, the blue points have Y=1 and the red points have Y=0. And, the goal of the algorithm is to have Y-hat resembling Y as closely as possible, which is exactly equivalent to finding the boundary line that keeps most of the blue points above it and most of the red points below it.

### 06. 09 Higher Dimensions-eBHunImDmWw.en

Now, you may be wondering what happens if we have more data columns so not just testing grades, but maybe something else like the ranking of the student in the class. How do we fit three columns of data? Well the only difference is that now, we won't be working in two dimensions, we'll be working in three. So now, we have three axis: x_1 for the test, x_2 for the grades and x_3 for the class ranking. And our data will look like this, like a bunch of blue and red points flying around in 3D. On our equation won't be a line in two dimension, but a plane in three dimensions with a similar equation as before. Now, the equation would be w_1_x_1 plus w_2_x_2 plus w_3_x_3 plus b equals zero, which will separate this space into two regions. This equation can still be abbreviated by Wx plus b equals zero, except our vectors will now have three entries instead of two. And our prediction will still be y head equals one if Wx plus b is greater than or equal to zero, and zero if Wx plus b is less than zero. And what if we have many columns like say n of them? Well, it's the same thing. Now, our data just leaps in n-dimensional space. Now, I have trouble picturing things in more than three dimensions. But if we can imagine that the points are just things with n coordinates called x_1, x_2, x_3 all the way up to x_n with our labels being y, then our boundaries just an n minus one dimensional hyperplane, which is a high dimensional equivalent of a line in 2D or a plane in 3D. And the equation of this n minus one dimensional hyperplane is going to be w_1_x_1 plus w_2_x_2 plus all the way to w_n_x_n plus b equals zero, which we can still abbreviate to Wx plus b equals zero, where our vectors now have n entries. And our prediction is still the same as before. It is y head equals one if Wx plus b is greater than or equal to zero and y head equals zero if Wx plus b is less than zero.

### 07. DL 06 Perceptron Definition Fix V2-hImSxZyRiOw.en

So let's recap. We have our data which is all these students. The blue ones have been accepted and the red ones have been rejected. And we have our model which consists of the equation two times test plus grades minus 18, which gives rise to this boundary which the point where the score is zero and a prediction. The prediction says that the student gets accepted of the score is positive or zero, and rejected if the score is negative. So now we'll introduce the notion of a preceptron, which is the building block of neural networks, and it's just an encoding of our equation into a small graph. The way we've build it is the following. Here we have our data and our boundary line and we fit it inside a node. And now we add small nodes for the inputs which, in this case, they are the test and the grades. Here we can see an example where test equals seven and grades equals six. And what the perceptron does is it blocks the points seven, six and checks if the point is in the positive or negative area. If the point is in the positive area, then it returns a yes. And if it is in the negative area, it returns and no. So let's recall that our equation is score equals two times test plus one times grade minus 18, and that our prediction consists of accepting the student if the score is positive or zero, and rejecting them if the score is negative. These weights two, one, and minus 18, are what define the linear equation, and so we'll use them as labels in the graph. The two and the one will label the edges coming from X1 and X 2 respectively, and the bias unit minus 18 will label the node. Thus, when we see a node with these labels, we can think of the linear equation they generate. Another way to grab this node is to consider the bias as part of the input. Now since W1 gets multiplied by X1 and W2 by X2, It's natural to think that B gets multiplied by a one. So we'll have the B labeling and and edge coming from a one. Then what the node does is it multiplies the values coming from the incoming nodes by the values and the corresponding edges. Then it adds them and finally, it checks if the result is greater that are equal to zero. If it is, then the node returns a yes or a value of one, and if it isn't then the node returns a no or a value of zero. We'll be using both notations throughout this class although the second one will be used more often. In the general case, this is how the nodes look. We will have our node over here then end inputs coming in with values X1 up to Xn and one, and edges with weights W1 up to Wn, and B corresponding to the bias unit. And then the node calculates the linear equation Wx plus B, which is a summation from I equals one to n, of WIXI plus B. This node then checks if the value is zero or bigger, and if it is, then the node returns a value of one for yes and if not, then it returns a value of zero for no. Note that we're using an implicit function, here, which is called a step function. What the step function does is it returns a one if the input is positive or zero, and a zero if the input is negative. So in reality, these perceptrons can be seen as a combination of nodes, where the first node calculates a linear equation and the inputs on the weights, and the second node applies the step function to the result. These can be graphed as follows: the summation sign represents a linear function in the first node, and the drawing represents a step function in the second node. In the future, we will use different step functions. So this is why it's useful to specify it in the node. So as we've seen there are two ways to represent perceptions. The one on the left has a bias unit coming from an input node with a value of one, and the one in the right has the bias inside the node.

### 08. -zAkzOZntK6Y.en

So you may be wondering why are these objects called neural networks. Well, the reason why they're called neural networks is because perceptions kind of look like neurons in the brain. In the left we have a perception with four inputs. The number is one, zero, four, and minus two. And what the perception does, it calculates some equations on the input and decides to return a one or a zero. In a similar way neurons in the brain take inputs coming from the dendrites. These inputs are nervous impulses. So what the neuron does is it does something with the nervous impulses and then it decides if it outputs a nervous impulse or not through the axon. The way we'll create neural networks later in this lesson is by concatenating these perceptions so we'll be mimicking the way the brain connects neurons by taking the output from one and turning it into the input for another one.

### 10. 07 Perceptron Algorithm Trick-lif_qPmXvWA.en

Now, let me show you a trick that will make a line go closer to a point. Let's say we have our linear equation for example, 3x1 + 4x2 -10. And that linear equation gives us a line which is the points where the equation is zero and two regions. The positive region drawn in blue where 3x1 + 4x2 - 10 is positive, and the negative region drawn in red with 3x1 + 4x2 - 10 is negative. So here we have our lonely misclassified point, the 0.4, 5 which is a red point in the blue area, and the point has to come closer. So how do we get that point to come closer to the line? Well, the idea is we're going to take the four and five and use them to modify the equation of the line in order to get the line to move closer to the point. So here are parameters of the line 3, 4 and -10 and the coordinates of the point are 4 and 5, and let's also add a one here for the bias unit. So what we'll do is subtract these numbers from the parameters of the line to get 3 - 4, 4 - 5, and -10 -1. The new line will have parameters -1, -1, -11. And this line will move drastically towards the point, possibly even going over it and placing it in the correct area. Now, since we have a lot of other points, we don't want to make any drastic moves since we may accidentally misclassify all our other points. We want the line to make a small move towards that point and for this, we need to take small steps towards the point. So here's where we introduce the learning rate, the learning rate is a small number for example, 0.1 and what we'll do is instead of subtracting four, five and one from the coordinates of the line, we'll multiply these numbers by 0.1 and then subtract them from the equation of the line. This means we'll be subtracting 0.4, 0.5, and 0.1 from the equation of the line. Obtaining a new equation of 2.6x1 + 3.5x 2 - 10.1 = 0. This new line will actually move closer to the point. In the same way, if we have a blue point in the red area, for example, the point 1,1 is a positively labeled point in the negative area. This point is also misclassified and it says, come closer. So what do we do here is the same thing, except now instead of subtracting the coordinates to the parameters of the line, we add them. Again, we multiply by the learning rate in order to make small steps. So here we take the coordinates of the point 1,1 and put an extra one for the constant term and now, we multiply them by the learning rates 0.1. Now, we add them to the parameters of the line and we get a new line with equation 3.1x1 + 4.1x2 - 9.9. And magic, this line is closer to the point. So that's the trick we're going to use repeatedly for the Perceptron Algorithm.

### 10. DL 10 S  Perceptron Algorithm-fATmrG2hQzI.en

Well, consider this. If you're in the wrong area, you would like the line to go over you, in order to be in the right area. Thus, the points just come closer! So the line can move towards it and eventually classify it correctly.

### 10. Perceptron Algorithm--zhTROHtscQ.en

So we had a question we're trying to answer and the question is, how do we find this line that separates the blue points from the red points in the best possible way? Let's answer this question by first looking at a small example with three blue points and three red points. And we're going to describe an algorithm that will find the line that splits these points properly. So the computer doesn't know where to start. It might as well start at a random place by picking a random linear equation. This equation will define a line and a positive and negative area given in blue and red respectively. What we're going to do is to look at how badly this line is doing and then move it around to try to get better and better. Now the question is, how do we find how badly this line is doing? So let's ask all the points. Here we have four points that are correctly classified. They are these two blue points in the blue area and these two red points in the red area. And these points are correctly classified, so they say, "I'm good." And then we have these two points that are incorrectly classified. That's this red point in the blue area and this blue point in the red area. We want to get as much information from them so we want them to tell us something so that we can improve this line. So what is it that they can tell us? So here we have a misclassified point, this red point in the blue area. Now think about this. If you were this point, what would you tell the line to do? Would you like it to come closer to you or farther from you? That's our quiz. Will the misclassified point want the line to come closer to it or farther from it?

### 11. Perceptron Agorithm Pseudocode-p8Q3yu9YqYk.en

Now, we finally have all the tools for describing the perceptron algorithm. We start with the random equation, which will determine some line, and two regions, the positive and the negative region. Now, we'll move this line around to get a better and better fit. So, we ask all the points how they're doing. The four correctly classified points say, "I'm good." And the two incorrectly classified points say, "Come closer." So, let's listen to the point in the right, and apply the trick to make the line closer to this point. So, here it is. Now, this point is good. Now, let's listen to the point in the left. The points says, "Come closer." We apply the trick, and now the line goes closer to it, and it actually goes over it classifying correctly. Now, every point is correctly classified and happy. So, let's actually write the pseudocode for this perceptron algorithm. We start with random weights, w1 up to wn and b. This gives us the question wx plus b, the line, and the positive and negative areas. Now, for every misclassified point with coordinates x1 up to xn, we do the following. If the prediction was zero, which means the point is a positive point in the negative area, then we'll update the weights as follows: for i equals 1 to n, we change wi, to wi plus alpha times xi, where alpha is the learning rate. In this case, we're using 0.1. Sometimes, we use 0.01 etc. It depends. Then we also change the bi as unit to b plus alpha. That moves the line closer to the misclassified point. Now, if the prediction was one, which means a point is a negative point in the positive area, then we'll update the weights in a similar way, except we subtract instead of adding. This means for i equals 1, change wi, to wi minus alpha xi, and change the bi as unit b to b minus alpha. And now, the line moves closer to our misclassified point. And now, we just repeat this step until we get no errors, or until we have a number of error that is small. Or simply we can just say, do the step a thousand times and stop. We'll see what are our options later in the class.

### 12. Non-Linear Regions-B8UrWnHh1Wc.en

Okay, so let's look more carefully at this model for accepting and rejecting students. Let's say we have this student four, who got nine in the test, but only one on the grades. According to our model this student gets accepted since it's placed over here in the positive region of this line. But let's say we don't want that since we'll say, "If your grades were terrible, no matter what you got on the test, you won't get accepted". So our data should look more like this instead. This model is much more realistic but now we have a problem which is the data can no longer be separated by just a line. So what is the next thing after a line? Maybe a circle. A circle would work. Maybe two lines. That could work, too. Or maybe a curve like this. That would also work. So let's go with that. Let's go with the curve. Now, unfortunately, the perceptron algorithm won't work for us this time. We'll have to come up with something more complex and actually the solution will be, we need to redefine our perceptron algorithm for a line in a way that it'll generalize to other types of curves.

### 13. Error Functions-YfUUunxWIJw.en

So the way we'll solve our problems from now on is with the help of an error function. An error function is simply something that tells us how far we are from the solution. For example, if I'm here and my goal is to get to this plant, an error function will just tell me the distance from the plant. My approach would then be to look around myself, check in which direction I can take a step to get closer to the plant, take that step and then repeat. Here the error is simply the distance from the plant.

### 14. Error Functions-jfKShxGAbok.en

Here is obvious realization of the error function. We're standing on top a mountain, Mount Errorest and I want to descend but it's not that easy because it's cloudy and the mountain is very big, so we can't really see the big picture. What we'll do to go down is we'll look around us and we consider all the possible directions in which we can walk. Then we pick a direction that makes us descend the most. Let's say it's this one over here. So we take a step in that direction. Thus, we've decreased the height. Once we take the step and we start the process again and again always decreasing the height until we go all the way down the mountain, minimizing the height. In this case the key metric that we use to solve the problem is the height. We'll call the height the error. The error is what's telling us how badly we're doing at the moment and how far we are from an ideal solution. And if we constantly take steps to decrease the error then we'll eventually solve our problem, descending from Mt. Errorest. Some of you may be thinking, wait, that doesn't necessarily solve the problem. What if I get stuck in a valley, a local minimum, but that's not the bottom of the mountain. This happens a lot in machine learning and we'll see different ways to solve it later in this Nanodegree. It's also worth noting that many times a local minimum will give us a pretty good solution to a problem. This method, which we'll study in more detail later, is called gradient descent. So let's try that approach to solve a problem. What would be a good error function here? What would be a good way to tell the computer how badly it's doing? Well, here's our line with our positive and negative area. And the question is how do we tell the computer how far it is from a perfect solution? Well, maybe we can count the number mistakes. There are two mistakes here. So that's our height. That's our error. So just as we did to descend from the mountain, we look around all the directions in which we can move the line in order to decrease our error. So let's say we move in this direction. We'll decrease the number of errors to one and then if we're moving in that direction, we'll decrease the number of errors to zero. And then we're done, right? Well, almost. There's a small problem with that approach. In our algorithms we'll be taking very small steps and the reason for that is calculus, because our tiny steps will be calculated by derivatives. So what happens if we take very small steps here? We start with two errors and then move a tiny amount and we're still at two errors. Then move a tiny amount again and we're still two errors. Another tiny amount and we're still at two and again and again. So not much we can do here. This is equivalent to using gradient descent to try to descend from an Aztec pyramid with flat steps. If we're standing here in the second floor, for the two errors and we look around ourselves, we'll always see two errors and we'll get confused and not know what to do. On the other hand in Mt. Errorest we can detect very small variations in height and we can figure out in what direction it can decrease the most. In math terms this means that in order for us to do gradient descent our error function can not be discrete, it should be continuous. Mt. Errorest is continuous since small variations in our position will translate to small variations in the height but the Aztec pyramid does not since the high jumps from two to one and then from one to zero. As a matter of fact, our error function needs to be differentiable, but we'll see that later. So, what we need to do here is to construct an error function that is continuous and we'll do this as follows. So here are six points with four of them correctly classified, that's two blue and two red, and two of them incorrectly classified, that is this red point at the very left and this blue point at the very right. The error function is going to assign a large penalty to the two incorrectly classified points and small penalties to the four correctly classified points. Here we are representing the size of the point as the penalty. The penalty is roughly the distance from the boundary when the point is misclassified and almost zero when the point is correctly classified. We'll learn the formula for the error later in the class. So, now we obtain the total error by adding all the errors from the corresponding points. Here we have a large number so it is two misclassified points add a large amount to the error. And the idea now is to move the line around in order to decrease these error. But now we can do it because we can make very tiny changes to the parameters of the line which will amount to very tiny changes in the error function. So, if you move the line, say, in this direction, we can see that some errors decrease, some slightly increase, but in general when we consider the sum, the sum gets smaller and we can see that because we've now correctly classified the two points that were misclassified before. So once we are able to build an error function with this property, we can now use gradient descent to solve our problem. So here's the full picture. Here we are at the summit of Mt. Errorest. We're quite high up because our error is large. As you can see the error is the height which is the sum of the blue and red areas. We explore around to see what direction brings us down the most, or equivalently, what direction can we move the line to reduce the error the most, and we take a step in that direction. So in the mountain we go down one step and in the graph we've reduced the error a bit by correctly classifying one of the points. And now we do it again. We calculate the error, we look around ourselves to see in what direction we descend the most, we take a step in that direction and that brings us down the mountain. So on the left we have reduced the height and successfully descended from the mountain and on the right we have reduced the error to its minimum possible value and successfully classified our points. Now the question is, how do we define this error function? That's what we'll do next.

### 15. Discrete vs Continuous-rdP-RPDFkl0.en

In the last section we pointed out the difference between a discrete and a continuous error function and discovered that in order for us to use gradient descent we need a continuous error function. In order to do this we also need to move from discrete predictions to continuous predictions. Let me show you what I mean by that.

### 15. Discrete vs. Continuous-Rm2KxFaPiJg.en

The prediction is basically the answer we get from the algorithm. A discreet answer will be of the form yes, no. Whereas a continued answer will be a number, normally between zero and one which we'll consider a probability. In the running example, here we have our students where blue is accepted and red is rejected. And the discrete algorithm will tell us if a student is accepted or rejected by typing a zero for rejected students and a one for accepted students. On the other hand, the farther our point is from the black line, the more drastic these probabilities are. Points that are well into the blue area get very high probabilities, such as this point with an 85% probability of being blue. And points that are well into the red region are given very low probabilities, such as this point on the bottom that is given a 20% probability of being blue. The points over the line are all given a 50% probability of being blue. As you can see the probability is a function of the distance from the line. The way we move from discrete predictions to continuous, is to simply change your activation function from the step function in the left, to the sigmoid function on the right. The sigmoid function is simply a function which for large positive numbers will give us values very close to one. For large negative numbers will give us values very close to zero. And for numbers that are close to zero, it'll give you values that are close to point five. The formula is sigmoid effects equals (x) = 1/(1 + exp(-x)) So, before our model consisted of a line with a positive region and a negative region. Now it consists of an entire probability space or for each point in the plane we are given the probability that the label of the point is one for the blue points, and zero for the red points. For example, for this point the probability of being blue is 50% and of being red is 50%. For this point, the probabilities are 40% for being blue, and 60% for being red. For this one over here it's 30% for blue, and 70% for red. And for this point all over here is 80% for being blue and 25 percent for being red. The way we obtain this probability space is very simple. We just combine the linear function WX + b with the sigmoid function. So in the left we have the lines that represent the points for which WX + b is zero, one, two, minus one, minus two, etc. And once we apply the sigmoid function to each of these values in the plane, we then obtain numbers from zero to one for each point. These numbers are just the probabilities of the point being blue. The probability of the point being blue is a prediction of the model Y hat to sigmoid of W x plus b. Here we can see the lines for which the prediction is point five, point six, point seven, point four, point three, et cetera. As you can see, as we get more into the blue area, (Wx + b) gets closer and closer to one. And as we move into the red area, (Wx + b) gets closer and closer to zero. When we're over the main line, W x plus b is zero, which means sigmoid of W s plus b is exactly zero point five. So here on the left we have our old perceptron with the activation function as a step function. And on the right we have our new perceptron, where the activation function is the sigmoid function. What our new perceptron does, it takes the inputs, multiplies them by the weights in the edges and adds the results, then applies the sigmoid function. So instead of returning one and zero like before it returns values between zero and one such as 0.99 or 0.67 etc. Before it used to say the student got accepted or not, and now it says the probability of the student got accepted is this much.

### 16. DL 18 Q Softmax V2-RC_A9Tu99y4.en

Let's switch to a different example for a moment. Let's say we have a model that will predict if you receive a gift or not. So, the model use predictions in the following way. It says, the probability that you get a gift is 0.8, which automatically implies that the probability that you don't receive a gift is 0.2. And what does the model do? What the model does is take some inputs. For example, is it your birthday or have it been good all year? And based on those inputs, it calculates a linear model which would be the score. Then, the probability that you get the gift or not is simply the sigmoid function applied to that score. Now, what if you had more options than just getting a gift or not a gift? Let's say we have a model that just tell us what animal we just saw, and the options are a duck, a beaver and a walrus. We want a model that tells an answer along the lines of, the probability of a duck is 0.67, the probability of a beaver is 0.24, and the probability of a walrus is 0.09. Notice that the probabilities need to add to one. Let's say we have a linear model based on some inputs. The inputs could be, does it have a beak or not? Number of teeth. Number of feathers. Hair, no hair. Does it live in the water? Does it fly? Etc. We calculate linear function based on those inputs, and let's say we get some scores. So, the duck gets a score of two, and the beaver gets a score of one, and the walrus gets a score of zero. And now the question is, how do we turn these scores into probabilities? The first thing we need to satisfy with probabilities is as we said, they need to add to one. So the two, the one, and the zero do not add to one. The second thing we need to satisfy is, since the duck had a higher score than the beaver and the beaver had a higher score than the walrus, then we want the probability of the duck to be higher than the probability of the beaver, and the probability of the beaver to be higher than the probability of the walrus. Here's a simple way of doing it. Let's take each score and divide it by the sum of all the scores. The two becomes two divided by two plus one plus zero, the one becomes one divided by two plus one plus zero, and the zero becomes zero divided by two plus one plus zero. This kind of works because the probabilities we obtain are two thirds for the duck, one third for the beaver, and zero for the walrus. That works but there's a little problem. Let's think about it. What could this problem be? The problem is the following. What happens if our scores are negative? This is completely plausible since the scores are linear function which could give negative values. What if we had, say, scores of 1, 0 and (-1)? Then, one of the probabilities would turn into one divided by one plus zero plus minus one which is zero, and we know very well that we cannot divide by zero. This unfortunately won't work, but the idea is good. How can we turn this idea into one that works all the time even for negative numbers? Well, it's almost like we need to turn these scores into positive scores. How do we do this? Is there a function that can help us? This is the quiz. Let's look at some options. There's sine, cosine, logarithm, and exponential. Quiz. Which one of these functions will turn every number into a positive number? Enter your answer below.

### 16. DL 18 S Softmax-n8S-v_LCTms.en

So, if you said exponential, you are correct. Because this is a function that returns a positive number for every input. E to the X is always a positive number. So, what we're going to do is exactly what we did before, except, applying it to the X to the scores. So, instead of 2,1, 0, we have E to the 2, E to the 1 and E to the 0. So, that 2 becomes E to the 2 divided by E to the two plus E to the 1 plus E to the 0. And, similarly for 1 and 0. So, the probabilities we obtain now are as 0.67, 0.24 and 0.09. This clearly add to 1. And, also notice that since the exponential function is increasing, then the duck has a higher probability than the beaver. And this one has a higher probability than the walrus. This function is called the Softmax function and it's defined formally like this. Let's say we have N classes and a linear model that gives us the following scores. Z1, Z2, up to ZN. Each score for each of the classes. What we do to turn them into probabilities is to say the probability that the object is in class I is going to be E to the power of the ZI divided by the sum of E to the power of Z1 plus all the way to E to the power ZN. That's how we turn scores into probabilities. So, here's a question for you. When we had two classes, we applied the sigmoid function to the scores. Now, that we have more classes we apply the softmax function to the scores. The question is, is the softmax function for N equals to the same as the sigmoid function? I'll let you think about it. The answer is actually, yes, but it's not super trivial why. And, it's a nice thing to remember.

### 16. Quiz - Softmax-NNoezNnAMTY.en

So far we have models that give us an answer of yes/no or the probability of a label being positive or negative. What if we have more classes? What if we want our model to tell us if something is red, blue, yellow or dog, cat, bird? In this video I'll show you what to do.

### 17. One-Hot Encoding-AePvjhyvsBo.en

So, as we've seen so far, all our algorithms are numerical. This means we need to input numbers, such as a score in a test or the grades, but the input data will not always look like numbers. Sometimes it looks like this. Let's say the module receives as an input the fact that you got a gift or didn't get a gift. How do we turn that into numbers? Well, that's easy. If you've got a gift, we'll just say that the input variable is 1. And, if you didn't get a gift, we'll just say that the input variable is 0. But, what if we have more classes as before or, let's say, our classes are Duck, Beaver and Walrus? What variable do we input in the algorithm? Maybe, we can input a 0 or 1 and a 2, but that would not work because it would assume dependencies between the classes that we can't have. So, this is what we do. What we do is, we come up with one variable for each of the classes. So, our table becomes like this. That's one variable for Duck, one for Beaver and one for Walrus. And, each one has its corresponding column. Now, if the input is a duck then the variable for duck is 1 and the variables for beaver and walrus are 0. Similarly for the beaver and the walrus. We may have more columns of data but at least there are no unnecessary dependencies. This process is called The One-Hot Encoding and it will be used a lot for processing data.

### 18. Maximum Likelihood 1-1yJx-QtlvNI.en

So we're still in our quest for an algorithm that will help us pick the best model that separates our data. Well, since we're dealing with probabilities then let's use them in our favor. Let's say I'm a student and I have two models. One that tells me that my probability of getting accepted is 80% and one that tells me the probability is 55%. Which model looks more accurate? Well, if I got accepted then I'd say the better model is probably the one that says 80%. What if I didn't get accepted? Then the more accurate model is more likely the one that says 55 percent. But I'm just one person. What if it was me and a friend? Well, the best model would more likely be the one that gives the higher probabilities to the events that happened to us, whether it's acceptance or rejection. This sounds pretty intuitive. The method is called maximum likelihood. What we do is we pick the model that gives the existing labels the highest probability. Thus, by maximizing the probability, we can pick the best possible model.

### 18. Maximum Likelihood 2-6nUUeQ9AeUA.en

So let me be more specific. Let's look at the following four points: two blue and two red and two models that classify them, the one on the left and the one on the right. Quick. Which model looks better? You are correct. The model on the right is much better since it classifies the four points correctly whereas the model in the left gets two points correctly and two points incorrectly. But let's see why the model in the right is better from the probability perspective. And by that, we'll show you that the arrangement in the right is much more likely to happen than the one in the left. So let's recall that our prediction is  = (Wx+b) and that that is precisely the probability of a point being labeled positive which means blue. So for the points in the figure, let's say the model tells you that the probability of being blue are 0.9, 0.6, 0.3, and 0.2. Notice that the points in the blue region are much more likely to be blue and the points in the red region are much less likely to be blue. Now obviously, the probability of being red is one minus the probability of being blue. So in this case, the probability of some of the points being red are 0.1, 0.4, 0.7 and 0.8. Now what we want to do is we want to calculate the probability of the four points are of the colors that they actually are. This means the probability that the two red points are red and that the two blue points are blue. Now if we assume that the colors of the points are independent events then the probability for the whole arrangement is the product of the probabilities of the four points. This is equal to 0.1  0.6  0.7  0.2 = 0.0084. This is very small. It's less than 1%. What we mean by this is that if the model is given by these probability spaces, then the probability that the points are of these colors is 0.0084. Now let's do this for both models. As we saw the model on the left tells us that the probabilities of these points being of those colors is 0.0084. If we do the same thing for the model on the right. Let's say we get that the probabilities of the two points in the right being blue are 0.7 and 0.9 and of the two points in the left being red are 0.8 and 0.6. When we multiply these we get 0.3024 which is around 30%. This is much higher than 0.0084. Thus, we confirm that the model on the right is better because it makes the arrangement of the points much more likely to have those colors. So now, what we do is the following? We start from the bad modeling, calculate the probability that the points are those colors, multiply them and we obtain the total probability is 0.0084. Now if we just had a way to maximize this probability we can increase it all the way to 0.3024. Thus, our new goal becomes precisely that, to maximize this probability. This method, as we stated before, is called maximum likelihood.

### 19. Quiz - Cross 1--xxrisIvD0E.en

Well we're getting somewhere now. We've concluded that the probability is important. And that the better model will give us a better probability. Now the question is, how we maximize the probability. Also, if remember correctly we're talking about an error function and how minimizing this error function will take us to the best possible solution. Could these two things be connected? Could we obtain an error function from the probability? Could it be that maximizing the probability is equivalent to minimizing the error function? Maybe.

### 19. Quiz Cross Entropy-njq6bYrPqSU.en

So a quick recap. We have two models, the bad one on the left and the good one on the right. And the way to tell they're bad or good is to calculate the probability of each point being the color it is according to the model. Multiply these probabilities in order to obtain the probability of the whole arrangement and then check that the model on the right gives us a much higher probability than the model on the left. Now all we need to do is to maximize this probability. But probability is a product of numbers and products are hard. Maybe this product of four numbers doesn't look so scary. But what if we have thousands of datapoints? That would correspond to a product of thousands of numbers, all of them between zero and one. This product would be very tiny, something like 0.0000 something and we definitely want to stay away from those numbers. Also, if I have a product of thousands of numbers and I change one of them, the product will change drastically. In summary, we really want to stay away from products. And what's better than products? Well, let's ask our friend here. Products are bad, but sums are good. Let's do sums. So let's try to turn these products into sums. We need to find a function that will help us turn products into sums. What would this function be? It sounds like it's time for a quiz. Quiz. Which function will help us out here? Sine, cosine, logarithm or the exponential function? Enter your answer below.

### 20. Cross Entropy 1-iREoPUrpXvE.en

Correct. The answer is logarithm, because logarithm has this very nice identity that says that the logarithm of the product A times B is the sum of the logarithms of A and B. So this is what we do. We take our products and we take the logarithms, so now we get a sum of the logarithms of the factors. So the ln(0.6*0.2*0.1*0.7) is equal to ln(0.6) + ln(0.2) + ln(0.1) + ln(0.7) etc. Now from now until the end of class, we'll be taking the natural logarithm which is base e instead of 10. Nothing different happens with base 10. Everything works the same as everything gets scaled by the same factor. So it's just more for convention. We can calculate those values and get minus 0.51, minus 1.61, minus 0.23 etc. Notice that they are all negative numbers and that actually makes sense. This is because the logarithm of a number between 0 and 1 is always a negative number since the logarithm of one is zero. So it actually makes sense to think of the negative of the logarithm of the probabilities and we'll get positive numbers. So that's what we'll do. We'll take the negative of the logarithm of the probabilities. That sums up negatives of logarithms of the probabilities, we'll call the cross entropy which is a very important concept in the class. If we calculate the cross entropies, we see that the bad model on left has a cross entropy 4.8 which is high. Whereas the good model on the right has a cross entropy of 1.2 which is low. This actually happens all the time. A good model will give us a low cross entropy and a bad model will give us a high cross entropy. The reason for this is simply that a good model gives us a high probability and the negative of the logarithm of a large number is a small number and vice versa. This method is actually much more powerful than we think. If we calculate the probabilities and pair the points with the corresponding logarithms, we actually get an error for each point. So again, here we have probabilities for both models and the products of them. Now, we take the negative of the logarithms which gives us sum of logarithms and if we pair each logarithm with the point where it came from, we actually get a value for each point. And if we calculate the values, we get this. Check it out. If we look carefully at the values we can see that the points that are mis-classified has like values like 2.3 for this point or 1.6 one for this point, whereas the points that are correctly classified have small values. And the reason for this is again is that a correctly classified point will have a probability that as close to 1, which when we take the negative of the logarithm, we'll get a small value. Thus we can think of the negatives of these logarithms as errors at each point. Points that are correctly classified will have small errors and points that are mis-classified will have large errors. And now we've concluded that our cross entropy will tell us if a model is good or bad. So now our goal has changed from maximizing a probability to minimizing a cross entropy in order to get from the model in left to the model in the right. And that error function that we're looking for, that was precisely the cross entropy.

### 21. CrossEntropy V1-1BnhC6e0TFw.en

Let's look a bit closer into Cross-Entropy by switching to a different example. Let's say we have three doors. And no this is not the Monty Hall problem. We have the green door, the red door, and the blue door, and behind each door we could have a gift or not have a gift. And the probabilities of there being a gift behind each door is 0.8 for the first one, 0.7 for the second one, 0.1 for the third one. So for example behind the green door there is an 80 percent probability of there being a gift, and a 20 percent probability of there not being a gift. So we can put the information in this table where the probabilities of there being a gift are given in the top row, and the probabilities of there not being a gift are given in the bottom row. So let's say we want to make a bet on the outcomes. So we want to try to figure out what is the most likely scenario here. And for that we'll assume they're independent events. In this case, the most likely scenario is just obtained by picking the largest probability in each column. So for the first door is more likely to have a gift than not have a gift. So we'll say there's a gift behind the first door. For the second door, it's also more likely that there's a gift. So we'll say there's a gift behind the second door. And for the third door it's much more likely that there's no gift, so we'll say there's no gift behind the third door. And as the events are independent, the probability for this whole arrangement is the product of the three probabilities which is 0.8, times 0.7, times 0.9, which ends up being 0.504, which is roughly 50 percent. So let's look at all the possible scenarios in the table. Here's a table with all the possible scenarios for each door and there are eight scenarios since each door gives us two possibilities each, and there are three doors. So we do as before to obtain the probability of each arrangement by multiplying the three independent probabilities to get these numbers. You can check that these numbers add to one. And from last video we learned that the negative of the logarithm of the probabilities across entropy. So let's go ahead and calculate the cross-entropy. And notice that the events with high probability have low cross-entropy and the events with low probability have high cross-entropy. For example, the second row which has probability of 0.504 gives a small cross-entropy of 0.69, and the second to last row which is very very unlikely has a probability of 0.006 gives a cross entropy a 5.12. So let's actually calculate a formula for the cross-entropy. Here we have our three doors, and our sample scenario said that there is a gift behind the first and second doors, and no gift behind the third door. Recall that the probabilities of these events happening are 0.8 for a gift behind the first door, 0.7 for a gift behind the second door, and 0.9 for no gift behind the third door. So when we calculate the cross-entropy, we get the negative of the logarithm of the product, which is a sum of the negatives of the logarithms of the factors, which is negative logarithm of 0.8 minus logarithm of 0.7 minus logarithm 0.9. And in order to drive the formula we'll have some variables. So let's call P1 the probability that there's a gift behind the first door, P2 the probability there's a gift behind the second door, and P3 the probability there's a gift behind the third door. So this 0.8 here is P1, this 0.7 here is P2, and this 0.9 here is one minus P3. So it's a probability of there not being a gift is one minus the probability of there being a gift. Let's have another variable called Yi, which will be one of there's a present behind the ith door, and zero there's no present. So Yi is technically a number of presents behind the ith door. In this case Y1 equals one, Y2 equals one, and Y3 equals zero. So we can put all this together and derive a formula for the cross-entropy and it's this sum. Now let's look at the formula inside the summation. Noted that if there is a present behind the ith door, then Yi equals one. So the first term is logarithm of the Pi. And the second term is zero. Likewise, if there is no present behind the ith door, then Yi is zero. So this first term is zero. And this term is precisely logarithm of one minus Pi. Therefore, this formula really encompasses the sums of the negative of logarithms which is precisely the cross-entropy. So the cross-entropy really tells us when two vectors are similar or different. For example, if you calculate the cross entropy of the pair one one zero, and 0.8, 0.7, 0.1, we get 0.69. And that is low because one one zero is a similar vector to 0.8, 0.7, 0.1. Which means that the arrangement of gifts given by the first set of numbers is likely to happen based on the probabilities given by the second set of numbers. But on the other hand if we calculate the cross-entropy of the pairs zero zero one, and 0.8, 0.7, 0.1, that is 5.12 which is very high. This is because the arrangement of gifts being given by the first set of numbers is very unlikely to happen from the probabilities given by the second set of numbers.

### 21. Formula For Cross 1-qvr_ego_d6w.en

So this cross entropy, it looks like kind of a big deal. Cross entropy really says the following. If I have a bunch of events and a bunch of probabilities, how likely is it that those events happen based on the probabilities? If it's very likely, then we have a small cross entropy. If it's unlikely, then we have a large cross entropy. Let's elaborate.

### 22. DL 27 Multi-Class Cross Entropy 2 Fix-keDswcqkees.en

Now that was when we had two classes namely receiving a gift or not receiving a gift. What happens if we have more classes? Let's take a look. So we have a similar problem. We still have three doors. And this problem is still not the Monty Hall problem. Behind each door there can be an animal, and the animal can be of three types. It can be a duck, it can be a beaver, or it can be a walrus. So let's look at this table of probabilities. According to the first column on the table, behind the first door, the probability of finding a duck is 0.7, the probability of finding a beaver is 0.2, and the probability of finding a walrus is 0.1. Notice that the numbers in each column need to add to one because there is some animal behind door one. The numbers in the rows do not need to add to one as you can see. It could easly be that we have a duck behind every door and that's okay. So let's look at a sample scenario. Let's say we have our three doors, and behind the first door, there's a duck, behind the second door there's a walrus, and behind the third door there's also a walrus. Recall that the probabilities are again by the table. So a duck behind the first door is 0.7 likely, a walrus behind the second door is 0.3 likely, and a walrus behind the third door is 0.4 likely. So the probability of obtaining this three animals is the product of the probabilities of the three events since they are independent events, which in this case it's 0.084. And as we learn, that cross entropy here is given by the sums of the negatives of the logarithms of the probabilities. So the first one is negative logarithm of 0.7. The second one is negative logarithm of 0.3. And the third one is negative logarithm of 0.4. The Cross entropy's and the sum of these three which is actually 2.48. But we want a formula, so let's put some variables here. So P11 is the probability of finding a duck behind door one. P12 is the probability of finding a duck behind door two etc. And let's have the indicator variables Y1j D1 if there's a duck behind door J. Y2j B1 if there's a beaver behind door J, and Y3j B1 if there's a walrus behind door J. And these variables are zero otherwise. And so, the formula for the cross entropy is simply the negative of the summation from i_ equals_ one to n, up to summation from y_ equals_ j to m of Yij_ times_ the logarithm of Pij. In this case, m is a number of classes. This formula works because Yij being zero one, makes sure that we're only adding the logarithms of the probabilities of the events that actually have occurred. And voila, this is the formula for the cross entropy in more classes. Now I'm going to leave this equestion. Given that we have a formula for cross entropy for two classes and one for m classes. These formulas look different but are they the same for m_ equals_ two? Obviously the answer is yes, but it's a cool exercise to actually write them down and convince yourself that they are actually the same.

### 23. DL 29 Logistic Regression-Minimizing The Error Function-KayqiYijlzc.en

Okay. So now our goal is to minimize the error function and we'll do it as follows. We started some random weights, which will give us the predictions (Wx+b). As we saw, that also gives us a error function given by this formula. Remember that the summands are also error functions for each point. So each point will give us a larger function if it's mis-classified and a smaller one if it's correctly classified. And the way we're going to minimize this function, is to use gradient decent. So here's Mt. Errorest and this is us, and we're going to try to jiggle the line around to see how we can decrease the error function. Now, the error function is the height which is E(W,b), where W and b are the weights. Now what we'll do, is we'll use gradient decent in order to get to the bottom of the mountain at a much smaller height, which gives us a smaller error function E of W', b'. This will give rise to new weights, W' and b' which will give us a much better prediction. Namely,  (W'x+b').

### 23. Error Function-V5kkHldUlVU.en

So this is a good time for a quick recap of the last couple of lessons. Here we have two models. The bad model on the left and the good model on the right. And for each one of those we calculate the cross entropy which is the sum of the negatives of the logarithms off the probabilities of the points being their colors. And we conclude that the one on the right is better because a cross entropy is much smaller. So let's actually calculate the formula for the error function. Let's split into two cases. The first case being when y=1. So when the point is blue to begin with, the model tells us that the probability of being blue is the prediction y_hat. So for these two points the probabilities are 0.6 and 0.2. As we can see the point in the blue area has more probability of being blue than the point in the red area. And our error is simply the negative logarithm of this probability. So it's precisely minus logarithm of y_hat. In the figure it's minus logarithm of 0.6. and minus logarithm of 0.2. Now if y=0, so when the point is red, then we need to calculate the probability of the point being red. The probability of the point being red is one minus the probability of the point being blue which is precisely 1 minus the prediction y_hat. So the error is precisely the negative logarithm of this probability which is negative logarithm of 1 - y_hat. In this case we get negative logarithm 0.1 and negative logarithm 0.7. So we conclude that the error is a negative logarithm of y_hat if the point is blue. And negative logarithm of one - y_hat the point is red. We can summarize these two formulas into this one. Error = - (1-y)(ln( 1- y_hat)) - y ln(y_hat). Why does this formula work? Well because if the point is blue, then y=1 which means 1-y=0 which makes the first term 0 and the second term is simply logarithm of y_hat. Similarly, if the point is red then y=0. So the second term of the formula is 0 and the first one is logarithm of 1- y_hat. Now the formula for the error function is simply the sum over all the error functions of points which is precisely the summation here. That's going to be this 4.8 we have over here. Now by convention we'll actually consider the average, not the sum which is where we are dividing by n over here. This will turn the 4.8 into a 1.2. From now on we'll use this formula as our error function. And now since y_hat is given by the sigmoid of the linear function wx + b, then the total formula for the error is actually in terms of w and b which are the weights of the model. And it's simply the summation we see here. In this case y_i is just the label of the point x_superscript_i. So now that we've calculated it our goal is to minimize it. And that's what we'll do next. And just a small aside, what we did is for binary classification problems. If we have a multiclass classification problem then the error is now given by the multiclass entropy. This formula is given here where for every data point we take the product of the label times the logarithm of the prediction and then we average all these values. And again it's a nice exercise to convince yourself that the two are the same when there are just two classes.

### 24. Gradient Descent-rhVIF-nigrY.en

So let's study gradient descent in more mathematical detail. Our function is a function of the weights and it can be graph like this. It's got a mathematical structure so it's not Mt. Everest anymore, it's more of a mount Math-Er-Horn. So we're standing somewhere in Mount Math-Er-Horn and we need to go down. So now the inputs of the functions are W1 and W2 and the error function is given by E. Then the gradient of E is given by the vector sum of the partial derivatives of E with respect to W1 and W2. This gradient actually tells us the direction we want to move if we want to increase the error function the most. Thus, if we take the negative of the gradient, this will tell us how to decrease the error function the most. And this is precisely what we'll do. At the point we're standing, we'll take the negative of the gradient of the error function at that point. Then we take a step in that direction. Once we take a step, we'll be in a lower position. So we do it again, and again, and again, until we are able to get to the bottom of the mountain. So this is how we calculate the gradient. We start with our initial prediction Y had equals sigmoid of W Expo's B. And let's say this prediction is bad because the error is large since we're high up in the mountain. The prediction looks like this, Y had equal sigmoid of W 1 x 1 plus all the way to WnXn plus b. Now the error function is given by the formula we saw before. But what matters here is the gradient of the error function. The gradient of the error function is precisely the vector formed by the partial derivative of the error function with respect to the weights and the bias. Now, we take a step in the direction of the negative of the gradient. As before, we don't want to make any dramatic changes, so we'll introduce a smaller learning rate alpha. For example, 0.1. And we'll multiply the gradient by that number. Now taking the step is exactly the same thing as updating the weights and the bias as follows. The weight Wi will now become Wi prime. Given by Wi minus alpha times the partial derivative of the error, with respect to Wi. And the bias will now become b prime given by b minus alpha times partial derivative of the error with respect to b. Now this will take us to a prediction with a lower error function. So, we can conclude that the prediction we have now with weights W prime b prime, is better than the one we had before with weights W and b. This is precisely the gradient descent step.

### 25. Gradient Descent Algorithm-snxmBgi_GeU.en

And now we finally have the tools to write the pseudocode for the grading descent algorithm, and it goes like this. Step one, start with random weights w_one up to w_n and b which will give us a line, and not just a line, but the whole probability function given by sigmoid of w x plus b. Now for every point we'll calculate the error, and as we can see the error is high for misclassified points and small for correctly classified points. Now for every point with coordinates x_one up to x_n, we update w_i by adding the learning rate alpha times the partial derivative of the error function with respect to w_i. We also update b by adding alpha times the partial derivative of the error function with respect to be. This gives us new weights, w_i_prime and then new bias b_prime. Now we've already calculated these partial derivatives and we know that they are y_hat minus y times x_i for the derivative with respect to w_i and y_hat minus y for the derivative with respect to b. So that's how we'll update the weights. Now repeat this process until the error is small, or we can repeat it a fixed number of times. The number of times is called the epochs and we'll learn them later. Now this looks familiar, have we seen something like that before? Well, we look at the points and what each point is doing is it's adding a multiple of itself into the weights of the line in order to get the line to move closer towards it if it's misclassified. That's pretty much what the Perceptron algorithm is doing. So in the next video, we'll look at the similarities because it's a bit suspicious how similar they are.

### 28. Gradient Descent Vs Perceptron Algorithm-uL5LuRPivTA.en

So let's compare the Perceptron algorithm and the Gradient Descent algorithm. In the Gradient Descent algorithm, we take the weights and change them from Wi to Wi_ plus_ alpha_ times_ Y hat_ minus_ Y_ times_ Xi. In the Perceptron algorithm, not every point changes weights, only the misclassified ones. Here, if X is misclassified, we'll change the weights by adding Xi to Wi if the point label is positive, and subtracting if negative. Now the question is, are these two things the same? Well, let's remember that in that Perceptron algorithm, the labels are one and zero. And the predictions Y-hat are also one and zero. So, if the point is correct, classified, then Y_ minus_ Y-hat is zero because Y is equal to Y-hat. Now, if the point is labeled blue, then Y_ equals_ one. And if it's misclassified, then the prediction must be Y-hat_ equals_ zero. So Y-hat_ minus_ Y is minus one. Similarly, with the points labeled red, then Y_ equals_ zero and Y-hat_ equals_ one. So, Y-hat_ minus_ Y_ equals_ one. This may not be super clear right away. But if you stare at the screen for long enough, you'll realize that the right and the left are exactly the same thing. The only difference is that in the left, Y-hat can take any number between zero and one, whereas in the right, Y-hat can take only the values zero or one. It's pretty fascinating, isn't it? But let's study Gradient Descent even more carefully. Both in the Perceptron algorithm and the Gradient Descent algorithm, a point that is misclassified tells a line to come closer because eventually, it wants the line to surpass it so it can be in the correct side. Now, what happens if the point is correctly classified? Well, the Perceptron algorithm says do absolutely nothing. In the Gradient Descent algorithm, you are changing the weights. But what is it doing? Well, if we look carefully, what the point is telling the line, is to go farther away. And this makes sense, right? Because if you're correctly classified, say, if you're a blue point in the blue region, you'd like to be even more into the blue region, so your prediction is even closer to one, and your error is even smaller. Similarly, for a red point in the red region. So it makes sense that the point tells the line to go farther away. And that's precisely what the Gradient Descent algorithm does. The misclassified points asks the line to come closer and the correctly classified points asks the line to go farther away. The line listens to all the points and takes steps in such a way that it eventually arrives to a pretty good solution.

### 29. Continuous Perceptrons-07-JJ-aGEfM.en

So, this is just a small recap video that will get us ready for what's coming. Recall that if we have our data in the form of these points over here and the linear model like this one, for example, with equation 2x1 + 7x2 - 4 = 0, this will give rise to a probability function that looks like this. Where the points on the blue or positive region have more chance of being blue and the points in the red or negative region have more chance of being red. And this will give rise to this perception where we label the edges by the weights and the node by the bias. So, what the perception does, it takes to point (x1, x2), plots it in the graph and then it returns a probability that the point is blue. In this case, it returns a 0.9 and this mimics the neurons in the brain because they receive nervous impulses, do something inside and return a nervous impulse.

### 30. Non-Linear Data-F7ZiE8PQiSc.en

Now we've been dealing a lot with data sets that can be separated by a line, like this one over here. But as you can imagine the real world is much more complex than that. This is where neural networks can show their full potential. In the next few videos we'll see how to deal with more complicated data sets that require highly non-linear boundaries such as this one over here.

### 31. Non-Linear Models-HWuBKCZsCo8.en

So, let's go back to this example of where we saw some data that is not linearly separable. So a line can not divide these red and blue points and we looked at some solutions, and if you remember, the one we considered more seriously was this curve over here. So what I'll teach you now is to find this curve and it's very similar than before. We'll still use grading dissent. In a nutshell, what we're going to do is for these data which is not separable with a line, we're going to create a probability function where the points in the blue region are more likely to be blue and the points in the red region are more likely to be red. And this curve here that separates them is a set of points which are equally likely to be blue or red. Everything will be the same as before except this equation won't be linear and that's where neural networks come into play.

### 32. 29 Neural Network Architecture 2-FWN3Sw5fFoM.en

So in the previous session we learn that we can add to linear models to obtain a third model. As a matter of fact, we did even more. We can take a linear combination of two models. So, the first model times a constant plus the second model times a constant plus a bias and that gives us a non-linear model. That looks a lot like perceptrons where we can take a value times a constant plus another value times a constant plus a bias and get a new value. And that's no coincidence. That's actually the building block of Neural Networks. So, let's look at an example. Let's say, we have this linear model where the linear equation is 5x1 minus 2x2 plus 8. That's represented by this perceptron. And we have another linear model with equations 7x1 minus 3x2 minus 1 which is represented by this perceptron over here. Let's draw them nicely in here and let's use another perceptron to combine these two models using the Linear Equation, seven times the first model plus five times the second model minus six. And now the magic happens when we join these together and we get a Neural Network. We clean it up a bit and we obtain this. All the weights are there. The weights on the left, tell us what equations the linear models have. And the weights on the right, tell us what the linear combination is of the two models to obtain the curve non-linear model in the right. So, whenever you see a Neural Network like the one on the left, think of what could be the nonlinear boundary defined by the Neural Network. Now, note that this was drawn using the notation that puts a bias inside the node. This can also be drawn using the notation that keeps the bias as a separate node. Here, what we do is, in every layer we have a bias unit coming from a node with a one on it. So for example, the minus eight on the top node becomes an edge labelled minus eight coming from the bias node. We can see that this Neural Network uses a Sigmoid Activation Function and the Perceptrons.

### 32. Combinando modelos-Boy3zHVrWB4.en

Now I'm going to show you how to create these nonlinear models. What we're going to do is a very simple trick. We're going to combine two linear models into a nonlinear model as follows. Visually it looks like this. The two models over imposed creating the model on the right. It's almost like we're doing arithmetic on models. It's like saying "This line plus this line equals that curve." Let me show you how to do this mathematically. So a linear model as we know is a whole probability space. This means that for every point it gives us the probability of the point being blue. So, for example, this point over here is in the blue region so its probability of being blue is 0.7. The same point given by the second probability space is also in the blue region so it's probability of being blue is 0.8. Now the question is, how do we combine these two? Well, the simplest way to combine two numbers is to add them, right? So 0.8 plus 0.7 is 1.5. But now, this doesn't look like a probability anymore since it's bigger than one. And probabilities need to be between 0 and 1. So what can we do? How do we turn this number that is larger than 1 into something between 0 and 1? Well, we've been in this situation before and we have a pretty good tool that turns every number into something between 0 and 1. That's just a sigmoid function. So that's what we're going to do. We applied the sigmoid function to 1.5 to get the value 0.82 and that's the probability of this point being blue in the resulting probability space. So now we've managed to create a probability function for every single point in the plane and that's how we combined two models. We calculate the probability for one of them, the probability for the other, then add them and then we apply the sigmoid function. Now, what if we wanted to weight this sum? What, if say, we wanted the model in the top to have more of a saying the resulting probability than the second? So something like this where the resulting model looks a lot more like the one in the top then like the one in the bottom. Well, we can add weights. For example, we can say "I want seven times the first model plus the second one." Actually, I can add the weights since I want. For example, I can say "Seven times the first one plus five times the second one." And when I do get the combine the model is I take the first probability, multiply it by seven, then take the second one and multiply it by five and I can even add a bias if I want. Say, the bias is minus 6, then we add it to the whole equation. So we'll have seven times this plus five times this minus six, which gives us 2.9. We then apply the sigmoid function and that gives us 0.95. So it's almost like we had before, isn't it? Before we had a line that is a linear combination of the input values times the weight plus a bias. Now we have that this model is a linear combination of the two previous model times the weights plus some bias. So it's almost the same thing. It's almost like this curved model in the right. It's a linear combination of the two linear models before or we can even think of it as the line between the two models. This is no coincidence. This is at the heart of how neural networks get built. Of course, we can imagine that we can keep doing this always obtaining more new complex models out of linear combinations of the existing ones. And this is what we're going to do to build our neural networks.

### 32. Layers-pg99FkXYK0M.en

Neural networks have a certain special architecture with layers. The first layer is called the input layer, which contains the inputs, in this case, x1 and x2. The next layer is called the hidden layer, which is a set of linear models created with this first input layer. And then the final layer is called the output layer, where the linear models get combined to obtain a nonlinear model. You can have different architectures. For example, here's one with a larger hidden layer. Now we're combining three linear models to obtain the triangular boundary in the output layer. Now what happens if the input layer has more nodes? For example, this neural network has three nodes in its input layer. Well, that just means we're not living in two-dimensional space anymore. We're living in three-dimensional space, and now our hidden layer, the one with the linear models, just gives us a bunch of planes in three space, and the output layer bounds a nonlinear region in three space. In general, if we have n nodes in our input layer, then we're thinking of data living in n-dimensional space. Now what if our output layer has more nodes? Then we just have more outputs. In that case, we just have a multiclass classification model. So if our model is telling us if an image is a cat or dog or a bird, then we simply have each node in the output layer output a score for each one of the classes: one for the cat, one for the dog, and one for the bird. And finally, and here's where things get pretty cool, what if we have more layers? Then we have what's called a deep neural network. Now what happens here is our linear models combine to create nonlinear models and then these combine to create even more nonlinear models. In general, we can do this many times and obtain highly complex models with lots of hidden layers. This is where the magic of neural networks happens. Many of the models in real life, for self-driving cars or for game-playing agents, have many, many hidden layers. That neural network will just split the n-dimensional space with a highly nonlinear boundary, such as maybe the one on the right.

### 32. Multiclass Classification-uNTtvxwfox0.en

We briefly mentioned multi-class classification in the last video but let me be more specific. It seems that neural networks work really well when the problem consist on classifying two classes. For example, if the model predicts a probability of receiving a gift or not then the answer just comes as the output of the neural network. But what happens if we have more classes? Say, we want the model to tell us if an image is a duck, a beaver, or a walrus. Well, one thing we can do is create a neural network to predict if the image is a duck, then another neural network to predict if the image is a beaver, and a third neural network to predict if the image is a walrus. Then we can just use SoftMax or pick the answer that gives us the highest probability. But this seems like overkill, right? The first layers of the neural network should be enough to tell us things about the image and maybe just the last layer should tell us which animal it is. As a matter of fact, as you'll see in the CNN section, this is exactly the case. So what we need here is to add more nodes in the output layer and each one of the nodes will give us the probability that the image is each of the animals. Now, we take the scores and apply the SoftMax function that was previously defined to obtain well-defined probabilities. This is how we get neural networks to do multi-class classification.

### 33. DL 41 Feedforward FIX V2-hVCuvMGOfyY.en

So now that we have defined what neural networks are, we need to learn how to train them. Training them really means what parameters should they have on the edges in order to model our data well. So in order to learn how to train them, we need to look carefully at how they process the input to obtain an output. So let's look at our simplest neural network, a perceptron. This perceptron receives a data point of the form x1, x2 where the label is Y=1. This means that the point is blue. Now the perceptron is defined by a linear equation say w1, x1 plus w2, x2 plus B, where w1 and w2 are the weights in the edges and B is the bias in the note. Here, w1 is bigger than w2, so we'll denote that by drawing the edge labelled w1 much thicker than the edge labelled w2. Now, what the perceptron does is it plots the point x1, x2 and it outputs the probability that the point is blue. Here is the point is in the red area and then the output is a small number, since the point is not very likely to be blue. This process is known as feedforward. We can see that this is a bad model because the point is actually blue. Given that the third coordinate, the Y is one. Now if we have a more complicated neural network, then the process is the same. Here, we have thick edges corresponding to large weights and thin edges corresponding to small weights and the neural network plots the point in the top graph and also in the bottom graph and the outputs coming out will be a small number from the top model. The point lies in the red area which means it has a small probability of being blue and a large number from the second model, since the point lies in the blue area which means it has a large probability of being blue. Now, as the two models get combined into this nonlinear model and the output layer just plots the point and it tells the probability that the point is blue. As you can see, this is a bad model because it puts the point in the red area and the point is blue. Again, this process called feedforward and we'll look at it more carefully. Here, we have our neural network and the other notations so the bias is in the outside. Now we have a matrix of weights. The matrix w superscript one denoting the first layer and the entries are the weights w1, 1 up to w3, 2. Notice that the biases have now been written as w3, 1 and w3, 2 this is just for convenience. Now in the next layer, we also have a matrix this one is w superscript two for the second layer. This layer contains the weights that tell us how to combine the linear models in the first layer to obtain the nonlinear model in the second layer. Now what happens is some math. We have the input in the form x1, x2, 1 where the one comes from the bias unit. Now we multiply it by the matrix w1 to get these outputs. Then, we apply the sigmoid function to turn the outputs into values between zero and one. Then the vector format these values gets a one attatched for the bias unit and multiplied by the second matrix. This returns an output that now gets thrown into a sigmoid function to obtain the final output which is y-hat. Y-hat is the prediction or the probability that the point is labeled blue. So this is what neural networks do. They take the input vector and then apply a sequence of linear models and sigmoid functions. These maps when combined become a highly non-linear map. And the final formula is simply y-hat equals sigmoid of w2 combined with sigmoid of w1 applied to x. Just for redundance, we do this again on a multi-layer perceptron or neural network. To calculate our prediction y-hat, we start with the unit vector x, then we apply the first matrix and a sigmoid function to get the values in the second layer. Then, we apply the second matrix and another sigmoid function to get the values on the third layer and so on and so forth until we get our final prediction, y-hat. And this is the feedforward process that the neural networks use to obtain the prediction from the input vector.

### 33. DL 42 Neural Network Error Function (1)-SC1wEW7TtKs.en

So, our goal is to train our neural network. In order to do this, we have to define the error function. So, let's look again at what the error function was for perceptrons. So, here's our perceptron. In the left, we have our input vector with entries x_1 up to x_n, and one for the bias unit. And the edges with weights W_1 up to W_n, and b for the bias unit. Finally, we can see that this perceptor uses a sigmoid function. And the prediction is defined as y-hat equals sigmoid of Wx plus b. And as we saw, this function gives us a measure of the error of how badly each point is being classified. Roughly, this is a very small number if the point is correctly classified, and a measure of how far the point is from the line and the point is incorrectly classified. So, what are we going to do to define the error function in a multilayer perceptron? Well, as we saw, our prediction is simply a combination of matrix multiplications and sigmoid functions. But the error function can be the exact same thing, right? It can be the exact same formula, except now, y-hat is just a bit more complicated. And still, this function will tell us how badly a point gets misclassified. Except now, it's looking at a more complicated boundary.

### 34. Backpropagation V2-1SmY3TZTyUk.en

So now we're finally ready to get our hands into training a neural network. So let's quickly recall feedforward. We have our perceptron with a point coming in labeled positive. And our equation w1x1 + w2x2 + b, where w1 and w2 are the weights and b is the bias. Now, what the perceptron does is, it plots a point and returns a probability that the point is blue. Which in this case is small since the point is in the red area. Thus, this is a bad perceptron since it predicts that the point is red when the point is really blue. And now let's recall what we did in the gradient descent algorithm. We did this thing called Backpropagation. We went in the opposite direction. We asked the point, "What do you want the model to do for you?" And the point says, "Well, I am misclassified so I want this boundary to come closer to me." And we saw that the line got closer to it by updating the weights. Namely, in this case, let's say that it tells the weight w1 to go lower and the weight w2 to go higher. And this is just an illustration, it's not meant to be exact. So we obtain new weights, w1' and w2' which define a new line which is now closer to the point. So what we're doing is like descending from Mt. Errorest, right? The height is going to be the error function E(W) and we calculate the gradient of the error function which is exactly like asking the point what does is it want the model to do. And as we take the step down the direction of the negative of the gradient, we decrease the error to come down the mountain. This gives us a new error, E(W') and a new model W' with a smaller error, which means we get a new line closer to the point. We continue doing this process in order to minimize the error. So that was for a single perceptron. Now, what do we do for multi-layer perceptrons? Well, we still do the same process of reducing the error by descending from the mountain, except now, since the error function is more complicated then it's not Mt. Errorest, now it's Mt. Kilimanjerror. But same thing, we calculate the error function and its gradient. We then walk in the direction of the negative of the gradient in order to find a new model W' with a smaller error E(W') which will give us a better prediction. And we continue doing this process in order to minimize the error. So let's look again at what feedforward does in a multi-layer perceptron. The point comes in with coordinates (x1, x2) and label y = 1. It gets plotted in the linear models corresponding to the hidden layer. And then, as this layer gets combined the point gets plotted in the resulting non-linear model in the output layer. And the probability that the point is blue is obtained by the position of this point in the final model. Now, pay close attention because this is the key for training neural networks, it's Backpropagation. We'll do as before, we'll check the error. So this model is not good because it predicts that the point will be red when in reality the point is blue. So we'll ask the point, "What do you want this model to do in order for you to be better classified?" And the point says, "I kind of want this blue region to come closer to me." Now, what does it mean for the region to come closer to it? Well, let's look at the two linear models in the hidden layer. Which one of these two models is doing better? Well, it seems like the top one is badly misclassifying the point whereas the bottom one is classifying it correctly. So we kind of want to listen to the bottom one more and to the top one less. So what we want to do is to reduce the weight coming from the top model and increase the weight coming from the bottom model. So now our final model will look a lot more like the bottom model than like the top model. But we can do even more. We can actually go to the linear models and ask the point, "What can these models do to classify you better?" And the point will say, "Well, the top model is misclassifying me, so I kind of want this line to move closer to me. And the second model is correctly classifying me, so I want this line to move farther away from me." And so this change in the model will actually update the weights. Let's say, it'll increase these two and decrease these two. So now after we update all the weights we have better predictions at all the models in the hidden layer and also a better prediction at the model in the output layer. Notice that in this video we intentionally left the bias unit away for clarity. In reality, when you update the weights we're also updating the bias unit. If you're the kind of person who likes formality, don't worry, we'll calculate these gradients in detail soon.

### 34. Calculating The Gradient 1 -tVuZDbUrzzI.en

Okay. So, now we'll do the same thing as we did before, painting our weights in the neural network to better classify our points. But we're going to do it formally, so fasten your seat belts because math is coming. On your left, you have a single perceptron with the input vector, the weights and the bias and the sigmoid function inside the node. And on the right, we have a formula for the prediction, which is the sigmoid function of the linear function of the input. And below, we have a formula for the error, which is the average of all points of the blue term for the blue points and the red term for the red points. And in order to descend from Mount Errorest, we calculate the gradient. And the gradient is simply the vector formed by all the partial derivatives of the error function with respect to the weights w1 up to wn and and the bias b. They correspond to these edges over here, and what do we do in a multilayer perceptron? Well, this time it's a little more complicated but it's pretty much the same thing. We have our prediction, which is simply a composition of functions namely matrix multiplications and sigmoids. And the error function is pretty much the same, except the  is a bit more complicated. And the gradient is pretty much the same thing, it's just much, much longer. It's a huge vector where each entry is a partial derivative of the error with respect to each of the weights. And these just correspond to all the edges. If we want to write this more formally, we recall that the prediction is a composition of sigmoids and matrix multiplications, where these are the matrices and the gradient is just going to be formed by all these partial derivatives. Here, it looks like a matrix but in reality, it's just a long vector. And the gradient descent is going to do the following; we take each weight, w_i_j super k and we update it by adding a small number, the learning rate times the partial derivative of E with respect to that same weight. This is the gradient descent step, so it will give us new updated weight w_i_j super k prime. That step is going to give us a whole new model with new weights that will classify the point much better.

### 34. Chain Rule-YAhIBOnbt54.en

So before we start calculating derivatives, let's do a refresher on the chain rule which is the main technique we'll use to calculate them. The chain rule says, if you have a variable x on a function f that you apply to x to get f of x, which we're gonna call A, and then another function g, which you apply to f of x to get g of f of x, which we're gonna call B, the chain rule says, if you want to find the partial derivative of B with respect to x, that's just a partial derivative of B with respect to A times the partial derivative of A with respect to x. So it literally says, when composing functions, that derivatives just multiply, and that's gonna be super useful for us because feed forwarding is literally composing a bunch of functions, and back propagation is literally taking the derivative at each piece, and since taking the derivative of a composition is the same as multiplying the partial derivatives, then all we're gonna do is multiply a bunch of partial derivatives to get what we want. Pretty simple, right?

### 34. DL 46 Calculating The Gradient 2 V2 (2)-7lidiTGIlN4.en

So, let us go back to our neural network with our weights and our input. And recall that the weights with superscript 1 belong to the first layer, and the weights with superscript 2 belong to the second layer. Also, recall that the bias is not called b anymore. Now, it is called W31, W32 etc. for convenience, so that we can have everything in matrix notation. And now what happens with the input? So, let us do the feedforward process. In the first layer, we take the input and multiply it by the weights and that gives us h1, which is a linear function of the input and the weights. Same thing with h2, given by this formula over here. Now, in the second layer, we would take this h1 and h2 and the new bias, apply the sigmoid function, and then apply a linear function to them by multiplying them by the weights and adding them to get a value of h. And finally, in the third layer, we just take a sigmoid function of h to get our prediction or probability between 0 and 1, which is . And we can read this in more condensed notation by saying that the matrix corresponding to the first layer is W superscript 1, the matrix corresponding to the second layer is W superscript 2, and then the prediction we had is just going to be the sigmoid of W superscript 2 combined with the sigmoid of W superscript 1 applied to the input x and that is feedforward. Now, we are going to develop backpropagation, which is precisely the reverse of feedforward. So, we are going to calculate the derivative of this error function with respect to each of the weights in the labels by using the chain rule. So, let us recall that our error function is this formula over here, which is a function of the prediction . But, since the prediction is a function of all the weights wij, then the error function can be seen as the function on all the wij. Therefore, the gradient is simply the vector formed by all the partial derivatives of the error function E with respect to each of the weights. So, let us calculate one of these derivatives. Let us calculate derivative of E with respect to W11 superscript 1. So, since the prediction is simply a composition of functions and by the chain rule, we know that the derivative with respect to this is the product of all the partial derivatives. In this case, the derivative E with respect to W11 is the derivative of either respect to  times the derivative  with respect to h times the derivative h with respect to h1 times the derivative h1 with respect to W11. This may seem complicated, but the fact that we can calculate a derivative of such a complicated composition function by just multiplying 4 partial derivatives is remarkable. Now, we have already calculated the first one, the derivative of E with respect to . And if you remember, we got  minus y. So, let us calculate the other ones. Let us zoom in a bit and look at just one piece of our multi-layer perceptron. The inputs are some values h1 and h2, which are values coming in from before. And once we apply the sigmoid and a linear function on h1 and h2 and 1 corresponding to the biased unit, we get a result h. So, now what is the derivative of h with respect to h1? Well, h is a sum of three things and only one of them contains h1. So, the second and the third summon just give us a derivative of 0. The first summon gives us W11 superscript 2 because that is a constant, and that times the derivative of the sigmoid function with respect to h1. This is something that we calculated below in the instructor comments, which is that the sigmoid function has a beautiful derivative, namely the derivative of sigmoid of h is precisely sigmoid of h times 1 minus sigmoid of h. Again, you can see this development underneath in the instructor comments. You also have the chance to code this in the quiz because at the end of the day, we just code these formulas and then use them forever, and that is it. That is how you train a neural network.



## Part 02-Module 02-Lesson 02_Deep Learning with PyTorch

### 04. PyTorch V2 Part 1 V1-6Z7WntXays8.en

Hello everyone and welcome to this lesson on deep learning with PyTorch. So, in this lesson I'm going to be showing you how we can build neural networks with pyTorch and train them. By working through all these notebooks I built, you'll be writing the actual code yourself for building these networks. By the end of the lesson, you will have built your own state of the art image classifier. But first we're going to start with basics, so how do you build just a simple neural network in pyTorch? So, as a reminder of how neural networks work, in general we have some input values so here x1, x2, and we multiply them by some weights w and bias. So, this b is this bias we just multiply it by one then you sum all these up and you get some value h. Then we have what's called an activation function. So, here f of h and passing these input values h through this activation function gets you output y. This is the basis of neural networks. You have these inputs, you multiply it by some waves, take the sum, pass it through some activation function and you get an output. You can stack these up so that the output of these units, of these neurons go to another layer like another set of weights. So, mathematically this what it looks like, y, our output is equal to this linear combination of the weights and the input values w's and x's plus your bias value b passes through your activation function f and you get y. You could also write it with this sum. So, sum of wi times xi and plus b, your bias term. That gives you y. So, what's nice about this is that you can actually think of the x's, your input features, your values, as a vector, and your weights as another vector. So, your multiplication and sum is the same as a dot or inner product of two vectors. So, if you consider your input as a vector and your weights as a vector, if you take the dot product of these two, then you get your value h and then you pass h through your activation function and that gets you your output y. So, now if we start thinking of our weights and our input values as vectors, so vectors are an instance of a tensor. So, a tensor is just a generalization of vectors and matrices. So, when you have these like regular structured arrangements of values and so a tensor with only one dimension is a vector. So, we just have this single one-dimensional array of values. So, in this case characters T-E-N-S-O-R. A matrix like this is a two-dimensional tensor and so we have values going in two directions from left to right and from top to bottom and so that we have individual rows and columns. So, you can do operations across the columns like along a row or you can do it across the rows like going down a column. You also have three-dimensional tensors so you can think of an image like an RGB color image as a three-dimensional tensor. So, for every pixel, there's some value for all the red and the green and the blue channels and so for every individual pixel, in a two-dimensional image, you have three values. So, that is a three-dimensional tensor. Like I said before, tensors are a generalization of this so you can actually have four-dimensional, five-dimensional, six-dimensional, and so on like tensors. It's just the ones that we normally work with are one and two-dimensional, three-dimensional tensors. So, these tensors are the base data structure that you use an pyTorch and other neural network frameworks. So, TensorFlow is named after tensors. So, these are the base data structures that you'll be using so you pretty much need to understand them really well to be able to use pretty much any framework that you'll be using for deep learning. So, let's get started. I'm going to show you how to actually create some tensors and use them to build a simple neural network. So, first we're going to import pyTorch and so just import torch here. Here I am creating activation function, so this is the Sigmoid activation function. It's the nice s shape that kind of squeezes the input values between zero and one. It's really useful for providing a probability. So, probabilities are these values that can only be between zero and one. So, you're Sigmoid activation if you want the output of your neural network to be a probability, then the sigmoid activation is what you want to use. So, here I'm going to create some fake data, I'm generating some data, I'm generating some weights and biases and with these you're actually going to do the computations to get the output of a simple neural network. So, here I'm just creating a manual seeds. So, I'm setting the seed for the random number generation that I'll be using and here I'm creating features. So, features are like the input features of the input data for your network. Here we see torch.randn. So, randn is going to create a tensor of normal variables. So, random normal variables as samples from a normal distribution. You give it a tuple of the size that you want. So, in this case I want the features to be a matrix, a 2-dimensional tensor of one row and five columns. So, you can think of this as a row vector that has five elements. For the weights, we're going to create another matrix of random normal variables and this time I'm using randn_like. So, what this does is it takes another tensor and it looks at the shape of this tensor and then it creates it, it creates another tensor with the same shape. So, that's what this this like means. So, I'm going to create a tensor of random normal variables with the same shape as features. So, it gives me my weights. Then I'm going to create a bias term. So, this is again just a random normal variable. Now I'm just creating one value. So, this is one row and one column. Here I'm going to leave this exercise up to you. So, what you're going to be doing is taking the features, weights, and the bias tensors and you're going to calculate the output of this simple neural network. So, remember with features and weights you want to take the inner product or you want to multiply the features by the weights and sum them up and then add the bias and then pass it through the activation function and from that you should get the output of your network. So, if you want to see how I did this, checkout my solution notebook or watch the next video which I'll show you my solution for this exercise.

### 05. PyTorch V2 Part 1 Solution V1-mNJ8CujTtpo.en

So now, this is my solution for this exercise on calculating the output of this small simple neural network. So, remember that what we want to do is multiply our features by our weights, so features times weights. So these tensors, they work basically the same as NumPy arrays, if you've used NumPy before. So, when you multiply features times weights, it'll just take the first element from each one, multiply them together, take the second element and multiply them together and so on and give you back a new tensor, where there's element by element multiplication. So, from that we can do torch.sum to sum it all up into one value, add our bias term and then pass it through the activation function and then we get Y. So, we can also do this where we do features times weights again, and this creates another tensor, but tensors have a method.sum, where you just take a tensor do.sum and then it sums up all the values in that tensor. So, we can either do it this way or we do torch.sum, or we can just take this method, this sum method of a tensor and some upper values that way. Again, pass it through our our activation function. So, here what we're doing, we're doing this element wise multiplication and taking the sum in two separate operations. We're doing this multiplication and then we're doing the sum. But you can actually do this in the same operation using matrix multiplication. So, in general, you're going to be wanting to use matrix multiplications most of the time, since they're the more efficient and these linear algebra operations have been accelerated using modern libraries, such as CUDA that run on GPUs. To do matrix multiplication in PyTorch with our two tensors features and weights, we can use one of two methods. So, either torch.mm or torch.matmul. So, torch.mm, so matrix multiplication is more simple and more strict about the tensors that you pass in. So, torch.matmul, it actually supports broadcasting. So, if you put in tensors that have weird sizes, weird shapes, then you could get an output that you're not expecting. So, what I tend to use torch.mm more often, so that it does what I expect basically, and then if I get something wrong it's going throw an error instead of just doing it and continuing the calculations. So, however, if we actually try to use torch.mm with features and weights, we'll get an error. So, here we see RuntimeError, size mismatch. So, what this means is that we passed in our two tensors to torch.mm, but there's a mismatch in the sizes and it can't actually do the matrix multiplication and it lists out the sizes here. So, the first tensor, M1 is one by five and the second tensor is one by five also. So, if you remember from your linear algebra classes or if you studied it recently, when you're doing matrix multiplication, the first matrix has to have a number of columns that's equal to the number of rows in the second matrix. So, really what we need is we need our weights tensor, our weights matrix to be five by one instead of one by five. To checkout the shape of tensors, as you're building your networks, you want to use tensor.shape. So, this is something you're going to be using all the time in PyTorch, but also in TensorFlow and in other deep learning frameworks So, most of the errors you're going to see when you're building networks and just a lot of the difficulty when it comes to designing the architecture of neural networks is getting the shapes of your tensors to work right together. So, what that means is that a large part of debugging, you're actually going to be trying to look at the shape of your tensors as they're going through your network. So, remember this, tensor.shape. So, for reshaping tensors, there are three, in general, three different options to choose from. So, we have these methods; reshape, resize, and view. The way these all work, in general, is that you take your tensor weights.reshape and then pass in the new shape that you want. So, in this case, you want to change our weights to be a five by one matrix, so we'd say.reshape and then five comma one. So, reshape here, what it will do is it's going to return a new tensor with the same data as weights. So, the same data that's sitting in memory at those addresses in memory. So, it's going to basically just create a new tensor that has the shape that you requested, but the actual data in memory isn't being changed. But that's only sometimes. Sometimes it does return a clone and what that means is that it actually copies the data to another part of memory and then returns you a tensor on top of that part of the memory. As you can imagine when it actually does that, when it's copying the data that's less efficient than if you had just changed the shape of your tensor without cloning the data. To do something like that, we can use resize, where there's underscore at the end. The underscore means that this method is an in-place operation. So, when it's in-place, that basically means that you're just not touching the data at all and all you're doing is changing the tensor that's sitting on top of that addressed data in memory. The problem with the resize method is that if you request a shape that has more or less elements than the original tensor, then you can actually cut off, you can actually lose some of the data that you had or you can create this spurious data from uninitialized memory. So instead, what you typically want is that you want a method that's going to return an error if you changed the shape from the original number of elements to a different number of elements. So, we can actually do that with.view. So.view is the one that I use the most, and basically what it does it just returns a new tensor with the same data in memory as weights. This is just all the time, 100 percent of the time, all it's going to do is return a new tensor without messing with any of the data in memory. If you tried to get a new size, a new shape for your tensor with a different number of elements, it'll return an error. So, you are basically using.view, you're ensuring that you will always get the same number of elements when you change the shape of your weights. So, this is why I typically use when I'm reshaping tensors. So, with all that out of the way, if you want to reshape weights to have five rows and one column, then you can use something like weights.view (5, 1), right. So, now, that you have seen how you can change the shape of a tensor and also do matrix multiplication, so this time I want you to calculate the output of this little neural network using matrix multiplication.

### 06. PyTorch V2 Part 1 Solution 2 V1-QLaGMz8Ca3E.en

Welcome to my solution for this exercise. So, for here, I had you calculate the output of our network using matrix multiplication. So remember, we wanted to use matrix multiplication because it's more efficient than doing these two separate operations of the multiplication and the sum. But to do the matrix multiplication, we actually needed to change the size of our weights tensor. So, to do that, just do weights.view 5, 1, and so this will change the shape of our weights tensor to be five rows and one column. If you remember, our features has the shape of one row and five columns, so we can do this matrix multiplication. So, there's just one operation that does the multiplication and the sum and just one go, and then we again add our bias term, pass it through the activation, and we get our output. So, as I mentioned before, you could actually stack up these simple neural networks into a multi-layer neural network, and this basically gives your network greater power to capture patterns and correlations in your data. Now, instead of a simple vector for our weights, we actually need to use a matrix. So, in this case, we have our input vector and our input data x_1, x_2, x_3. You think of this as a vector of just x, which our features. Then we have weights that connect our input to one hidden unit in this middle layers, usually called the hidden layer, hidden units, and we have two units in this hidden layer. So then, if we have our features, our inputs as a row vector, if we multiply it by this first column, then we're going to get the output, we're going to get this value of h_1. Then if we take our features and multiply it by the second column, then we're going to get the value for h_2. So again, mathematically looking at this with matrices and vectors and linear algebra, we see that to get the values for this hidden layer that we do a matrix multiplication between our feature vector, this x_1 to x_n, and our weight matrix. Then as before with these values, we're going to pass them through some activation function or maybe not an activation function, maybe we just want the row output of our network. So here, I'm generating some random data, some features, and some random weight matrices and bias terms that you'll be using to calculate the output of a multi-layer network. So, what I've built is basically we have three input features, two hidden units and one output unit. So, you can see that I've listed it here. So, our features we're going to create three features and this features vector here, and then we have an input equals three, so the shape is this, and two hidden units, one output unit. So, these weight matrices are created using these values. All right. I'll leave it up to you to calculate the output for this multi-layer network. Again, feel free to use the activation function defined earlier for the output of your network and the hidden layer. Cheers.

### 07. PyTorch V2 Part 1 Solution 3 V1-iMIo9p5iSbE.en

All right. So, here's my solution for this exercise. So, here, I had you calculate the output of this multi-layer network using the weights and features that we've defined up here. So, it was really similar to what we did before with our single layer simple neural network. So, it's basically just taking the features and our weight matrix, our first weight matrix, and calculating a matrix multiplication. So, here's the torch.mm plus B1, and then that gives us values for our hidden layer H. Now, we can use the values H as the input for the next layer of our network. So, we just do, again, a matrix multiplication of these hidden values H, with our second weight matrix W2, and adding on our bias terms, and then we get the output. So, my favorite features of PyTorches is being able to convert between Numpy arrays and Torch tensors, in a very nice and easy manner. So, this is really useful because a lot of the times, you'll be preparing your data and to do some preprocessing using Numpy, and then you want to move it into your network, and so, you have to bridge these Numpy arrays, what you're using for your data, and then the Torch tensors that you're using for your network. So, actually, to do this, we can actually get a tensor from a Numpy array using torch.fromnumpy. So, here I've just created a random array, a four-by-three array, and then we can create a Torch tensor from this array just by doing.from Numpy, and passing an array. So, this creates a nice tensor for us. So, this is a tensor in PyTorch, we can use with all of our Torch methods and eventually, use it in a neural network. Then, we can go backwards, so we can take a tensor such as B here. This is our Torch tensor and we can go back to a Numpy array doing b.numpy. So, this gives us back our Numpy array. So, one thing to remember when you're doing this, is that the memory is actually shared between the Numpy array and this Torch tensor. So, what this means, is that if you do any operations in place on either the Numpy array or the tensor, then you're going to change the values for the other one. So, for example, if we do this in-place operation of multiplying by two, which means that we're actually changing the values in memory, and not creating a new tensor, then we will actually change the values in the Numpy array. So, you see here, we have our Numpy array. So initially, it's like this, convert it to a Torch tensor, and here, I'm doing this in-place multiplication, and we've changed our values for this tensor. Then, if you look back at the Numpy array, the values have changed. So, that's just something to keep in mind as you're doing this, so you're not caught off guard when you're seeing your arrays, your Numpy arrays, being changed because of operations you're doing on the tensor. See you in the next video, cheers.

### 08. PyTorch V2 Part 2 V1-CSQOdOb2mlg.en

Hello everyone and welcome back. So, in this notebook and series of videos, I'm going to be showing you a more powerful way to build neural networks and PyTorch. So, in the last notebook, you saw how you can calculate the output for network using tensors and matrix multiplication. But PyTorch has this nice module, nn, that has a lot of my classes and methods and functions that allow us to build large neural networks in a very efficient way. So, to show you how this works, we're going to be using a dataset called MNIST. So, MNIST it's a whole bunch of grayscale handwritten digits. So ,0, 1, 2, 3, 4 and so on through nine. Each of these images is 28 by 28 pixels and the goal is to actually identify what the number is in these images. So, that dataset consists of each of these images and it's labeled with the digit that is in that image. So, ones are labeled one, twos are labeled two and so on. So, what we can do is we can actually show our network and image and the correct label and then it learns how to actually determine what the number and the image is. This dataset is available through the torchvision package. So, this is a package that sits alongside PyTorch, that provides a lot of nice utilities like datasets and models for doing computer vision problems. We can run this cell to download and load the MNIST dataset. What it does is it gives us back an object which I'm calling trainloader. So, with this trainloader we can turn into an iterator with iter and then this will allow us to start getting good at it or we can actually just use this in a loop, in a for loop and so we can get our images and labels out of this generator with four image, comma label and trainloader. One thing to notice is that when I created the trainloader, I set the batch size to 64. So, what that means and every time we get a set of images and labels out, we're actually getting 64 images out from our data loader. So, then if you look at the shape and the size of these images, we'll see that they are 64 by one by 28 by 28. So, 64 images and then one color channels so it's grayscale, and then it's 28 by 28 pixels is the shape of these images and so we can see that here. Then our labels have a shape of 64 so it's just a vector that's 64 elements which with a label for each of our images and we can see what one of these images looks like this is a nice number four. So, we're going to do here is build a multi-layer neural network using the methods that we saw before. By that I mean you're going to initialize some weight matrices and some bias vectors and use those to calculate the output of this multi-layer network. Specifically, we want to build this network with 784 input units, 256 hidden units, and 10 output units. So, 10 output units, one for each of our classes. So, the 784 input units, this comes from the fact that with this type of network is called a fully connected network or a dense network. We want to think of our inputs as just one vector. So, our images are actually this 28 by 28 image, but we want to put a vector into our network and so what we need to do is actually convert this 28 by 28 image into a vector and so, 784 is 28 times 28. When we actually take this 28 by 28 image and flatten it into a vector then it's going to be 784 elements long. So, now what we need to do is take each of our batches which is 64 by one by 28 by 28 and then convert it into a shape that is to another tensor which shapes 64 by 784. This is going to be the tensor that's the input to our network. So, go and give this a shot. So again, build the networks 784 input units, 256 hidden units and 10 output units and you're going to be generating your own random initial weight and bias matrices. Cheers.

### 09. PyTorch V2 Part 2 Solution V1-zym36ihtOMY.en

Here is my solution for this multi-layer neural network for classifying handwritten digits from the MNIST dataset. So, here I've defined our activation function like before, so, again this is the sigmoid function and here I'm flattening the images. So, remember how to reshape your tensors. So, here I'm using.view. So, I'm just grabbing the batch size. So, images.shape. The first element zero here, gives you the number of batches in your images tensor. So, I want to keep the number of batches the same, but I want to flatten the rest of the dimensions. So, to do this, you actually can just put in negative one. So, I could type in 784 here but a kind of a shortcut way to do this is to put in negative one. So, basically what this does is it takes 64 as your batch size here and then when you put a negative one it sees this and then it just chooses the appropriate size to get the total number of elements. So, it'll work out on its own that it needs to make the second dimension, 784 so that the number of elements after reshaping matches the number elements before reshaping. So, this is just a kind of quick way to flatten a tensor without having to know what the second dimension used to be. Then here I'm just creating our weight and bias parameters. So, we know that we want an input of 784 units and we want 256 hidden units. So, our first weight matrix is going to be 784 by 256. Then, we need a bias term for each of our hidden units. So we have 256 bias terms here in b1. Then, for our second weight's going from the hidden layer to the output layer we want 256 inputs to 10 outputs. Then again 10 elements in our bias. Before we can do a matrix multiplication of our inputs with the first set of weights, our first weight parameters, add in the bias terms and passes through our activation functions so that gives us the output of our hidden layer. Then we can use that as the input to our output layer, and again, a matrix multiplication with a second set of weights and the second set of bias terms. This gives us the output of our network. All right. So, if we look at the output of this network, we see that we get those 64. So, first let me print the shape just to make sure we did that right. So, 64 rows for one of each of our sort of input examples and then 10 values, so, basically it's a value that's trying to say this image belongs to this class like this digit. So, we can inspect our output tensor and see what's going on here. So, we see these values are just sort of all over the place. So, you got like six and negative 11 and so on. But we really want is we want our network to kind of tell us the probability of our different classes given some image. So, kind of we want to pass in an image to our network and then the output should be a probability distribution that tells us which are the most likely classes or digits that belong to this image. So, if it's the image of a six, then we want a probability distribution where most of the probability is in the sixth class. So, it's telling us that it's a number six. So, we want it to look something like this. This is like a class probability. So, it's telling us the probabilities of the different classes given this image that we're passing in. So, you can see that the probability for each of these different classes is roughly the same, and so it's a uniform distribution. This represents an untrained network, so it's a uniform probability distribution. It's because it hasn't seen any data yet, so it hasn't learned anything about these images. So, whenever you give an image to it, it doesn't know what it is so it's just going to give an equal probability to each class, regardless of the image that you pass in. So, what we want is we want the output of our network to be a probability distribution that gives us the probability that the image belongs to any one of our classes. So for this, we use the softmax function. So what this looks like is the exponential. So,you pass in your 10 values. So, for each of those values, we calculate the exponential of that value divided by the sum of exponentials of all the values. So, what this does is it actually kind of squishes each of the input values x between zero and one, and then also normalizes all the values so that the sum of each of the probabilities is one. So, the entire thing sums up to one. So, this gives you a proper probability distribution. What I want you to do here is actually implement a function called softmax that performs this calculation. So, what you're going to be doing is taking the output from this simple neural network and has shaped 64 by 10 and pass it through a dysfunction softmax and make sure it calculates the probability distribution for each of the different examples that we passed in. Right? Good luck.

### 10. PyTorch V2 Part 2 Solution 2 V1-8KRX7HvqfP0.en

Welcome back. Here is my solution for the softmax function. Here in the numerator, we know we want to take the exponential, so it's pretty straight forward with torch.exp. So we're going to use the exponential of x, which is our input tensor. In the denominator, we know we want to do something like, again take exponentials so torch.exp, and then take the sum across all those values. So, one thing we need to remember is that we want the sum across one single row. So, each of the columns in one single row for each example. So, for one example, we want to sum up those values. So, for here in torch.sum, we're going to use dimension equals one. So, this is basically going to take the sum across the columns. What this does, torch.sum here, is going to actually going to give us a tensor, that is just a vector of 64 elements. So, the problem with this is that, if this is 64 by 10, and this is just a 64-long vector, it's going to try to divide every element in this tensor by all 64 of these values. So, it's going give us a 64 by 64 tensor, and that's not what we want. We want our output to be 64 by 10. So, what you actually need to do is reshape this tensor here to have 64 rows, but only one value for each of those rows. So, what that's going do, it's going look at for each row in this tensor, is going to look at the equivalent row in this tensor. So, since each row in this tensor only has one value, it's going to divide this exponential by the one value in this denominator tensor. This can be really tricky, but it's also super important to understand how broadcasting works in PyTorch, and how to actually fit all these tensors together with the correct shape and the correct operations to get everything out right. So, if we do this, it look what we have, we pass our output through the softmax function, and then we get our probabilities, and we can look the shape and it is 64 by 10, and if you take the sum across each of the rows, then it adds up to one, like it should with a proper probability distribution. So, now, we're going to look at how you use this nn module to build neural networks. So, you'll find that it's actually in a lot of ways simpler and more powerful. You'll be able to build larger and larger neural networks using the same framework. The way this works in general, is that we're going to create a new class, and you can call it networking, you can call it whatever you want, you can call it classifier, you can call it MNIST. It doesn't really matter so much what you call it, but you need to subclass it from nn.module. Then, in the init method, it's __init method. You need to call it super and run the init method of nn.module. So, you need to do this because then, PyTorch will know to register all the different layers and operations that you're going to be putting into this network. If you don't do this part then, it won't be able to track the things that you're adding to your network, and it just won't work. So, here, we can create our hidden layers using nn.Linear. So, what this does, is it creates a operation for the linear transformation. So, when we take our inputs x and then multiply it by weights and add your bias terms, that's a linear transformation. So, what this does is calling NN.Linear, it creates an object that itself has created parameters for the weights and parameters for the bias and then, when you pass a tensor through this hidden layer, this object, it's going to automatically calculate the linear transformation for you. So, all you really need to do is tell it what's the size of the inputs, and then what are the size of the output. So, 784 by 256, we're going to use 256 outputs for this. So, it's kind of rebuilding the network that we saw before. Similarly, we want another linear transformation between our hidden units and our output. So, again, we have 256 hidden units, and we have 10 outputs, 10 output units, so we're going to create a output layer called self.output, and create this linear transformation operation. We also want to create a sigmoid operation for the activation and then, softmax for the output, so we get this probability distribution. Now, we're going to create a forward method and so, forward is basically going to be, as we pass a tensor in to the network. It's gonna go through all these operations, and eventually give us our output. So, here, x, the argument is going to be the input tensor and then, we're going to pass it through our hidden layer. So, this is again, like this linear transformation that we defined up here, and it's going to go through a sigmoid activation, and then through our output layer or output linear transformation, we have here, and then through the sigmoid function, and then finally return the output of our softmax. so we can create this. Then, if we kind of look at it, so it'll print it out, and it'll tell us the operations, and not necessarily the order, but at least it tells us the operations that we have defined for this network. You can also use some functional definitions for things like sigmoid and softmax, and it kind of makes the class the way you write the code a little bit cleaner. We can get that from torch.nn.functional. Most of the time, you'll see is like import torch.nn.functional as capital F. So, there's kind of that convention in PyTorch code. So, again, we define our linear transformations, self.hidden, self.output but now in our forward method. So, we can call self.hidden to get like our values for hidden layer, but then, we pass it through the sigmoid function, f.sigmoid, and the same thing with the output layers. So, we have our output linear transformations of the output, and we pass it through this softmax operation. So, the reason we can do this because, when we create these linear transformations, it's creating the weights and bias matrices on its own. But for sigmoid and softmax, it's just an element wise operation, so it doesn't have to create any extra parameters or extra matrices to do these operations, and so we can have these be purely functional without having to create any sort of object or classes. However, they are equivalent. So this way to build the network is equivalent to this way up here, but it's a little bit more succinct when you're doing it with these kind of functional pattern. So far, we've only been using the sigmoid function as an activation function, but there are, of course, a lot of different ones you want to use. Really the only requirement is that, these activation functions should typically be non-linear. So, if you want your network to be able to learn non-linear correlations and patterns, and we want the output to be non-linear, then you need to use non-linear activation functions in your hidden layers. So, a sigmoid is one example. The hyperbolic tangent is another. One that is pretty much used all the time, like almost exclusively as activation function and hidden layers, is the ReLU, so the rectified linear unit. This is basically the simplest non-linear function that you can use, and it turns out that networks tend to train a lot faster when using ReLU as compared to sigmoid and hyperbolic tangent, so ReLU was what we typically use. Okay. So, here, you're going to build your own neural network, that's larger. So, this time, it's going to have two hidden layers, and you'll be using the ReLU activation function for this on your hidden layers. So using this object-oriented class method within a.module, go ahead and build a network that looks like this, with 784 input units, a 128 units in the first hidden layer, 64 units and the second hidden layer, and then 10 output units. All right. Cheers.

### 11. PyTorch V2 Part 3 V1-9ILiZwbi9dA.en

Hello, everyone, and welcome back. So, in this video and in this notebook, I'll be showing you how to actually train neural networks in PyTorch. So, previously, we saw how to define neural networks in PyTorch using the nn module, but now we're going to see how we actually take one of these networks that we defined and train it. So, what I mean by training is that we're going to use our neural networks as a universal function approximator. What that means is that, for basically any function, we have some desired input for example, an image of the number four, and then we have some desired output of this function. In this case a probability distribution that is telling us the probabilities of the various digits. So, in this case, if we passed it in image four, we want to get out a probability distribution where there's a lot of probability in the digit four. So, the cool thing about neural networks is that if you use non-linear activations and then you have the correct dataset of these images that are labeled with the correct ones, then basically you pass in an image and the correct output, the correct label or class, and eventually your neural network will build to approximate this function that is converting these images into this probability distribution, and that's our goal here. So, basically we want to see how in PyTorch, we can build a neural network and then we're going to give it the inputs and outputs, and then adjust the weights of that network so that it approximates this function. So, the first thing that we need for that is what is called a loss function. So, it's sometimes also called the cost, and what this is it's a measure of our prediction error. So, we pass in the image of a four and then our network predicts something else that's an error. So, we want to measure how far away our networks prediction is from the correct label, and we do that using loss function. So, in this case, it's the mean squared error. So, a lot of times you'll use this in regression problems, but use other loss functions and classification problems like this one here. So, the loss depends on the output of our network or the predictions our network is making. The output of a network depends on the weight. So, like the network parameters. So, we can actually adjust our weights such that this loss is minimized, and once the loss is minimized, then we know that our network is making as good predictions as it can. So, this is the whole goal to adjust our network parameters to minimize our loss, and we do this by using a process called gradient descent. So, the gradient is the slope of the loss function with respect to our perimeters. The gradient always points in the direction of fastest change. So, for example if you have a mountain, the gradient is going to always point up the mountain. So, you can imagine our loss function being like this mountain where we have a high loss up here and we have a low loss down here. So, we know that we want to get to the minimum of our loss when we minimize our loss, and so, we want to go downwards. So, basically, the gradient points upwards and so, we just go the opposite direction. So, we go in the direction of the negative gradient, and then if we keep following this down, then eventually we get to the bottom of this mountain, the lowest loss. With multilayered neural networks, we use an algorithm called backpropagation to do this. Backpropagation is really just an application of the chain rule from calculus. So, if you think about it when we pass in some data, some input into our network, it goes through this forward pass through the network to calculate our loss. So, we pass in some data, some feature input x and then it goes through this linear transformation which depends on our weights and biases, and then through some activation function like a sigmoid, through another linear transformation with some more weights and biases, and then that goes in, from that we can calculate our loss. So, if we make a small change in our weights here, W1, it's going to propagate through the network and end up like results in a small change in our loss. So, you can think of this as a chain of changes. So, if we change here, this is going to change. Even that's going to propagate through here, it's going to propagate through here, it's going to propagate through here. So, with backpropagation, we actually use these same changes, but we go in the opposite direction. So, for each of these operations like the loss and the linear transformation into the sigmoid activation function, there's always going to be some derivative, some gradient between the outputs and inputs, and so, what we do is we take each of the gradients for these operations and we pass them backwards through the network. Each step we multiply the incoming gradient with the gradient of the operation itself. So, for example just starting at the end with the loss. So, we pass this gradient or the loss dldL2. So, this is the gradient of the loss with respect to the second linear transformation, and then we pass that backwards again and if we multiply it by the loss of this L2. So, this is the linear transformation with respect to the outputs of our activation function, that gives us the gradient for this operation. If you multiply this gradient by the gradient coming from the loss, then we get the total gradient for both of these parts, and this gradient can be passed back to this softmax function. So, as the general process for backpropagation, we take our gradients, we pass it backwards to the previous operation, multiply it by the gradient there, and then pass that total gradient backwards. So, we just keep doing that through each of the operations in our network, and eventually we'll get back to our weights. What this does is it allows us to calculate the gradient of the loss with respect to these weights. Like I was saying before, the gradient points in the direction of fastest change in our loss, so, to maximize it. So, if we want to minimize our loss, we can subtract the gradient off from our weights, and so, what this will do is it'll give us a new set of weights that will in general result in a smaller loss. So, the way that backpropagation algorithm works is that it will make a forward pass through a network, calculate the loss, and then once we have the loss, we can go backwards through our network and calculate the gradient, and get the gradient for a weights. Then we'll update the weights. Do another forward pass, calculate the loss, do another backward pass, update the weights, and so on and so on and so on, until we get sufficiently minimized loss. So, once we have the gradient and like I was saying before, we can subtract it off from our weights, but we also use this term Alpha which is called the learning rate. This is basically just a way to scale our gradients so that we're not taking too large steps in this iterative process. So, what can happen if you're update steps are too large, you can bounce around in this trough around the minimum and never actually settle in the minimum of the loss. So, let's see how we can actually calculate losses in PyTorch. Again using the nn module, PyTorch provides us a lot of different losses including the cross-entropy loss. So, this loss is what we'll typically use when we're doing classification problems. In PyTorch, the convention is to assign our loss to its variable criterion. So, if we wanted to use cross-entropy, we just say criterion equals nn.crossEntropyLoss and create that class. So, one thing to note is that, if you look at the documentation for cross-entropy loss, you'll see that it actually wants the scores like the logits of our network as the input to the cross-entropy loss. So, you'll be using this with an output such as softmax, which gives us this nice probability distribution. But for computational reasons, then it's generally better to use the logits which are the input to the softmax function as the input to this loss. So, the input is expected to be the scores for each class and not the probabilities themselves. So, first I'm going to import the necessary modules here and also download our data and create it in, like you've seen before, as a trainloader, and so, we can get our data out of here. So, here I'm defining a model. So, I'm using nn.Sequential, and if you haven't seen this, checkout the end of the previous notebook. So, the end of part two, will show you how to use nn.Sequential. It's just a somewhat more concise way to define simple feed-forward networks, and so, you'll notice here that I'm actually only returning the logits, the scores of our output function and not the softmax output itself. Then here we can define our loss. So, criterions equal to nn.crossEntropyLoss. We get our data with images and labels, flatten it, pass it through our model to get the logits, and then we can get the actual loss by bypassing in our logits and the true labels, and so, again we get the labels from our trainloader. So, if we do this, we see we have calculated the loss. So, my experience, it's more convenient to build your model using a log-softmax output instead of just normal softmax. So, with a log-softmax output to get the actual probabilities, you just pass it through torch.exp. So, the exponential. With a log-softmax output, you'll want to use the negative log-likelihood loss or nn.NLLLoss. So, what I want you to do here is build a model that returns the log-softmax as the output, and calculate the loss using the negative log-likelihood loss. When you're using log-softmax, make sure you pay attention to the dim keyword argument. You want to make sure you set it right so that the output is what you want. So, go and try this and feel free to check out my solution. It's in the notebook and also in the next video, if you're having problems. Cheers.

### 12. PyTorch V2 Part 3 Solution V2-zBWlOeX2sQM.en

Hi and welcome back. Here's my solution for this model that uses a LogSoftmax output. It is a pretty similar to what I built before with an index sequential. So, we just use a linear transformation, ReLU, linear transformation, ReLU, another linear transformation for output and then we can pass this to our LogSoftmax module. So, what I'm doing here is I'm making sure I set the dimension to one for LogSoftmax and this makes it so that it calculates the function across the columns instead of the rows. So, if you remember, the rows correspond to our examples. So, we have a batch of examples that we're passing to our network and each row is one of those examples. So, we want to make sure that we're covering the softmax function across each of our examples and not across each individual feature in our batches. Here, I'm just defining our loss or criterion as the negative log likelihood loss and again get our images and labels from our train loader, flatten them, pass it through our model to get the logits. So, this is actually not the largest anymore, this is like a log probability, so we call it like logps, and then you do that. There you go. You see we get our nice loss. Now, we know how to calculate a loss, but how do we actually use it to perform backpropagation? So, PyTorch towards actually has this really great module called Autograd that automatically calculates the gradients of our tensors. So, the way it works is that, PyTorch will keep track of all the operations you do on a tensor and then when you can tell it to do a backwards pass, to go backwards through each of those operations and calculate the gradients with respect to the input parameters. In general, you need to tell PyTorch that you want to use autograd on a specific tensor. So, in this case, you would create some tensor like x equals torch.zeros, just to make it a scalar, say one and then give it requires grad equals true. So, this tells PyTorch to track the operations on this tensor x, so that if you want to get the gradient then it will calculate it for you. So, in general, if you're creating a tensor and you don't want to calculate the gradient for it, you want to make sure this is set to false. You can also use this context torch.no grad to make sure all the gradients are shut off for all of the operations that you're doing while you're in this context. Then, you can also turn on or off gradients globally with torch.set grad enabled and give it true or false, depending on what you want to do. So, the way this works in PyTorch is that you basically create your tensor and again, you set requires grad equals true and then you just perform some operations on it. Then, once you are done with those operations, you type in.backwards. So, if you use x, this tensor x, then calculate some other tensor z then if you do z.backward, it'll go backwards through your operations and calculate the total gradient for x. So, for example, if I just create this random tensor, random two-by-two tensor, and then I can square it like this. What it does, you can actually see if you look at y, so y is our secondary or squared tensor. If you look at y.grad function, then it actually shows us that this grad function is a power. So, PyTorch just track this and it knows that the last operation done was a power operation. So, now, we can take the mean of y and get another tensor z. So, now this is just a scalar tensor, we've reduced y, y is a two-by-two matrix, two by two array and then we take in the mean of it to get z. Ingredients for tensor show up in this attribute grad, so we can actually look at what's the gradient of our tensor x right now, and we've only done this forward pass, we haven't actually calculated the gradient yet and so it's just none. So, now if we do z.backward, it's going to go backwards through this tiny little set of operations that we've done. So, we did a power and then a mean and let's go backwards through this and calculate the gradient for x. So, if you actually work out the math, you find out that the gradient of z with respect to x should be x over two and if we look at the gradient, then we can also look at x divided by two then they are the same. So, our gradient is equal to what it should be mathematically, and this is the general process for working with gradients, and autograd, and PyTorch. Why this is useful, is because we can use this to get our gradients when we calculate the loss. So, if remember, our loss depends on our weight and bias parameters. We need the gradients of our weights to do gradient descent. So, what we can do is we can set up our weights as tensors that require gradients and then do a forward pass to calculate our loss. With the loss, you do a backwards pass which calculates the gradients for your weights, and then with those gradients, you can do your gradient descent step. Now, I'll show you how that looks in code. So, here, I'm defining our model like I did before with LogSoftmax output, then using the negative log-likelihood loss, get our images and labels from our train loader, flatten it, and then we can get our log probabilities from our model and then pass that into our criterion, which gives us the actual loss. So, now, if we look at our models weights, so model zero gives us the parameters for this first linear transformation. So, we can look at the weight and then we can look at the gradient, then we'll do our backwards pass starting from the loss and then we can look at the weight gradients again. So, we see before the backward pass, we don't have any because we haven't actually calculated it yet but then after the backwards pass, we have calculated our gradients. So, we can use these gradients in gradient descent to train our network. All right. So, now you know how to calculate losses and you know how to use those losses to calculate gradients. So, there's one piece left before we can start training. So, you need to see how to use those gradients to actually update our weights, and for that we use optimizers and these come from PyTorch's Optim package. So, for example, we can use stochastic gradient descent with optim.SGD. The way this is defined is we import this module optim from PyTorch and then we'd say optim.SGD, we give it our model parameters. So, these are the parameters that we want this optimizer to actually update and then we give it a learning rate, and this creates our optimizer for us. So, the training pass consists of four different steps. So, first, we're going to make a forward pass through the network then we're going to use that network output to calculate the loss, then we'll perform a backwards pass through the network with loss.backwards and this will calculate the gradients. Then, we'll make a step with our optimizer that updates the weights. I'll show you how this works with one training step and then you're going to write it up for real and a loop that is going to train a network. So, first, we're going to start by getting our images and labels like we normally do from our train loader and then we're going to flatten them. Then next, what we want to do is actually clear the gradients. So, PyTorch by default accumulates gradients. That means that if you actually do multiple passes and multiple backwards like multiple forward passes, multiple backwards passes, and you keep calculating your gradient, it's going to keep summing up those gradients. So, if you don't clear gradients, then you're going to be getting gradients from the previous training step in your current training step and it's going to end up where your network is just not training properly. So, for this in general, you're going to be calling zero grad before every training passes. So, you just say optimizer.zero grad and this will just clean out all the gradients and all the parameters in your optimizer, and it'll allow you to train appropriately. So, this is one of the things in PyTorch that is easy to forget, but it's really important. So, try your hardest to remember to do this part, and then we do our forward pass, backward pass, then update the weights. So, we get our output, so we do a forward pass through our model with our images, then we calculate the loss using the output of the model and our labels, then we do a backwards pass and then finally we take an optimizer step. So, if we look at our initial weights, so it looks like this and then we can calculate our gradient, and so the gradient looks like and then if we take an optimizer step and update our weights, then our weights have changed. So, in general, what has worked is you're going to be looping through your training set and then for each batch out of your training set, you'll do the same training pass. So, you'll get your data, then clear the gradients, pass those images or your input through your network to get your output, from that, in the labels, calculate your loss, and then do a backwards pass on the loss and then update your weights. So, now, it's your turn to implement the training loop for this model. So, the idea here is that we're going to be looping through our data-set, so grabbing images and labels from train loader and then on each of those batches, you'll be doing the training pass, and so you'll do this pass where you calculate the output of the network, calculate the loss, do backwards pass on loss, and then update your weights. Each pass through the entire training set is called an epoch and so here I just have it set for five epochs. So, you can change this number if you want to go more or less. Once you calculate the loss, we can accumulate it to keep track, so we're going to be looking at a loss. So, this is running loss and so we'll be printing out the training loss as it's going along. So, if it's working, you should see the loss start falling, start dropping as you're going through the data. Try this out yourself and if you need some help, be sure to check out my solution. Cheers.

### 13. PyTorch V2 Part 3 Solution 2 V1-ExyFG2MjsKs.en

Hi again. So, here's my solution for the train pass that I had you implement. So, here we're just defining our model like normal and then our negative log-likelihood loss using stochastic gradient descent and pass in our parameters. Then, here is our training pass. So, for each image in labels in trainloader, we're going to flatten it and then zero out the gradients using optimizer. zero_ grad. Pass our images forward through the model and the output and then from that, we can calculate our loss and then do a backward pass and then finally with the gradients, we can do this optimizer step. So, if I run this and we wait a little bit for to train, we can actually see the loss dropping over time, right? So, after five epochs, we see that the first one, it starts out fairly high at 1.9 but after five epochs, continuous drop as we're training and we see it much lower after five epochs. So, if we kept training then our network would learn the data better and better and the training loss would be even smaller. So, now with our training network, we can actually see what our network thinks it's seen in these images. So, for here, we can pass in an image. In this case, it's the image of a number two and then this is what our network is predicting now. So, you can see pretty easily that it's putting most of the probability, most of its prediction into the class for the digit two. So we try it again and put in passes in number eight and again, it's predicting eight. So, we've managed to actually train our network to make accurate predictions for our digits. So next step, you'll write the code for training a neutral network on a more complex dataset and you'll be doing the whole thing, defining the model, running the training loop, all that. Cheers.

### 14. PyTorch - Part 4-AEJV_RKZ7VU.en

Welcome back. So, in this notebook, you'll be building your own neural network to classify clothing images. So, like I talked about in the last video, MNIST is actually a fairly trivial dataset these days. It's really easy to get really high accuracy with a neural network. So instead, you're going to be using Fashion-MNIST, and this is basically just a drop-in replacement for MNIST so we have 28 by 28 grayscale images, but this time it's clothing. So, you have a lot more variation in the classes, and it just ends up being a much more difficult problem to classify like there's a t-shirt, there's pants, there's a sweater, there's shoes instead of handwritten digits. So it's a better representation of datasets that you'd use in the real world. So, I've left this up to you to actually build a network and train it. So here you can define your network architecture, then here you will create your network to define the criterion and optimizer and then write the code for the training pass. Once you have your network built and trained, you can test out your network. So here, you'd want to do a forward pass, get your logits, calculate the class probabilities, maybe output of your network, and then pass in one of these images from the test set and check out if your network can actually predict it correctly. If you want to see my solution, it's in the next notebook, part five, and you'll also see it in the next video. Cheers.

### 15. PyTorch V2 Part 4 Solution V1-R6Y4hPLVQWM.en

Again. So, in the last video, I hand you try out building your own neural network to classify this fashion in this dataset. Here is my solution like how I decided to build this. So first, building our network. So, here, I'm going to import our normal modules from PyTorch. So, nn and optim, so, nn is going to allow us to build our network, and optim is going to give us our optimizers. I must going to import this functional modules, so, we can use functions like ReLU and log softmax. I decided to define my network architectures using the class. So, in nn.modules subclassing from this, and it's called a classifier. Then I created four different linear transformations. So, in this case, it's three hidden layers and then one output layer. Our first hidden layer has 256 units. The second hidden layer has a 128, one after that has 64. Then our output has 10 units. So, in the forward pass, I did something a little different. So, I made sure here that the input tensor is actually flattened. So now, you don't have to flatten your input tensors in the training loop, it'll just do it in the forward pass itself. So, to do this is do x.view, which is going to change our shape. So, x.shape zero is going to give us our batch size. Then the negative one here is going to basically fill out the the second dimension with as many elements as it needs to keep the same total number of elements. So, what this does is it basically gives us another tensor, that is the flattened version of our input tensor. It doing a pass these through our linear transformations, and then ReLU activation functions. Then finally, we use a log softmax with a dimension set to one, as our output, and return that from our forward function. With the model defined, I can do model equals classifiers. So, this actually creates our model. Then we define our criterion with the negative log likelihood loss. So, I'm using log softmax as the output my model. So, I want to use the NLLLoss as the criterion. Then, here I'm using the Adam optimizer. So, this is basically the same as stochastic gradient descent, but it has some nice properties where it uses momentum which speeds up the actual fitting process. It also adjust the learning rate for each of the individual parameters in your model. Here, I wrote my training loop. So, again I'm using five epochs. So, for e in range epoch, so, this is going to basically loop through our dataset five times, I'm tracking the loss with running loss, and just kind of instantiated it here. Then, getting our images. So, from images labels in train loader, so I get our log probabilities by passing in the images to a model. So, one thing to note, you can kind of do a little shortcut. If you just pass these in to model as if it was a function, then it will run the forward method. So, this is just a kind of a shorter way to run the forward pass through your model. Then with the log probabilities and the labels, I can calculate the loss. Then here, I am zeroing the gradients. Now I'm doing the lost up backwards to calculating our gradients, and then with the gradients, I can do our optimizer step. If we tried it, we can see at least for these first five epochs that are loss actually drops. Now the network is trained, we can actually test it out. So, we pass in data to our model, calculate the probabilities. So, here, doing the forward pass through the model to get our actual log probabilities and with the log probabilities, you can take the exponential to get the actual probabilities. Then with that, we can pass it into this nice little view classify function that I wrote, and it shows us, if we pass an image of a shirt, it tells us that it's a shirt. So, our network seems to have learned fairly well, what this dataset is showing us.

### 16. PyTorch V2 Part 5 V1 (1)-XACXlkIdS7Y.en

Hey there. So now, we're going to start talking about inference and validation. So, when you have your trained network, you typically want to use it for making predictions. This is called inference, it's a term borrowed from statistics. However, neural networks have a tendency to perform too well on your training data and they aren't able to generalize the data that your network hasn't seen before. This is called overfitting. This happens because as you're training more and more and more on your training set, your network starts to pick up correlations and patterns that are in your training set but they aren't in the more general dataset of all possible handwritten digits. So, to test for overfitting, we measure the performance of the network on data that isn't in the training set. This data is usually called the validation set or the test set. So, while we measure the performance on the validation set, we also tried to reduce overfitting through regularization such as dropout. So, in this notebook, I'll show you how we can both look at our validation set and also use dropout to reduce overfitting. So, to get the training set for your data like from PyTorch, then we say train equals true and for fashionMNIST. To get our test set, we're actually going to set train equals false here. Here, I'm just defining the model like we did before. So, the goal of validation is to measure our model's performance on data that is not part of our training set. But what we mean by performance is up to you, up to the developer, the person who's writing the code. A lot of times, it'll just be the accuracy. So, like how many correct classifications did our model make compared to all of the predictions? And other options for metrics are precision and recall, and the top five error rate. So here, I'll show you how to actually measure the accuracy on the validation set. So first, I'm going to do a forward pass that is one batch from the test set. So, see in our test set we get our probabilities. So, just 64 examples in a batch. Then 10 columns like one for each of the classes. So, the accuracy, we want to see if our model made the correct prediction of the class given the image. The prediction we can consider it to be whichever class has the highest probability. So, for this, we can use this top-k method on our tensors. This returns the k highest values. So, if we pass in one, then this is going to give us the one highest value. This one highest value is the most likely class that our network is predicting. So, for the first ten examples, and this batch of test data that I grabbed, we see that the class four and class five are what are being predicted for these. So, remember that this network actually hasn't been trained yet, and so it's just making these guesses randomly because it doesn't really know anything about the data yet. So, top-k actually returns a tuple with two tensors. So, the first tensor is the actual probability values, and the second tensor are the class indices themselves. So typically, we just want this top class here. So, I'm calling top-k here and I'm separating out the probabilities in the classes. So, we'll just use this top class going forward. So, now that we have the predicted classes from our network, we can compare that with the true labels. So, we say, we can say like top class equals equals labels. The only trick here is that we need to make sure our top class tensor and the labels tensor has the same shape. So, this equality actually operates appropriately like we expect. So, labels from the test loader is actually a 1D tensor with 64 elements, but top class itself is a 2D tensor, 64 by one. So here, I'm just like changing the shape of labels to match the shape of top class. This gives us this equals tensor. We can actually see it looks like. So, it gives us a bunch of zeros and ones. So, zeros are where they don't match, and then ones are where they do match. Now, we have this tensor that's all just a bunch of zeros and ones. So, if we want to know the accuracy, right? We can just sum up all the correct things, all the correct predictions, and then divide by the total number of predictions. If you're tensor is all zeros and ones, that's actually equivalent to taking the mean. So for that, we can do torch.mean, but the problem is that equals is actually a byte tensor, and torch.mean won't work on byte tensors. So, we actually need to convert equals until a float tensor. If we do that, then we can actually see our accuracy for this one particular batch is 15.6 percent. So, this is roughly what we expect. So, our network hasn't been trained yet. It's making pretty much random guesses. That means that we should see our accuracy be about one in ten for any particular image because it's just uniformly guessing one of the classes, okay? So here, I'm going to have you actually implement this validation loop, where you'll pass in data from the test set through the network and calculate the loss and the accuracy. So, one thing to note, I think I mentioned this before. For the validation paths, we're not actually going to be doing any training. So, we don't need the gradients. So, you can actually speed up your code a little bit if you turn off the gradients. So, using this context, so with torch.no_grad, then you can put your validation pass in here. So, for images and labels in your test loader and then do the validation pass here. So, I've basically built a classifier for you, set all this up. Here's the training pass, and then it's up to you to implement the validation pass, and then print out the accuracy. All right. Good luck, and if you get stuck or want any help, be sure to check out my solution.

### 17. PyTorch V2 Part 5 Solution V1-AjrXltxqsK4.en

Welcome back. So, here's my solution for the validation pass. So, here, our model has been defined, our loss, and our optimizer, and all this stuff. I've set to 30 epochs so we can see like how this trains or how the training loss drops, and how the validation loss changes over time. The way this works is that after each pass, after each epoch, after each pass through the training set, then we're going to do a validation pass. That's what this else here means. So, basically, four of this stuff and then else basically says after this four loop completes, then run this code. That's what this else here means. As seen before, we want to turn off our gradients so with torch.no_grad, and then we're going to get our images and labels from a test set, pass it into the images into our model to get our log probabilities, calculate our loss. So, here, I am going to just be updating our test_loss. So, test_loss is just an integer that's going to count up our loss on our test set as we're training, as we're doing more of these validation passes. So, this way, we can actually track the test loss over all the epochs that we're training. So, from the law of probabilities, I can get our actual probability distributions using torch.exponential. Again, topk1 gives us our predicted classes and we can measure, we can calculate the equalities. So, here, we can get our probabilities from our log probabilities using torch.exp, taking the exponential of a log gives you back into the probabilities. From that, we do ps.topk, so one, and this gives us the top_class or predicted class from the network. Then, using checking for equality, we can see where our predicted classes match with the true classes from labels. Again, measure our calculator accuracy. So, using torch.mean and changing equals into a FloatTensor. So, I'm going to run this and then let it run for a while, and then we can see what the actual training and validation losses look like as we were training this network. Now, the network is trained, we can see how the validation loss and the training loss actually changed over time like as we continue training on more and more data. So, we see is the training loss drops but the validation loss actually starts going up over time. There's actually a clear sign of overfitting so our network is getting better and better and better on the training data, but it's actually starting to get worse on the validation data. This is because as it's learning the training data, it's failing to generalize the data outside of that. Okay. So, this is what the phenomenon of overfitting looks like. The way that we combine it, the way that we try to avoid this and prevent it is by using regularization and specifically, dropout. So, deep behind dropout is that we randomly drop input units between our layers. What this does is it forces the network to share information between the weights, and so this increases this ability to generalize to new data. PyTorch adding dropout is pretty straightforward. We just use this nn.Dropout module. So, we can basically create our classifier like we had before using the linear transformations to do our hidden layers, and then we just add self.dropout, nn.Dropout, and then you give it some drop probability. In this case, this is 20 percent, so this is the probability that you'll drop a unit. In the forward method, it's pretty similar. So, we just pass in x, which is our input tensor, we're going to make sure it's flattened, and then we pass this tensor through each of our fully connected layers into an activation, relu activation and then through dropout. Our last layer is the output layer so we're not going to use dropout here. There's one more thing to note about this. So, when we're actually doing inference, if we're trying to make predictions with our network, we want to have all of our units available, right? So, in this case, we want to turn off dropout when we're doing validation, testing, when we're trying to make predictions. So, to do that, we do model.eval. So, model.eval will turn off dropout and this will allow us to get the most power, the highest performance out of our network when we're doing inference. Then, to go back in the train mode, use model.train. So, then, the validation pass looks like this now. So, first, we're going to turn off our gradients. So, with torch.no_grad, and then we set our model to evaluation mode, and then we do our validation pass through the test data. Then, after all this, we want to make sure the model is set back to train mode so we do model.train. Okay. So, now, I'm going to leave it up to you to create your new model. Try adding dropout to it and then try training your model with dropout. Then, again, checkout the training progress of validation using dropout. Cheers.

### 18. PyTorch V2 Part 5 Solution 2 V1-3Py2SbtZLbc.en

Hi. Here's my solution for your building and training this network using dropout now. Just like I showed you before, we can define our dropout module as self.dropout and then nn.dropout to give it some drop probability. So, in this case, 20 percent, and then just adding it to our forward method now on each of our hidden layers. Now, our validation code looks basically the same as before except now we're using model.eval. So, again, this turns our model into evaluation or inference mode which turns off dropout. Then, like the same way before, we just go through our data and the test say, calculate the losses and accuracy and after all that, we do model.train to set the model back into train mode, turn dropout back on, and then continue on in train smart. So, now, we're using dropout and if you look at again the training loss and the validation loss over these epochs that we're training, you actually see that the validation loss sticks a lot closer to the train loss as we train. So, here, with dropout, we've managed to at least reduce overfitting. So, the validation losses isn't as low as we got without dropout being is still, you can see that it's still dropping. So, if we kept training for longer, we would most likely manage to get our validation loss lower than without dropout.

### 19. PyTorch - Part 6-3ZJfo2bR-uw.en

Welcome back. So, in this video, I'm going to be showing you how to save and load models that you've trained with PyTorch. Again, this is important because most of the time you'll want to train your network on some data and then just save it to disk, and then you can later load it up and train more or use it for inference, like making predictions. So I've already gone ahead and ran most of this codes, trained my network on fashion hymnist. Again, we see we get about an accuracy of 84-85 percent. Now that the network is trained, we actually don't want to take these weights, never really learned this problem, and we want to save them so that we can in the future load them back up and use it again. These weights, these parameters, are actually stored in model.state_dict. So we print this out, so we can see our network here. So we have two hidden layers. So, this one goes to 500, 500 to 100, and then this goes to our output layer of 10 output units, and then we have drop out. So this is what our network looks like. If we look at the state_dict keys, this is showing us we have hidden layers at zero weight. So that's here. Hidden layers gives us a bias, the second hidden layer gives us the weight and also the weights and bias for our output layer. So that the tensors that for our weights and biases are actually stored and model.state_dict. So this is what typically you want to save. So to save the state_dict, it's pretty simple. So use the torch.save, and then pass in your model.state_dict and then call it checkpoint.pth. There you go. It's saved. So once it's saved, we can then load in the state_dict. So state_dict is equal to torch.load. Let's give it the same thing, checkpoint.pth, and then one p to state_dict, then we can print this out just see.keys. So it gives us the same layers that we saw up here, so in layers.0.weight. So, we all see the same thing. But the state_dict, it's self loaded. How do we actually load this into a model? So we say model.load_state_dict and we pass in our state_dict. So notice that we need to have our model existing already. We need to have it already created, and it's going to be created with randomly initialize the weights and biases. The parameters are going to be randomly initialized. Basically we just pass in our state dict and then those random parameters are replaced by the ones that we trained previously. This seems pretty straightforward, you just save your state_dict and you can load it backup into a model, but it's not actually all that simple, and I'll show you why. So, if I create a new network, 784 at normal, 10 output units, but this time I'm going to use three hidden layers with 400, 200 and 100 units each. Okay. So now, I have this pre-trained State_dict, right? So it's like, okay well I created this new model, I'm going to try to load this state_dict, and it gives me an error. So basically it says, inconsistent tensor size, expected tensor, this size but the sources is dense. Right? So basically what that means is that when I trained this state_dict with this checkpoint, I had a different network architecture. So that in the original network I had 500 units in the first layer, and now I have 400 units in the first layer. So, the thing is that when you build your network and you train it, when you load the state_dict back up, it has to go to a model with exactly the same architecture. So if you have code that can generate networks with different architectures when you load your your state_dict back up, you also have to include information about the architecture. So, for instance, here with this network since I have an arbitrary number of hidden layers, I need to include this information about the hidden layers in my checkpoint that I saved. To rebuild this model exactly as it was trained, I'm going to store the state_dict and all the information about the model architecture. Here I can say let's say, create a dictionary, given it my input size it's 784, my output size 10. So if I have a model, then I can do up features for each model. So remember here that model.hidden_layers is a list of these linear operations. So basically hidden_layers is a list of the layers in the network. So, to get the actual values I had set for the hidden_layers depending on the number of units, I just call out features on each of those layers. So this is basically just going through each of the layers, in this list of hidden layers and then getting the number of features or units in that layer. Then I can save the state_dict. Then once I have that dictionary, then I can simply save the dictionary. Checkpoint, checkpoint.pth again. There you go. Now our checkpoint has all the necessary information we need to rebuild our model. So what I'd like to do is create a function just called like load_checkpoint and give it a file path, and then we can load our checkpoint. So our checkpoint is torch.load(file path) then we can create our model. So our model is network, then we get our parameters for this model from our checkpoint. So we can put in the input size, we can put in the output size, we pass in the hidden layers. So what this does is passing in all the necessary arguments parameters to create our model. So, once we have our model, then we can load the state_dict from uh, I want to return the model. Then we can call the checkpoint, passed in checkpoint.pth. So then we load the model and we successfully load the state_dict and everything is present. So in general that's how you're going to save and load your networks. So, it's important to remember that every parameter that you use for building your network, you have to also include that in your checkpoints. So you can reconstruct your model exactly the same way it was when you saved it. Right. See you in the next video. Cheers

### 20. PyTorch - Part 7-hFu7GTfRWks.en

In this video, I'll be showing you how to load image data. This is really useful for what you'll be doing in real projects. So previously, we used MNIST. Fashion-MNIST were just toy datasets for testing your networks, but you'll be using full-size images like you'd get from smartphone cameras and your actual projects that you'll be doing with deep learning networks. So with this, we'll be using a dataset of cat and dog photos, super cute. That come from Kaggle. So, if you want to learn more about it, you can just click on this link. So, you can see our images are now much larger, much higher resolution and they're coming in different shapes and sizes than what we saw with MNIST and fashion-MNIST. So, the first step to using these is to actually load them in with PyTorch. Then once you have them in, you can train a network using these things. So, the easiest way to load in our image data is with datasets.ImageFolder. This is from torchvision, that datasets module. So basically, you just pass in a path to your dataset, so into the folder where your data is sitting into image folder and give us some transforms, which we talked about before. I'll go into some more detail about transforms next. So, the image folder, it expects your files and directories to look like this, where you have some root directory that's where all your data. Then each of the different classes has their own folder. So in this case, we have two classes. We have dog and cat. So, we have these two folders, dog and cat. Get more classes like for MNIST, now you have ten classes. There will be one folder for each of the different digits, right? Those are our classes or labels. Then within each of the specific class folders, you have your images that belong to those classes. So, in your dog folder are going to be all of your dog pictures and the cat folder are going to be all of your cat pictures. So, if you're working in a workspace, then the data should already be there, but if you're working on your local computer, you can get the data by clicking here. I've also already split this into a training set and test set for you. When you load in the image folder, you need to define some transforms. So, what I mean by this is you'll want to resize it, you can crop it, you can do a lot of things like typically you'll want to convert it to a PyTorch tensor and it is loaded in as a pillow image. So, you need to change the image into a tensor. Then you combine these transforms into a pipeline of transforms, using transforms.compose. So, if you want to resize your image to be 255 by 255, then you say transforms.resize 255 and then you take just the center portion, you just crop that out with a size of 224 by 224. Then you can convert it to a tensor. So, these are the transforms that you'll use and you pass this into ImageFolder to define the transforms that you're performing on your images. Once you have your dataset from your image folder, defining your transforms and then you pass that to dataloader. From here, you can define your batch size, so it's the number of images you get per batch like per loop through this dataloader and then you can also do things like set shuffle to true. So basically, what shuffle does is it randomly shuffles your data every time you start a new epoch. This is useful because when you're training your network, we prefer it the second time it goes through to see your images in a different order, the third time it goes through you see your images in a different order. Rather than just learning in the same order every time because then this could introduce weird artifacts in how your network is learning from your data. So, the thing to remember is that this dataloader that you get from this class dataloader, the actual dataloader object itself, is a generator. So, this means to get data out of it you actually have to loop through it like in a for loop or you need to call iter on it, to turn into an iterator. Then call next to get the data out of it. Really what's happening here in this for loop, this for images comma labels in dataloader is actually turning this into an iterator. Every time you go through a loop, it calls next. So basically, this for loop is an automatic way of doing this. Okay. So, I'm going to leave up to you is to define some transforms, create your image folder and then pass that image folder to create a dataloader. Then if you do everything right, you should see an image that looks like this. So, that's the basic way of loading in your data. You can also do what's called data augmentation. So, what this is is you want to introduce randomness into your data itself. What this can do is you can imagine if you have images, you can translate where a cat shows up and you can rotate the cat, you can scale the cat, you can crop different parts of things, you can mirror it horizontally and vertically. What this does is it helps your network generalized because it's seen these images in different scales, at different orientations and so on. This really helps your network train and will eventually lead to better accuracy on your validation tests. Here, I'll let you define some transforms for training data. So here, you want to do the data augmentation thing, where you're randomly cropping and resizing and rotating your images and also define transforms for the test dataset. So, one thing to remember is that for testing when you're doing your validation, you don't want to do any of this data augmentation. So basically, you just want to just do a resize and center crop of your images. This is because you want your validation to be similar to the eventual like in state of your model. Once you train your data, you're going to be sending in pictures of cats and dogs. So, you want your validation set to look pretty much exactly like what your eventual input images will look like. If you do all that correctly, you should see training examples are like this. So, you can see how these are rotated. Then you're testing examples should look like this, where they are scaled proportionally and they're not rotated. Once you've loaded this data, you should try to build a network based on what you've already learned that can then classify cats and dogs from this dataset. I should warn you this is actually a pretty tough challenge and it probably won't work. So, don't try too hard at it. Before you used MNIST and fashion-IMNIST. Those are very simple images, right? So, there are 20 by 28. They only have grayscale colors. But now these cat and dog images, they're much larger. Their colors, so you have those three channels. Just in general, it's going to be very difficult to build a classifier that can do this just using this fully connected network. The next part, I'll show you how to use a pre-trained network to build a model that can actually classify these cat and dog images. Cheers.

### 21. PyTorch V2 Part 7 Solution V1-d_NhvI1yEf0.en

Hello, and welcome back. So, here are my solutions for the exercises I had you do on loading image data. Here, I had you define some transforms and then load the actual dataset with image folder and then turn that into a data loader using this torch utils data loader class. So, here, I chose a couple transforms. So, first, I'm resizing the images to be 255 by 255 squares. So, basically, even if your image is actually a rectangle, then this will resize it to be square with 255 pixels on each size. The first transform I used was resize. So, this resizes your images to be squares with 255 pixels on each side. So, even if your original image is a rectangle, this will change it into a square. Then, I did a center crop with 224 pixels. So, this crops a square out of the center of the image with 224 pixels on each side. Then, I convert it into a tensor which we can then use in our networks. With the transform defined, we can pass that into this image folder and along with the path to our dataset and that creates a dataset object. Then, with the dataset object, we can pass that to data loader. So, this will give us back a generator were we actually can get our images and labels. So, here, I just chose a batch size of 32 and this shuffle set to true. So, basically, every time you loop through the generator again like multiple times, every time you do that, it'll randomly shuffle the images and labels. So, that loaded, here's what it looks like. We have a nice little dogs now here. So, here, I had you define transforms for our training data and our testing data. So, like I was saying before, with training data, you typically want to do data augmentation. So, that means rotating it, resizing it, flipping it, et cetera, to create this simulated dataset of more images than we actually have. Firstly, it just gives you more data to actually train with. But secondly, it helps the network generalize to images that aren't in the training set. So, my transformations here, I first chose to do a random rotation with 30 degrees. So, this is going to rotate in either direction up to 30 degrees. Then, I did a random resize crop. So, this is going to randomly resize the image and then take a crop from the center of 224 pixels square. Then, after that crop, then it do a random horizontal flip. So, it's going to mirror it horizontally and change it to a tensor. Then, with the test transforms kind of the same as before resize it to 255 pixels and then do a center crop 224, and it finally change it to a tensor. Then, here with the train data and test data, we can pass our data directories and our transforms through this image folder. I should actually load the data, and then give our loaded data to our data loaders to actually get our load our datasets so that we can see data from the train loader, it looks like this, and we can see data from our test loader, so like that.

### 24. PyTorch - Part 8-S9F7MtJ5jls.en

Hello everyone, welcome back. So in this network, we will be using a pre-trained network to solve this challenging problem of creating a classifier for your cat and dog images. These pre-trained networks were trained on ImageNet which is a massive dataset of over one million labeled images from 1,000 different categories. These are available from torchvision and this module, torchvision.models. And so, we see we have six different architectures that we can use, and here's a nice breakdown of the performance of each of these different models. So, AlexNet gives us the top one error and the top five error. So basically, as you see, some of these networks and these numbers here, 19, 11, 34, and so on, they usually indicate the number of layers in this model. So, the larger this number, the larger the model is. And accordingly, the larger the model is, you get better accuracy, you get lower errors. At the same time, again, the larger the model is, the longer it's going to take to compute your predictions and to train and all that. So when you're using these, you need to think about the tradeoff between accuracy and speed. So, all these networks use an architecture called convolutional layers. What these do, they exploit patterns and regularities in images. I'm not going to get into the details but if you want to learn more about them, you can watch this video. So we're saying, these deep learning networks are typically very deep. So that means, they have dozens or even hundreds of different layers, and they were trained on this massive ImageNet dataset. It turns out that they were astonishingly well as future detectors for images that they weren't trained on. So using a pre-trained network like this on a training set that it hasn't seen before is called transfer learning. So basically, what's learned from the ImageNet dataset is being transferred to your dataset. So here, we're going to use transfer learning to train our own network to classify our cat and dog photos. What you'll see is you'll get really good performance with very little work on our side. So again, you can download these models from torchvision.models, this model here, so we can include this in our imports, right here. Most of these pre-trained models require a 224 by 224 image as the input. You'll also need to match a normalization used when these models were trained on ImageNet. So when they train these models, each color channel and images were normalized separately. And you can see the means here and the standard deviations here. So, I'm going to leave it up to you to define the transformations for the training data and the testing data now. And if you're done, we can get to a new one. Now, let's see how we can actually load in one of these models. So here, I'm going to use the Densenet-121 model. So you see, it has very high accuracy on the ImageNet dataset and it's one 121 tells us that it has 121 layers. To load this in our code and use it, so we just say model models.densenet121 and then we say pretrained equals true. So this is going to download the pre-trained network, the weights, the parameters themselves, and then load it into our model. So now, we can do that and then we can look at what the architecture of this model. And this is what our DenseNet architecture looks like. So, you'll notice that we have this features part here and then a bunch of these layer. So this is like a convolutional layer which again I'm not going to talk about here but you don't really need to understand it to be able to actually use this thing. There's two main parts that we're interested in. So firstly, again, this features part, but then if we scroll all the way to the bottom, we also see this classifier part. So we see here is that we have the classifier. This has been defined as a linear combination layer, it's a fully connected dense layer, and it has 1,024 input features and then 1,000 output features. So again, the ImageNet dataset has 1,000 different classes. And so, the the number of outputs of this network should be 1,000 for each of those classes. So, the thing to know is that this whole thing was trained on ImageNet. Now, the features will work for other datasets but the classifier itself has been trained for ImageNet. So this is the part that we need to retrain, the classifier. We want to keep the feature part static. We don't want to update that, but we just need to update the classifier part. So then, the first thing we need to do is freeze our feature parameters. To do that, we go through our parameters in our model. And then, we just say, requires_grad equals false. So what this will do is that when we run our tensors through the model, it's not going to calculate the gradients. It's not going to keep track of all these operations. So firstly, this is going to ensure that our our feature parameters don't get updated but it also will speed up training because we're not keeping track of these operations for the features. Now, we need to replace the classifier with our own classifier. So here, I'm going to use a couple of new things. I'm going to use the sequential module available from PyTorch. And so, what this does, you basically just give it a list of different operations you want to do and then it will automatically pass a tensor through them sequentially. So, you can pass in an ordered dict to name each of these layers. So I'll show you how this works. So we want a fully connected layer, so I'll just name it FC1, and then that is a fully connected layer coming from 1,024 inputs and I'm going to say 500 for this hidden layer. And then we want to pass this through ReLu activation and then this should go through another fully connected layer and this will be our output layer. So, 500 to two, so we have cat and dog, so we want two outputs here. And finally, our output is going to be the LogSoftmax like before. Okay, and that is how we define the classifier. So now, we can take this classifier, just a classifier built from fully connected layers, and we can attach it to our model.classifier. So now, the new classifier that we built that is untrained is attached to our model and this model also has the features parts. The features parts are going to remain frozen. We're not going to update those weights but we need to train our new classifier. Now, if we want to train our network that we're using, this Densenet-121 is really deep and it has 121 layers. So, if we can try to train this on the CPU like normal, it's going to take pretty much forever. So instead, what we can do is use the GPU. GPUs are built specifically for doing a bunch of linear algebra computations in parallel and our neural networks are basically just a bunch of linear algebra computations. So if we run these on the GPU, they're done in parallel and we get something like 100 times increase speeds. In PyTorch, it's pretty straightforward to use the GPU. If you have your model, so model, the idea is that your model has all these parameters in there tensors that are sitting in your memory on your computer, but we can move them over to our GPU by saying model.cuda. So what this does is it moves the parameters for your model to the GPU and then all of the computations and the processing and are going to be done on the GPU. Similarly, if you have a tensor like your images, select images, if you want to run your images through your model, you have to make sure that the tensors that you're putting through your model or on the GPU if your model's on the GPU. So you just have to make those match up. So to do that, to move a tensor from computer to the GPU, you just, again, say images.cuda. So that will move a tensor, that's images, to the GPU. Then oftentimes, you'll want to move your model and your tensors back from the GPU to your local memory and CPU, and so, to do that, you just say like model.cpu or images.cpu, so this'll bring your tensors back from the GPU to your local computer to run on your CPU. Now, I'm going to give you a demonstration of how this all works and the amazing increased speed we get by using the GPU. So here, I'm just going to do for cuda and false, true. So this way, I'm going to be able to basically like loop through and try it once where we're not using the GPU, and once where we are using the GPU. So let's define my criterion which is going to be natural log_loss like we'd normally do, define our optimizer. So again, here, remember that we only want to update the parameters for the classifier. So we're just going to pass in model.classifier.parameters. This will work and that it's going to update the premise for our classifier but it's going to lead the parameters for the feature detector part of the model static. So I typically do is, say like, if cuda, then we want to move our model to the GPU. Otherwise, let's leave it on the CPU. And then I'm going to write a little training loop. We'll get our inputs and our labels, changes into variables like normal, then again, if we have cuda enabled, so if we have GPUs, then we can do inputs, labels, and we'll just move these over to the GPU. We're using the GPU now and we're also using this pre-trained network, but in general, you're going to do the training loop exactly the same way you have been doing it with these feed forward networks that you've been building. So first, I'm actually going to define a start time just so I can time things, then you just do your training pass like normal. So, you just do a forward pass through your model and you can calculate the loss, do your backward pass. Finally, update your weights with your optimizer. So I'm going to do here, I'm going to break this training loop after the first three iterations. So I want to time the difference between using a GPU and not using the GPU. What happens is the very first batch to go through the training loop tends to take longer than the other batches, so I'm just going to take the first three or four and then average over those just so we get a better sense of how long it actually takes to process one batch. So, that will just print out our training times. So we can see that if we're not using the GPU, then each batch takes five and a half seconds to actually go through this training step. Whereas, with the GPU, it only takes 0.012 seconds. So, I mean, this is a speedup of over 100 times. So here, I basically set cuda manually but you can also check if a GPU is available so you say torch.cuda is available, and this will give you back true or false depending if you have a GPU available that can use cuda. Okay, so from here, I'm going to let you finish training this model. So you can either continue with a DenseNet model that is already loaded or you can try ResNet which is also a good model to try out. I also really like VGGNet, I think that one's pretty good. It's really up to you. Cheers.

### 25. PyTorch V2 Part 8 Solution V1-4n6T93hKRD4.en

Hi everyone, here is my solution for the transfer learning exercise that I had to do. So, this one's going to be a little different. I'm going to be typing it out as I do it so you can understand my that process is kind of the combination of everything you've learned in this lesson. So, the first thing I'm going do is, if I have a GPU available, I'm going to write this code in agnostic way so that I can use the GPU. So, what I'm going to say, device = torch.device and then this is going to be cuda. So, it's going to run on our GPU if torch.cuda is available, else CPU. So, what this will do is, if our GPU is available, then this will be true and then we'll return cuda here and then otherwise, it'll be CPU. So, now we can just pass device to all the tensors and models and then it will just automatically go to the GPU if we have it available. So, next, I'm going to get our pre-trained model. So, here I'm actually going to use ResNet. So, to do this, model dot models. So, we already imported models from torch vision then we can kind of like took out all the ones they have. So, there's ResNet there. So, I'm just going to use a fairly small one, ResNet 50 and then we want pre-trained true and that should get us our model. So, now if we look, so we can just print it out like this and it will tell us all the different operations and layers and everything that's going on. So, if we scroll down, we can see that at the end here has fc. So, this is the last layer, this fully connected layer that's acting like a classifier. So, we can see that it has, it expects 2,048 inputs to this layer and then the out features are 1,000. So, remember that this was trained on ImageNet and so ImageNet is typically trained with 1,000 different classes of images. But here we're only using cat and dog, so we just need to output features and in our classifier. So, we can load the model like that and now I'm going to make sure that our models' perimeters are frozen so that when we're training they don't get updated. So, I'll just run this make sure it works. So, now, we can load the model and we can turn off gradients. Turn off gradients for our model. So, then, the next step is we want to define our new classifier which we will be training. So, here, we can make it pretty simple. So, models= nn.sequential. You can define this in a lot of different ways, so I'm just using an industrial sequential here. So, our first layer, so linear, so remember we needed 248 inputs and then let's say, let's drop it down to 512 at a ReLu layer, a dropout. Now our output layer, 512 to two and then we're going to do log softmax. I should change this to be a classifier. Okay. So, that's to finding our classifier and now we can attach it to our model, so to say, model.fc= classifier. Now, if we look at our model again, so we can scroll down to the bottom here. So, we see now this fully-connected module layer here is a sequential classifier linear operation ReLu, dropout, another linear transformation and then log softmax. So, the next thing we do is define my loss, my criterion. So, this is going to be the negative log like we had loss. Then, define our optimizer, optim.Adam. So, we want to use the parameters from our classifier which is fc here and then set our learning rate. The final thing to do is to move our model to whichever device we have available. So, now we have the model all set up and it's time to train it. Here's is the first thing I'm going to do is define some variables that we're going to be using during the training. So, for example, I'm going to set our epochs, so I'm going to set to one. I'll be tracking the number of train steps we do, so set that to zero. I'll be tracking our loss, so also set this to zero, and finally, we want to kind of set a loop for how many steps we're going to go before we print out the validation loss. So, now we want to loop through our epochs. So, for epoch and range epochs. Now, we're going to loop through our data for images, labels in trainloader, cumulate steps. So, basically, every time we go through one of these batches, we're going to increment steps here. So, now that we have our images and our labels, we're going to want to move them over to the GPU, if that's available. So, we're just going to do is images.to (device), labels.to(device). Now we're just going to write out our training loop. So, the first thing we need to do is zero our gradients. So, it's very important, don't forget to do this. Then, get our log probabilities from our model, model passed in the images with the log probabilities, we can get our loss from the criterion in the labels. Then do a backwards pass and then finally with our optimizer we take a step. Here, we can increment are running loss like so. So, this way we can keep track of our training loss as we are going through more and more data. All right. So, that is the training loop. So, now every once in a while so which is set by this like print every variable. We actually want to drop out of the train loop and test our network's accuracy and loss on our test dataset. So, for step modulo print_every, this is equal to zero, then we're going to go into our validation loop. So, what we need to do first is set model.eval. So, this'll turn our model into evaluation inference mode which turns off dropout. So, we can actually accurately use our network for making predictions instead a test loss and accuracy. So, now we'll get our images and labels from our test data. Now we'll do our validation loop. So, with our model so we'll pass in the images. So, these are the images from our test set. So, we're going to get our logps from our test set so again, get the loss with our criterion and keep track of our loss to test loss plus+= loss.item. So, this will allow us to keep track of our test loss as we're going through these validation rules. So, next we want to calculate our accuracy. So, probabilities= torch.exponential(logps). So, remember that our model is returning log softmax, so it's the log probabilities of our classes and to get the actual probabilities, we're going to use torch.exponential. So we get our top probabilities and top classes from ps.topk(1). So, that's going to give us our first largest value in our probabilities. Here, we need to make sure we set dimension to one to make sure it's actually like looking for the top probabilities along the columns. Go to the top classes, now we can check for equality with our labels and then with the equality tensor, we can update our accuracy. So, here remember we can calculate our accuracy from equality. Once we change it to a FloatTensor then we can do torch.mean and get our accuracy and so again just kind of incremented accumulated into this accuracy variable. All right. Now, we are in this loop here so this four step every print_every. So basically, now we have a running loss of our training loss and we have a test loss that we passed our test data through our model and measured the loss in accuracy. So now we can print all this out and I'm just going to copy and paste this because it's a lot to type. So, basically here, we're just printing out our epochs. So, we can keep track and know where we are and keep track of that. So, running_loss divided by print_every so basically we're taking the average of our training loss. So every time we print it out, we're just going to take the average. Then, test_loss over length the testloader. So basically length test loader tells us how many batches are actually in our test dataset that we're getting from testloader. So, since we're we're summing up all the losses for each of our batches, if we take the total loss and divide by the number of batches and that gives us our average loss, we do the same thing with accuracy. So, we're summing up the accuracy for each batch here and then we just divide by the total number of batches and that gives us our average accuracy for the test set. Then at the end, we can set our running loss back to zero and then we also want to put our model back into training mode. Great. So, that should be the training code and we'll see if it works. Now, this should be an if instead of a for. So, here I forgot this happens a lot. I forgot to transfer my tensors over to the GPU. So, hopefully this will work. All right. So, even like pretty quickly, we see that we can actually get our test accuracy on this above 95 percent. So, this is, remember that we're printing this out every five steps and so this is a total of 15 batches, training batches that were updated in the model. So, we're able to easily fine tune these classifiers on top and get greater than a 95 percent accuracy on our dataset.



## Part 02-Module 02-Lesson 03_Recurrent Neural Networks

### 02. RNN Vs LSTM-70MgF-IwAr8.en

Okay so, let's say we have a regular neural network which recognizes images and we fitted this image. And the neural neural network guesses that the image is most likely a dog with a small chance of being a wolf and an even smaller chance of being a goldfish. But, what if this image is actually a wolf? How would the neural network know? So, let's say we're watching a TV show about nature and the previous image before the wolf was a bear and the previous one was a fox. So, in this case, we want to use this information to hint to us that the last image is a wolf and not a dog. So, what we do is analyze each image with the same copy of a neural network. But, we use the output of the neural network as a part of the input of the next one. And, that will actually improve our results. Mathematically, this is simple. We just combine the vectors in a linear function, which will then be squished with an activation function, which could be sigmoid or hyperbolic tan. This way, we can use previous information and the final neural network will know that the show is about wild animals in the forest and actually use this information to correctly predict that the image is of a wolf and not a dog. And, this is basically how recurrent neural networks work. However, this has some drawbacks. Let's say the bear appeared a while ago and the two recent images are a tree and a squirrel. Based on those two, we don't really know if the new image is a dog or a wolf. Since trees and squirrels are just as associated to domestic animals as they are with forest animals. So, the information about being in the forest comes all the way back from the bear. But, as we've already experienced, information coming in gets repeatedly squished by sigmoid functions and even worse than that, training a network using back propagation all the way back, will lead to problems such as the vanishing gradient problem etc. So, by this point pretty much all the bear information has been lost. That's a problem with recurring neural networks; that the memory that is stored is normally short term memory. RNNs, have a hard time storing long term memory and this is where LSTMs or long short term memory networks will come to the rescue. So, as a small summary, an RNN works as follows; memory comes in and merges with a current event and the output comes out as a prediction of what the input is. And also, as part of the input for the next iteration of the neural network. And in a similar way, an LSTM works as follows; it keeps track not just of memory but of long term memory, which comes in and comes out. And also, short term memory, which also comes in and comes out. And in every stage, the long and short term memory in the event get merged. And from there, we get a new long term memory, short term memory and a prediction. In here, we protect old information more. If we deem it necessary, the network can remember things from long time ago. So, in the next few videos, I will show you the architecture of LSTMs and how they work.

### 03. LSTM Basics-gjb68a4XsqE.en

So let's recap. We have the following problem: we are watching a TV show and we have a long term memory which is that the show is about nature and science and lots of forest animal have appeared. We also have a short term memory which is what we have recently seen which is squirrels and trees. And we have a current event which is what we just saw, the image of a dog which could also be a wolf. And we want these three things to combine to form a prediction of what our image is. In this case, the long term memory which says that the show is about forest animals will give us a hint that the picture is of a wolf and not a dog. We also want the three pieces of information, long term memory, short term memory, and the event, to help us update the long term memory. So let's say we keep the fact that the show is about nature and we forget that it's about science. And we also remember that the show is about forest animals and trees since we recently saw a tree. So we add a bit and remove a bit to the long term memory. And finally we also want to use these three pieces of information to help us update the short term memory. So let's say in our short term memory you want to forget that the show has trees and remember that it has wolves since the trees happened a few images ago and we just saw a wolf. So basically we have an architecture like this and we use even more animals to represent our stages of memory. The long term memory is represented by an elephant since elephants have long term memory. The short term memory will be represented by a forgetful fish and the event will still be represented by the Wolf we just saw. So LSTM works as follows: the three pieces of information go inside the node and then some math happens and then the new pieces of information get updated and come out. There is a long term memory, a short term memory and the prediction of the event. More specifically the architecture of the LSTM contains a few gates. It contains a forget gate, a learn gate, a remember gate, and a use gate. And here's basically how they work. So the long term memory goes to the forget gate where it forgets everything that it doesn't consider useful. The short term memory and the event are joined together in the learn gate, containing the information that we've recently learned and it removes any unnecessary information. Now the long term memory that we haven't forgotten yet plus the new information that we've learned get joined together in the remember gate. This gate puts these two together and since it's called remember gate, what it does is it outputs an updated long term memory. So this is what we'll remember for the future. And finally, the use gate is the one that decides what information we use from what we previously know plus what we just learned to make a prediction so it also takes those inputs the long term memory, and the new information joins them and decides what to output. The output becomes both the prediction and the new short term memory. And so the big unfolded picture that we have is as follows: we have the long term memory and the short term memory coming in which we call LTM and STM. And then an event and an output are coming in and out of the LSTM. And then this passes to the next node, and so on and so forth. So in general at time t we label everything with an underscore t as we can see information passes from time t -1 to time t.

### 04. LSTM Architecture-ycwthhdx8ws.en

So in order to study the architecture of an LSTM, let's quickly recall the architecture of an RNN. Basically what we do is we take our event E_t and our memory M_t-1, coming from the previous point in time, and we apply a simple tanh or sigmoid activation function to obtain the output and then your memory M_t. So to be more specific, we join these two vectors and multiply them by a matrix W and add a bias b, and then squish this with the tanh function, and that gives us the output M_t. This output is a prediction and also the memory that we carry to the next node. The LSTM architecture is very similar, except with a lot more nodes inside and with two inputs and outputs since it keeps track of the long- and short-term memories. And as I said, the short-term memory is, again, the output or prediction. Don't get scared. These are actually not as complicated as they look. We'll break them down in the next few videos.

### 05. Learn Gate-aVHVI7ovbHY.en

So, let's keep this our base case. We have a long term memory which is at the show we're watching it's about nature and science. We also have a short term memory which is what we've recently seen, a squirrel and a tree. And finally, we have our current event which is a picture we just saw that looks like a dog but it could also be a wolf. So let's study the learn gate. What the learn gate does is the following. It takes a short term memory and the event and it joins it. Actually, it does a bit more. It takes the short term memory and the event and it combines them and then it ignores a bit of it keeping the important part of it. So here it forgets the fact that there's a tree and it remembers how we recently saw a squirrel and a dog/wolf. And how does this work mathematically? Well, it works like this. We have the short term memory STMt minus one and the event Et and it combines them by putting them through a linear function which consists of joining the vectors multiplying by a matrix adding a bias and finally squishing the result with a tanh activation function. Then the new information Nt has this form over here. Now, how do we ignore part of it? Well, by multiplying by an ignore factor, I-T. The ignore factor, I-T, is actually a vector but it multiplies element wise. And how do we calculate I-T? Well, we use our previous information of the short term memory and the event. So again, we create a small neural network whose inputs are the short term memory and the event. We'll pass them through a small linear function with a new matrix and a new bias and squish them with the sigmoid function to keep it between zero and one. So that's it. That's how the learn gate works.

### 06. Forget Gate-iWxpfxLUPSU.en

Now, we go to the Forget Gate, this one works as follows: It takes a long term memory and it decides what parts to keep and to forget. In this case, the show is about nature and science and the forget gate decides to forget that the show is about science and keep the fact that it's about nature. How does the Forget Gate work mathematically? Very simple. The long-term memory (LTM) from time T minus 1 comes in, and it gets multiplied by a Forget Factor ft. And how does the forget factor ft get calculated? Well, simple. We'll use a short term memory STM and the event information to calculate ft. So, just as before, we run a small one layer neural network with a linear function combined with the sigmoid function to calculate this Forget Factor and that's how the Forget Gate works.

### 07. Remember Gate-0qlm86HaXuU.en

And now we're going to learn the Remember Gate. This one is the simplest. It take the long-term memory coming out of the Forget Gate and the short-term memory coming out of the Learn Gate and simply combines them together. And how does this work mathematically? Again, very simple. We just take the outputs coming from the Forget Gate and from the Learn Gate and we just add them. That's it, that's all we do. And that's how the Remember Gate works.

### 08. LSTM 7 Use Gate-5Ifolm1jTdY.en

And finally, we come to the use gate or output gate. This is the one that uses the long term memory that just came out of the forget gate and the short term memory that just came out of the learned gate, to come up with a new short term memory and an output. These are the same thing. In this case, we'll take what's useful from the long term memory which is this bear over here, and what's useful from the short term memory which is these dark wolf, and the squirrel, and that's what's going to be our new short term memory. So our output basically says, your image is most likely a wolf but it also carry some of the other animals that I've seen recently. And mathematically what this does is the following: it applies a small neural network on the output of the forget gate using the tanh activation function, and it applies to another small neural network on the short term memory and the events using the sigmoid activation function. And as a final step, it multiplies these two in order to get the new output. The output also worth of the new short term memory. And that's how they use gate works.

### 09. Putting It All Together-IF8FlKW-Zo0.en

So here we go. As we've seen before, here is the architecture for an LSTM with the four gates. There is the forget gate, which takes the long-term memory and forgets part of it. The learn gate puts the short-term memory together with the event as the information we've recently learned. The remember gate joins the long-term memory that we haven't yet forgotten plus the new information we've learned in order to update our long-term memory and output it. And finally, the use gate also takes the information we just learned together with long-term memory we haven't yet forgotten, and it uses it to make a prediction and update the short-term memory. So this is how it looks all put together. It's not so complicated after all, isn't it? Now you may be thinking, wait a minute, this looks too arbitrary. Why use tanh sometimes and sigmoid other times? Why multiply sometimes and add other times, and other times apply a more complicated linear function? You can probably think of different architectures that make more sense or that are simpler, and you are absolutely right. This is an arbitrary construction. And as many things in machine learning, the reason why it is like this is because it works. And in the following section, we'll see some other architectures which can be simpler or more complex and that also do the job. But you're welcome to look for others and experiment. This is an area very much under development so if you come up with a different architecture and it works, that is wonderful.

### 10. Other Architectures-MsxFDuYlTuQ.en

In this video, I will show you a pair of similar architectures that also work well, but there are many variations to LSTMs and we encourage you to study them further. Here's a simple architecture which also works well. It's called the gated recurring unit or GRU for short. It combines the forget and the learn gate into an update gate and then runs this through a combine gate. It only returns one working memory instead of a pair of long- and short-term memories, but it actually seems to work in practice very well too. I won't go much into details, but in the instructor comments I'll recommend some very good reference to learn more about gated recurrent units. Here's another observation. Let's remember the forget gate. The forget factor f_t was calculating using as input a combination of the short-term memory and the event. But what about the long term memory? It seems like we left it away from the decision. Why does a long-term memory not have a say into which things get remembered or not? Well let's fix that. Let's also connect the long-term memory into the neural network that calculates the forget factor. Mathematically, this just means the input matrix is larger since we're also concatenating it with the long-term memory matrix. This is called a peephole connection since now the long-term memory has more access into the decisions made inside the LSTM. We can do this for every one of the forget-type nodes, and this is what we get: an LSTM with peephole connections.

### 12. 02 Time Series Prediction V2-xV5jHLFfJbQ.en

To introduce you to RNNs in PyTorch, I've created a notebook that will show you how to do simple time series prediction with an RNN. Specifically, we'll look at some data and see if we can create an RNN to accurately predict the next data point given a current data point, and this is really easiest to see in an example. So, let's get started. I'm importing our usual resources, and then I'm actually going to create some simple input and target training data. A classic example is to use a sine wave as input because it has enough variance and shape to be an interesting task, but it's also very predictable. So, I want to create a sample input and target sequence of data points of length 20, which I specify here as sequence length. Recall that RNNs are meant to work with sequential data, and so the sequence length is just the length of a sequence that it will look at as input. Often, the sequence length will indicate the number of words in a sentence or just some length of numerical data as is the case here. So, in these two lines, I'm just going to generate the start of a sine wave in a range from zero to Pi time steps. At first, I'm going to create a number of points that sequence length 20 plus 1, then I'm going to reshape my sine wave data to give it one extra dimension, the input size, which is just going to be one. Then, to create an input and target sequence of the length I want, I'm going to say an input X is equal to all but the last point in data, and the target Y is equal to all but the first point. So, X and Y should contain 20 data points and have an input size of one. Finally, I'm going to display this data using the same x-axis. You can see the input X is in red and the target Y is shifted over by one in blue. So, if we look at this point as an example at the same time step, Y is basically X shifted one time step in the future, and that's exactly what we want. So, now we have our training data and the next step is defining an RNN to learn from this data. We can define an RNN as usual, which is to say as a class using PyTorche's NN library. The syntax will look similar to how we've defined CNNs in the past. Let's actually click on the RNN documentation to read about the parameters that our recurrent layer takes in as input. So, here's the documentation for an RNN layer. We can see that this layer is responsible for calculating a hidden state based on its inputs. Now, to define a layer like this, we have these parameters: an input size, a hidden size, a number of layers and a few other arguments. The input size is just the number of input features, and in our specific case we're going to have inputs that are 20 values in sequence and one in input size features. This is like when we thought about the depth of an input image when we made CNN's. Next, we have a hidden size that defines how many features the output of an RNN will have and its hidden state. We also have a number of layers, which if it's greater than one, just means we're going to stack two RNNs on top of each other. Lastly, I want you to pay attention to this batch first parameter. If it is true, that means the input and output tensors that we provide are going to have the batch size as the first dimension, which in most cases that we go through will be true. So, this is how you define an RNN layer, and later in the forward function we'll see that it takes in an input and an initial hidden state, and it produces an output and a new hidden state. Back to our notebook. Here, I'm defining an RNN layer, self- doubt RNN. This RNN is taking in an input size and a hidden dimension that defines how many features the output of this RNN will have. Then it takes in a number of layers which allows you to create a stacked RNN if you want and this is typically a value kept between one and three layers. Finally, I'm setting batch first to true because I'm shaping the input such that the batch size will be the first dimension. Okay. Then to complete this model I have to add one more layer which is a final fully-connected layer. This layer is responsible for producing the number of outputs, output size that I want given the output of the RNN. So, all of these parameters are just going to be passed into our RNN when we create it. You'll also note that I'm storing the value of our hidden dimension so I can use it later in our forward function. In the forward function, I'm going to specify how a batch of input sequences will pass through this model. Note that this forward takes in an input X and the hidden state. The first thing I'm doing is grabbing the batch size of our input calling X dot size of 0. Then I'm passing my initial input and hidden state into the RNN layer. This produces the RNN output and a new hidden state. Then I'm going to call view on the RNN output to shape it into the size I want. In this case that's going to be batch size, times sequence length rows and the hidden dimension number of columns. This is a flattening step where I'm preparing the output to be fed into a fully-connected layer. So, I'll pass this shaped output to the final fully-connected layer, and return my final output here and my hidden state generated from the RNN. Now, as a last step here, I'm going to actually create some text data and to test RNN and see if it's working as I expect. The most common error I get when programming RNNs is that I've messed up the data dimension somewhere. So, I'm just going to check that there as I expect. So, here I'm just creating a test RNN with an input and output size of one, a hidden dimension of 10, and the number of layers equal to two, and you can change the hidden dimension and the number of layers. I basically just want to see that this is making the shape of outputs I expect. So, here I'm creating some test data that are sequence length along. I'm converting that data into a tensor datatype, and I'm squeezing the first dimension to give it a batch size of one as a first dimension. Then I'm going to print out this input size and I'll pass it into our test RNN as input. Recall that this takes an initial hidden state, and an initial one here is just going to be none. Then this should return an output and a hidden state, and I'm going to print out those sizes as well. Okay. So, our input size is a 3D tensor which is exactly what I expect. If first dimension is one our batch size, then 20 our sequence length, and finally our input number of features. Which is just one as we specified here. The output size is a 2D tensor. This is because in the forward function of our model definition actually smooshed the batch size and sequence length into one parameter. So, batch size times sequence length is 20 and then we have an output size of one. Finally we have our hidden state. Now, the first dimension here is our number of layers that I specified in the model definition two. Next we have the value one which is just the batch size of our input here. Finally, the last dimension here is 10 which is just our hidden dimension. So, all of these look pretty good and as I expect and I can proceed. Next I'll show you how to train a model like this.

### 13. 03 Training Memory V1-sx7T_KP5v9I.en

Last time, we defined a model, and next, I want to actually instantiate it and train it using our training data. First, I'll specify my model hyperparameters. The input and output will just be one, it's just one sequence at a time that we're processing and outputting, then I'll specify a hidden dimension which is just the number of features expect to generate with the RNN layer. I'll set this to 32, but for a small data set like this, I may even be able to go smaller. I'll set n_layers to one for now. So, I'm not stacking any RNN layers. I'll create this RNN and printed out. I should see the variables that I expect. My RNN layer with an input size and hidden dimension, and a linear layer with an input number of features and output number. Before training, I'm defining my loss and optimization functions. Now in this case, we're training our model to generate data points that are going to be basically coordinate values. So to compare a predicted and ground truth point like this, we'll use a regression loss because this is just a quantity rather than something like a class probability. So for the loss function, I'm going to use mean squared error loss which will just measure the distance between two points. I'll use an Adam optimizer which is standard for recurrent models, passing in my parameters and our learning rate. Next, I have a function train that's going to take in and RNN a number of steps to train for, and the parameter that will determine when it will print out law statistics. Now, at the very start of this function, I'm initializing my hidden state. At first, this is going to be nothing and it will default to a hidden state of all zeros. Then let's take a look at our batch loop. Now, this is a little unconventional, but I'm just generating data on the fly here according to how many steps we will train for. So, in these lines, I'm just generating a sequence of 20 sine wave values at a time. As we saw when I generated data at the start. Here, I'm getting my input x and a target y that's just shifted by one time step in the future. Here, I'm converting this data into tensors and squeezing the first dimension of our x_tensor to give it a batch size of one. Then I can pass my input tensor into my RNN model. So this is taking in my x input tensor and my initial hidden state at first. It produces a predicted output and a new hidden state. Next is an important part. I want to feed this new hidden state into the RNN as input at the next time step when we loop around once more. So I'm just copying the values from this produced hidden state into a new variable. This essentially detaches the hidden state from its history and I will not have to backpropagate through a series of accumulated hidden states. So this is what's going to be passed as input to the RNN at the next time step or next point in our sequence. So then, I have the usual training commands, I zero out any accumulated gradients. Calculate the loss, and perform a backpropagation in optimization step. Down here, I have some code to print out the loss and show what our input and predicted outputs are. Finally, this function returns a trained RNN which will be useful if you want to save a model for example. So, let's run this code. I'll choose to train our RNN, and that we defined above for 75 steps. I'll print out the result every 15 steps. We can see the mean squared error loss here and the difference between our red input in our blue output values. Recall that we want the blue output values to be one times step in the future when compared to the red ones. So it starts out pretty incorrect. Then we can see the loss decreases quite a lot after the first 15 steps. Our blue line is getting closer to our red one. As we train the blue predicted line gets closer to what we know our target is, at the end of 75 steps, our loss is pretty low. Our blue line looks very similar to what we know or output should be. If we look at the same time step for a red input dot, and a blue input dot, we we shouldn't see that the blue input is one time-step shifted in the future. It's pretty close. You could imagine getting even better performance after training for more steps or if you wanted to add more layers to your RNN. So, in this video, I wanted to demonstrate the basic structure of a simple RNN and show you how to keep track of the hidden state and represent memory over time as you train. You could imagine doing something very similar with data about world temperature or stock prices which are a little bit more complicated than this. But it will be really interesting to see if you could predict the future given that kind of data. Okay, so this is just an example, you can check out this code in our program GitHub, which is linked to below. I encourage you to play around with these model parameters until you have a good handle on the dimensions of an RNN input and output and how hyperparameters might change, how this model trains. Next, Matt and I will go over an exercise in generating text.

### 14. Character-Wise RNN-dXl3eWCGLdU.en

Coming up in this lesson you'll implement a character-wise RNN. That is, the network will learn about some text one character at a time and then generate new text one character at a time. Let's say, we want to generate new Shakespeare plays. As an example, to be or not to be. We'd pass the sequence into our RNN one character at a time. Once trained the network will generate new text by predicting the next character based on the characters it's already seen. So then to train this network we wanted to predict the next character in the input sequence. In this way the network will learn to produce a sequence of characters that look like the original text. Let's consider what the architecture of this network will look like. First, let's unroll the RNN so we can see how this all works as a sequence. Here, we have our input layer where we'll pass in the characters as one hot encoded vectors. These vectors go to the hidden layer. The hidden layer is built with LSTM cells where the hidden state and cell state pass from one cell to the next in the sequence. In practice, we'll actually use multiple layers of LSTM cells. You just stack them up like this. The output of these cells go to the output layer. The output layer is used to predict to the next character. We want the probabilities for each character the same way you did image classification with the cabinet. That means that we want a Softmax activation on the output. Our target here will be the input sequence but shifted over one so that each character is predicting the next character in the sequence. Again, we'll use cross entropy loss for training with gradient descent. When this network is trained up we can pass in one character and get out a probability distribution for the likely next character. Then we can sample from that distribution to get the next character. Then we can take that character, pass it in and get another one. We keep doing this and eventually we'll build up some completely new text. We'll be training this network on the text from Anna Karenina, one of my favorite books. It's in the public domain so it's free to use however you want. Also, it's an amazing novel.

### 15. Sequence-Batching-Z4OiyU0Cldg.en

One of the most difficult parts of building networks for me is getting the batches right. It's more of a programming challenge than anything deep learning specific. So here I'm going to walk you through how batching works for RNN. With RNNs we're training on sequences of data like text, stock values, audio etc. By taking a sequence and splitting it into multiple shorter sequences, we can take advantage of matrix operations to make training more efficient. In the fact, the RNN is training on multiple sequences in parallel. Let's look at a simple example, a sequence of numbers from 1 to 12. We can pass these into an RNN as one sequence. What's better. We could split it in half and pass in two sequences. The batch size corresponds to the number of sequences we're using. So here we'd say the batch size is 2. Along with the batch size we also choose the length of the sequences we feed to the network. For example, let's consider using a sequence length of 3. Then the first batch of data we pass into the network are the first 3 values in each mini sequence. The next batch contains the next three values and so on until we run out of data. We can retain the hidden state from one batch and use it at the start of the next batch. This way the sequence information is transferred across batches for each mini sequence. Next up you'll see how to actually build a recurrent network. Cheers.

### 17. 04 Implementing CharRNN V2-MMtgZXzFB10.en

This is a notebook where you'll be building a characterwise RNN. You're going to train this on the text of Anna Karenina, which is a really great but also quite sad a book. The general idea behind this, is that we're going to be passing one character at a time into a recurrent neural network. We're going to do this for a whole bunch of text, and at the end what's going to happen, is that our network is going to be able to generate new text, one character at a time. This is the general structure. We have our input characters and we want to one-hot encode them. This one-hot vector, will be fed into a hidden recurrent layer, and then the hidden layer has two outputs. First, it produces some RNN output, and it produces a hidden state, which will continue to change and be fed to this hidden layer at the next time step in the sequence. We saw something similar in the last code example. So, our recurrent layer keeps track of our hidden state, and its output goes to a final fully connected output layer. Our linear output layer, will produce a series of character class scores. So, this output will be as long as our input vector, and we can apply a Softmax function to get a probability distribution for the most likely next character. So, this network is based off of Andrej Karpathy's post on RNNs, which you can find here. It's a really good post and so you can check out these links to read more about RNNs. [inaudible] notebook is broken into a small series of exercises that you can implement yourself. For each exercise, I'm also going to provide a solution to consult. I recommend that you open the exercise notebook in one window and watch videos in another. That way you can work alongside me. Okay, so first things first, I'm loading in and taking a look at our text data. Here, I'm loading in the Anna Karenina text file and I'm printing out the first 100 characters. The characters are everything from letters, to spaces, to newline characters, and we can see the classic first-line, "Happy families are all alike. Every unhappy family is unhappy in its own way." Then, I'll actually want to turn our text into numerical tokens. This is because our network can only learn from numerical data, and so we want to map every character in the text to a unique index. So, first off, with the text, we can just create a unique vocabulary as a set. Sets, are a built in python data structure, and what this will do, is look at every character in the past in the text. Separate it out as a string and get rid of any duplicates. So, chars, is going to be a set of all our unique characters. This is also sometimes referred to as a vocabulary. Then, I'm creating a dictionary from a vocabulary of all our characters, that maps the actual character to a unique integer. So, it's just giving a numerical value to each of our unique characters, and putting it in a dictionary int2char. Then I'm doing this the other way, where we have a dictionary that goes from integers to characters. Recall that any dictionary is made of a set of key and value pairs. In the int2char case, the keys are going to be integers and the values are going to be string characters. In the char2int case, our keys are going to be the characters and our values are going to be their unique integers. So, these basically give us a way to encode text as numbers. Here, I am doing just that. I'm encoding each character in the text as an integer. This creates an encoded text, and just like I printed the first 100 characters before, I can print the first 100 encoded values. If you look at the length of our unique characters, you'll see that we have 83 unique characters in the text. So, our encoded values will fall in this range. You can also see some repeating values here like 82, 82, 82 and 19,19. If we scroll back up to our actual text, we can surmise that the repeated 82s are probably this new line character, and 19 is maybe a p. Okay, so are encodings are working, and now what we want to do, is turn these encodings into one-hot vectors, that our RNN can take in as input, just like in our initial diagram. Here, I've actually written a function that takes in an encoded array, and turns it into a one-hot vector of some specified length. I can show you what this does with an example below. I've made a short test sequence three, five, one and a vector length that I specify, eight. So, I'm passing this test sequence and the number of labels that I expect into our one-hot function. I can see that the result is an array of three one-hot vectors. All of these vectors are of length eight and the index three, five, and one are on for their respective encodings. Now, for our vocabulary of 83 characters, these are just going to be much longer vectors. Cool. So, we have our preprocessing functions and data in place, and now your first task will be to take our encoded characters, and actually turn them into mini batches that we can feed into our network. So, as Matt mentioned before, the idea is that we actually want to run multiple sequences through our network at a time. Where one mini batch of data contains multiple sequences. So, here's an example starting sequence. If we say we want a batch size of two, we're going to split this data into two batches. Then, we'll have these sequence length windows that specify how big we want our sequences to be. In this case, we have a sequence length of three, and so our window will be three in width. For a batch size of two and sequence length of three these values will make up our first mini-batch. We'll just slide this window over by three to get the next mini-batch. So, each mini-batch is going to have the dimensions batch size by sequence length. In this case, we have a two by three window on are encoded array that we pass into our network. If you scroll down, I have more specific instructions. The first thing you're going to be doing is taking in an encoded array, and you'll want to discard any values that don't fit into completely full mini-batches. Then, you want to reshape this array into batch size number of rows. Finally, once you have that batch data, you're going to want to create a window that iterates over the batches a sequence length at a time, to get your mini batches. So, here's the skeleton code. Your array is going to be some encoded data, then you have a batch size and sequence length. Basically, you want to create an input x that should be a sequence length or number of timesteps wide and a batch size tall. This will make up our input data and you'll also want to provide targets. The targets y, for this network are going to be just like the input characters x, only shifted over by one. That's because we want our network to predict the most likely next character more some input sequence. So, you'll have your input sequence x and our targets y shifted over by one. Then finally, when we [inaudible] batches, we're going to create a generator that iterates through our array and returns x and y with this yield command. Okay, I'll leave implementing this batching function up to you. You can find more information about how you could do this in the notebook. There's some code for testing out your implementation below. In fact, this is what your batches should look like when you run this code. If you need any help or you just want to see my solution, go ahead and check out the solution video next.

### 18. 05 Batching Data V1-9Eg0wf3eW-k.en

So, this is my complete get_batches code that generates mini-batches of data. So, the first thing I wanted to do here is get the total number of complete batches that we can make in batches. To do that, I first calculated how many characters were in a complete mini-batch. So, in one mini-batch, there's going to be batch size times sequence length number of characters. Then, the number of complete batches that we can make is just the length of the array divided by the total number of characters in a mini-batch. This double slash is an integer division which we'll just round down any decimal leftover from this division. With that, we have the number of completely full batches that we can make. Then, we get our array and we take all the characters in the array up to n_batches times this total character size for a mini-batch. So here, we're making sure that we're keeping only enough characters to make full batches, and we may lose some characters here. But in general, you're going to have enough data that getting rid of a last unfold batch is not really going to matter. Next, with reshaping, we can take our array and we can make the number of rows equal to our batch size, and that's just how many sequences we want to include in a mini-batch. So, we just say we want the number of rows to be batch size and then we put this negative one. Negative one here is kind of a dimension placeholder, and it'll just automatically fill up the second dimension to whatever size it needs to be to accommodate all the data. Then finally, I'm iterating over my batch data using a window of length sequence length. So here, I'm taking in a reshaped complete array and then looking at all our rows, all our batches, and the columns are in a range from n to n plus sequence length, which makes our sequence length window. This completes x our input mini-batch. Then, what I did here for the target y is I just initialized an array of all zeros that's the same shape as x, and I just kind of fill it up with values from our x array shifted by one. From the start to the end, I just shifted over x by one. Then in the case of reaching the very end of our array, I'm going to make the last element of y equal to the first element in our array. I'm not super sure why most people do it this way, wrapping our array around so that the last element of y is the first element of x. But I've seen this many times, and so I did it in the cyclical way and it seems like the network trains perfectly fine doing this. So, it does not seem to be a problem. The main thing is we want x and y to be the same size. So, if you did this right and you want to test your implementation, you should have gotten batches that looks something like this. Right here, we have a batch size of eight, so you have eight rows here, and then we're just printing out the first 10 items in a sequence. So, you should see 10 items here. The important thing to note here is that you want to make sure that the elements in x, like the actual encoded values, are shifted over by one in y. So, we have 51 as the first item here and as the zeroth item here, then 23 and 23. Likewise, 55 is here and 55 is here in y. I basically want to make sure that everything is shifted over correctly, and this looks good. So now that we have our batch data, the next step we'll talk about is actually building the network.

### 19. 06 Defining Model V2-_LWzyqq4hCY.en

All right. So, we have our mini batches of data and now it's time to define our model. This is a little diagram of what the model will look like. We'll have our character's put into our input layer and then a stack of LSTM cells. These LSTM cells make up our hidden recurrent layer and when they look at a mini batch of data as input they'll look at one character at a time and produce an output and a hidden state. So, we will pass an input character into our first LSTM cell which produces a hidden state. Then at the next time step, we'll look at the next character in our sequence and pass that into this LSTM cell which will see the previous hidden state as input. You have so far seen this behavior in a one layer RNN but in this case we plan on using a two-layer model that has stacked LSTM layers and that means that the output of this LSTM layer is going to go to the next one as input and each of these cells is sharing its hidden state with the next cell in the unrolled series. Finally, the output of the last LSTM layer will include some character class scores that will be the length of our vocabulary. We'll put this through a Softmax activation function which we'll use to get the probability distribution for predicting the most likely next character. So, to start you off on this task, you've been given some skeleton code for creating a model. First, we're going to check to see if a GPU is available for training then you'll see this class character RNN. You can see that this character RNN class has our usual init and forward functions and later you've been given some code to initialize the hidden state of an LSTM layer and I'll go over this in a moment. You can definitely take a look at this given code and how we're creating our initial character dictionaries but you won't need to change it. We also have several parameters that are going to be passed in when a character RNN is instantiated and I've saved some of these as class variables. So, using these input parameters and variables, it will be up to you to create our model layers and complete the forward function. You'll first create an LSTM layer which you can read about in the documentation here. We can see that an LSTM layer is created using our usual parameters; an input size, hidden size, number of layers, and a batch first parameter. We'll also add a dropout value. This introduces a dropout layer in between the outputs of LSTM layers if you've decided to stack multiple layers. So, after you define an LSTM layer, I'll ask you to define two more layers; one dropout layer and a final fully-connected layer for getting our desired output size. Once you've defined these layers, you'll move on to define the forward function. This takes in an input x and hidden state. You'll pass this input through the layers of the model and return a final output and hidden state. You'll have to make sure to shape the LSTM output so that it can be fed into the last fully connected layer. Okay. Then at the bottom here, you'll see this function for initializing the hidden state of an LSTM. An LSTM has a hidden and a cell state that are saved as a tuple hidden. The shape of the hidden and cell state is defined first by the number of layers in our model, the batch size of our input, and then the hidden dimension that we specified in model creation. In this function, we're initializing the hidden weights all to zero and moving them to GPU if it's available. Okay, so all the code that you see you don't need to change, you just need to define the model layers and feedforward behavior. If you've implemented this correctly, you should be able to set your model hyperparameters and proceed with training and generating some sample text. Try this out on your own then next, check out my solution.

### 20. 07 CharRNN Solution V1-ed33qePHrJM.en

We wanted to define a character RNN with a two layer LSTM. Here in my solution, I am running this code on GPU and here's my code for defining our character level RNN. First, I defined an LSTM layer, self.lstm. This takes in an input size, which is going to be the length of a one-hot encoded input character and that's just the length of all of my unique characters. Then, it takes a hidden dimension a number of layers and a dropout probability that we've specified. Remember that this will create a dropout layer in between multiple LSTM layers, and all of these are parameters that are going to be passed in as input to our RNN when it's constructed. Then, I've set batch first to true because when we created our batch data, the first dimension is the batch size, rather than the sequence length. Okay. Next, I've defined a dropout layer to go in-between my LSTM and a final linear layer. Then, I have FC, my final fully connected linear layer. This takes in our LSTM outputs, which are going to be dimension and hidden. It's going to output our character class scores for the most likely next character. So, these are the class scores for each possible next character. This output size is the same size as our input, the length of our character vocabulary. Then, I move to the forward function. I'm passing my input X and a hidden state to my LSTM layer here. This produces my LSTM output and a new hidden state. I'm going to pass the LSTM output through the dropout layer that I defined here to get a new output. Then, I'm making sure to reshape this output, so that the last dimension is our hidden dim. This negative one basically means I'm going to be stacking up the outputs of the LSTM. Finally, I'm passing this V-shaped output to the final fully connected layer. Then, I'm returning this final output and the hidden state that was generated by our LSTM. These two functions in addition to the init hidden function complete my model. Next, it's time to train and let's take a look at the training loop that was provided. This function takes in a model to train some data, and the number of epics to train for, and a batch size, and sequence length that define our mini batch size. It also takes in a few more training parameters. First in here, I've defined my optimizer and my loss function. The optimizer is a standard Adam optimizer with a learning rate set to the past and learning rate up here. The last function is cross entropy loss, which is useful for when we're outputting character class scores. Here, you'll see some details about creating some validation data and moving our model to GPU if it's available. Here, you can see the start of our epic loop. At the start of each epic, I'm initializing the hidden state of our LSTM. Recall that this takes in the batch size of our data to define the size of the hidden state and it returns a hidden in cell state that are all zeros. Then, inside this epic loop, I have my batch loop. This is getting our X and Y mini batches from our get batches generator. Remember that this function basically iterates through are encoded data, and returns batches of inputs X and targets Y. I'm then converting the input into a one-hot encoded representation, and I'm converting both X and Y are inputs and targets into Tensors that can be seen by our model. If GPU's available, I'm moving those inputs and targets to our GPU device. The next thing that you see is making sure that we detach any past in hidden state from its history. Recall that the hidden state of an LSTM layer is a Tuple, and so here, we are getting the data as a tuple. Then, we proceed with back propagation as usual. We zero out any accumulated gradients and pass in our input Tensors to our model. We also pass in the latest hidden state here. In this returns of final output and a new hidden state, then we calculate the loss by looking at the predicted output and the targets. Recall that in the forward function of our model, I smashed the batch size and sequence length of our LSTM outputs into one dimension, and so I'm doing the same thing for our targets here. Then, we're performing back propagation and moving one step in the right direction updating the weights of our network. Now before the optimization step, I've added one line of code that may look unfamiliar. I'm calling clip grad norm. Now, this kind of LSTM model has one main problem with gradients. They can explode and get really, really big. So, what we do is we can clip the gradients, we just set some clip threshold, and then if the gradient is larger than that threshold, we set it to that clip threshold, and encode we do this by just passing in the parameters and the value that we want to clip the gradients at. In this case, this value is passed in, in our train function as a value five. Okay. So, we take a backwards step, then we clip our gradients, and we perform an optimization step. At the end here, I'm doing something very similar for processing our validation data except not performing the back propagation step. Then, I'm printing out some statistics about our loss. Now with this train function defined, I can go about instantiating and training a model. In the exercise notebook, I've left these hyper parameters for you to define. I've set our hidden dimension to the value of 512 and a number of layers up two, which we talked about before. Then, I have instantiated our model, and printed it out, and we can see that we have 83 unique characters as input, 512 as a hidden dimension, and two layers in our LSTM. For a dropout layer, we have the default dropout value of 0.5 and for our last fully connected layer, we have our Input features, which is the same as this hidden dimension and our output features, the number of characters. Then, there are more hyper parameters that define our batch size sequence length and number of epics to train for. Here, I've set the sequence length to 100, which is a lot of characters, but it gives our model a great deal of context to learn from. I also want to note that the hidden dimension is basically the number of features that your model can detect. Larger values basically allow a network to learn more text features. There's some more information below in this notebook about defining hyper parameters. In general, I'll try to start out with a pretty big model like this, multiple LSTM layers and a large hidden dimension. Then, I'll basically take a look at the loss as this model trains and if it's decreasing, I'll keep going. But if it's not decreasing as I expect, then I'll probably change some hyper parameters. Our text data is pretty large and here, I've trained our entire model for 20 epics on GPU. I can see the training and validation loss over time decreasing. Around epic 15, I'm seeing the lost slow down a bit. But it actually looks like the validation and training loss are still decreasing even after epic 20. I could have stood to train for an even longer amount of time. I encourage you to read this information about setting the hyper parameters of a model and really getting the best model. Then, after you've trained a model like I've just done, you can save it by name and then there's one last step, which is using that model to make predictions and generate some new text, which I'll go over next.

### 21. 08 Making Predictions V3-BhrpV3kwATo.en

Now, the goal of this model is to train it so that it can take in one character and produce a next character and that's what this next step, Making Predictions is all about. We basically want to create functions that can take in a character and have our network predict the next character. Then, we want to take that character, pass it back in, and get more and more predicted next characters. We'll keep doing this until we generate a bunch of text. So, you've been given this predict function which will help with this. This function takes in a model and occurring character and its job is to basically give us back the encoded value of the predictive next character and the hidden state that's produced by our model. So, let's see what it's actually doing step-by-step. It's taking in our input character and converting it into it's encoded integer value. Then, as part of pre-processing, we're turning that into a one-hot encoded representation and then converting these inputs into a tensor. These inputs we can then pass to our model, and then you'll see a couple of steps that are really similar to what we saw in our training loop. We put our inputs on a GPU if it's available and we detach our hidden state from its history here. Then, we pass in the inputs and the hidden state to our model which returns an output and a new hidden state. Next, we're processing the output a little more. We're applying a softmax function to get p probabilities for the likely next character. So, p is a probability distribution over all the possible mixed characters given the input character x. Now, we can generate more sensible characters by only considering the k most probable characters. So, here we're giving you a couple of lines of code to use top k sampling, which finds us the k most likely next characters. Then, here we're adding an element of randomness, something that selects from among those top likely next characters. So, then we have a most likely next character and we're actually returning the encoded value of that character and the hidden state produced by our model, but we'll basically want to call the predict function several times, generating one character's output, then passing that in as input and predicting the next and next characters. That brings me to our next function sample. Sample will take in our trained model and the size of text that we want to generate. It will also take in prime, which is going to be a set of characters that we want to start our model off with. Lastly, we will take in a value for top k which will just return our k most probable characters in our predict function. So, in here, we're starting off by moving our model to GPU if it's available, and here we're also initializing the hidden state with a batch size of one because, for one character that we're inputting at a time, the batch size will be one. In this way, prediction is quite different than training a model. Then, you'll see that we're getting each character in our prime word. The prime word basically helps us answer the question, how do we start to generate text? We shouldn't just start out randomly. So, what is usually done is to provide a prime word or a set of characters. Here the default prime set is just the, T-H-E, but you can pass in any set of characters that you want as the prime. The sample function first processes these characters in sequence adding them to a list of characters. It then calls predict on these characters passing in our model, each character and hidden state and this returns the next character after our prime sequence and the hidden state. So, here we have all our prime characters in the default case. This is going to be T, H, and E and then we're going to append the next most likely character. So, we're basically building up a list of characters here, then we're going to generate more and more characters. In this loop, we're passing in our model and the last character in our character list. This returns the next character and the hidden state. This character is appended to our list and the cycle starts all over again. So, predict is generating a next likely character which is appended to our list and then that goes back as input into our predict function. The effect is that we're getting next and next and next characters and adding them to our characters list, that is until we reach our desired text length. Finally, we join all these characters together to return a sample text, and here I've generated a couple samples. You can see that I've passed in my model that was trained for 20 epochs, and I said, generate a text that's 1,000 characters long starting with the prime word Anna. I've also passed in a value for top k equal to five. You can see that this starts with the prime word and generates what might be thought of as a paragraph of text in a book. Even with just a few prime characters, our model is definitely making complete and real words that make sense. The structure and spelling looks pretty good even if the content itself is a little confusing, and here's another example where I've loaded in a model by name and I'm using this loaded model to generate a longer piece of text, starting with the prime words, "And Levin said." So, this is pretty cool. A well-trained model can actually generate some text that makes some sense. It learned just from looking at long sequences of characters what characters were likely to come next. Then in our sampling and prediction code, we used top-k sampling and some randomness in selecting the best likely next character. You can train a model like this on any other text data. For example, you could try it on generating Shakespeare sonnets or another text of your choice. Great job on getting this far. You've really learned a lot about implementing RNNs in PyTorch.



## Part 02-Module 02-Lesson 04_Training Neural Networks

### 02. Training Optimization-UiGKhx9pUYc.en

So by now we've learned how to build a deep neural network and how to train it to fit our data. Sometimes however, we go out there and train on ourselves and find out that nothing works as planned. Why? Because there are many things that can fail. Our architecture can be poorly chosen, our data can be noisy, our model could maybe be taking years to run and we need it to run faster. We need to learn ways to optimize the training of our models and this is what we'll do next.

### 03. Testing-EeBZpb-PSac.en

So let's look at the following data form by blue and red points, and the following two classification models which separates the blue points from the red points. The question is which of these two models is better? Well, it seems like the one on the left is simpler since it's a line and the one on the right is more complicated since it's a complex curve. Now the one in the right makes no mistakes. It correctly separates all the points, on the other hand, the one in the left does make some mistakes. So we're inclined to think that the one in the right is better. In order to really find out which one is better, we introduce the concept of training and testing sets. We'll denote them as follows: the solid color points are the training set, and the points with the white inside are the testing set. And what we'll do is we'll train our models in the training set without looking at the testing set, and then we'll evaluate the results on that testing to see how we did. So according to this, we trained the linear model and the complex model on the training set to obtain these two boundaries. Now we reintroduce the testing set and we can see that the model in the left made one mistake while the model in the right made two mistakes. So in the end, the simple model was better. Does that match our intuition?. Well, it does, because in machine learning that's what we're going to do. Whenever we can choose between a simple model that does the job and a complicated model that may do the job a little bit better, we always try to go for the simpler model.

### 04. Underfitting And Overfitting-xj4PlXMsN-Y.en

So, let's talk about life. In life, there are two mistakes one can make. One is to try to kill Godzilla using a flyswatter. The other one is to try to kill a fly using a bazooka. What's the problem with trying to kill Godzilla with a flyswatter? That we're oversimplifying the problem. We're trying a solution that is too simple and won't do the job. In machine learning, this is called underfitting. And what's the problem with trying to kill a fly with a bazooka? It's overly complicated and it will lead to bad solutions and extra complexity when we can use a much simpler solution instead. In machine learning, this is called overfitting Let's look at how overfitting and underfitting can occur in a classification problem. Let's say we have the following data, and we need to classify it. So what is the rule that will do the job here? Seems like an easy problem, right? The ones in the right are dogs while the ones in the left are anything but dogs. Now what if we use the following rule? We say that the ones in the right are animals and the ones in the left are anything but animals. Well, that solution is not too good, right? What is the problem? It's too simple. It doesn't even get the whole data set right. See? It misclassified this cat over here since the cat is an animal. This is underfitting. It's like trying to kill Godzilla with a flyswatter. Sometimes, we'll refer to it as error due to bias. Now, what about the following rule? We'll say that the ones in the right are dogs that are yellow, orange, or grey, and the ones in the left are anything but dogs that are yellow, orange, or grey. Well, technically, this is correct as it classifies the data correctly. There is a feeling that we went too specific since just saying dogs and not dogs would have done the job. But this problem is more conceptual, right? How can we see the problem here? Well, one way to see this is by introducing a testing set. If our testing set is this dog over here, then we'd imagine that a good classifier would put it on the right with the other dogs. But this classifier will put it on the left since the dog is not yellow, orange, or grey. So, the problem here, as we said, is that the classifier is too specific. It will fit the data well but it will fail to generalize. This is overfitting. It's like trying to kill a fly with a bazooka. Sometimes, we'll refer to overfitting as error due to variance. The way I like to picture underfitting and overfitting is when studying for an exam. Underfitting, it's like not studying enough and failing. A good model is like studying well and doing well in the exam. Overfitting is like instead of studying, we memorize the entire textbook word by word. We may be able to regurgitate any questions in the textbook but we won't be able to generalize properly and answer the questions in the test. But now, let's see how this would look like in neural networks. So let's say this data where, again, the blue points are labeled positive and the red points are labeled negative. And here, we have the three little bears. In the middle, we have a good model which fits the data well. On the left, we have a model that underfits since it's too simple. It tries to fit the data with the line but the data is more complicated than that. And on the right, we have a model that overfits since it tries to fit the data with an overly complicated curve. Notice that the model in the right fits the data really well since it makes no mistakes, whereas the one in the middle makes this mistake over here. But we can see that the model in the middle will probably generalize better. The model in the middle looks at this point as noise while the one in the right gets confused by it and tries to feed it too well. Now the model in the middle will probably be a neural network with a slightly complex architecture like this one. The one in the left will probably be an overly simplistic architecture. Here, for example, the entire neural network is just one preceptors since the model is linear. The model in the right is probably a highly complex neural network with more layers and weights than we need. Now here's the bad news. It's really hard to find the right architecture for a neural network. We're always going to end either with an overly simplistic architecture like the one in the left or an overly complicated one like the one in the right. Now the question is, what do we do? Well, this is like trying to fit in a pair of pants. If we can't find our size, do we go for bigger pants or smaller pants? Well, it seems like it's less bad to go for a slightly bigger pants and then try to get a belt or something that will make them fit better, and that's what we're going to do. We'll err on the side of an overly complicated models and then we'll apply certain techniques to prevent overfitting on it.

### 05. Model Complexity Graph-NnS0FJyVcDQ.en

So, let's start from where we left off, which is, we have a complicated network architecture which would be more complicated than we need but we need to live with it. So, let's look at the process of training. We start with random weights in her first epoch and we get a model like this one, which makes lots of mistakes. Now as we train, let's say for 20 epochs we get a pretty good model. But then, let's say we keep going for a 100 epochs, we'll get something that fits the data much better, but we can see that this is starting to over-fit. If we go for even more, say 600 epochs, then the model heavily over-fits. We can see that the blue region is pretty much a bunch of circles around the blue points. This fits the training data really well, but it will generalize horribly. Imagine a new blue point in the blue area. This point will most likely be classified as red unless it's super close to a blue point. So, let's try to evaluate these models by adding a testing set such as these points. Let's make a plot of the error in the training set and the testing set with respect to each epoch. For the first epoch, since the model is completely random, then it badly misclassifies both the training and the testing sets. So, both the training error and the testing error are large. We can plot them over here. For the 20 epoch, we have a much better model which fit the training data pretty well, and it also does well in the testing set. So, both errors are relatively small and we'll plot them over here. For the 100 epoch, we see that we're starting to over-fit. The model fits the data very well but it starts making mistakes in the testing data. We realize that the training error keeps decreasing, but the testing error starts increasing, so, we plot them over here. Now, for the 600 epoch, we're badly over-fitting. We can see that the training error is very tiny because the data fits the training set really well but the model makes tons of mistakes in the testing data. So, the testing error is large. We plot them over here. Now, we draw the curves that connect the training and testing errors. So, in this plot, it is quite clear when we stop under-fitting and start over-fitting, the training curve is always decreasing since as we train the model, we keep fitting the training data better and better. The testing error is large when we're under-fitting because the model is not exact. Then it decreases as the model generalizes well until it gets to a minimum point - the Goldilocks spot. And finally, once we pass that spot, the model starts over-fitting again since it stops generalizing and just starts memorizing the training data. This plot is called the model complexity graph. In the Y-axis, we have a measure of the error and in the X-axis we have a measure of the complexity of the model. In this case, it's the number of epochs. And as you can see, in the left we have high testing and training error, so we're under-fitting. In the right, we have a high testing error and low training error, so we're over-fitting. And somewhere in the middle, we have our happy Goldilocks point. So, this determines the number of epochs we'll be using. So, in summary, what we do is, we degrade in descent until the testing error stops decreasing and starts to increase. At that moment, we stop. This algorithm is called Early Stopping and is widely used to train neural networks.

### 06. DL 53 Q Regularization-KxROxcRsHL8.en

Now let me show you a subtle way of overfitting a model. Let's look at the simplest data set in the world, two points, the point one one which is blue and the point minus one minus one which is red. Now we want to separate them with a line. I'll give you two equations and you tell me which one gives a smaller error and that's going to be the quiz. Equation one is x1 plus x2. That means w1 equals w2 is one and the bias b is zero. And then equation two is 10x1 plus 10x2. So that means w1 equals w2 equals 10 and the bias b equals zero. Now the question is, which prediction gives a smaller error? This is not an easy question but I want you to think about it and maybe make some calculations, if necessary.

### 07. Regularization-ndYnUrx8xvs.en

Well the first observation is that both equations give us the same line, the line with equation X1+X2=0. And the reason for this is that solution two is really just a scalar multiple of solution one. So let's see. Recall that the prediction is a sigmoid of the linear function. So in the first case, for the 0.11, it would be sigmoid of 1+1, which is sigmoid of 2, which is 0.88. This is not bad since the point is blue, so it has a label of one. For the point (-1, -1), the prediction is sigmoid of -1+-1, which is sigmoid of -2, which is 0.12. It's also not best since a point label has a label of zero since it's red. Now let's see what happens with the second model. The point (1, 1) has a prediction sigmoid of 10 times 1 plus 10 times 1 which is sigmoid of 20. This is a 0.9999999979, which is really close to 1, so it's a great prediction. And the point (-1, -1) has prediction sigmoid of 10 times negative one plus 10 times negative one, which is sigmoid of minus 20, and that is 0.0000000021. That's a really, really close to zero so it's a great prediction. So the answer to the quiz is the second model, the second model is super accurate. This means it's better, right? Well after the last section you may be a bit reluctant since this hint's a bit towards overfitting. And your hunch is correct. The problem is overfitting but in a subtle way. Here's what's happening and here's why the first model is better even if it gives a larger error. When we apply sigmoid to small values such as X1+X2, we get the function on the left which has a nice slope to the gradient descent. When we multiply the linear function by 10 and take sigmoid of 10X1+10X2, our predictions are much better since they're closer to zero and one. But the function becomes much steeper and it's much harder to do great descent here. Since the derivatives are mostly close to zero and then very large when we get to the middle of the curve. Therefore, in order to do gradient descent properly, we want a model like the one in the left more than a model like the one in the right. In a conceptual way, the model in the right is too certain and it gives little room for applying gradient descent. Also as we can imagine, the points that are classified incorrectly in the model in the right, will generate large errors and it will be hard to tune the model to correct them. These can be summarized in the quote by the famous philosopher and mathematician BertrAIND Russell. The whole problem with artificial intelligence, is that bad models are so certain of themselves, and good models are so full of doubts. Now the question is, how do we prevent this type of overfitting from happening? This seems to not be easy since the bad model gives smaller errors. Well, all we have to do is we have to tweak the error function a bit. Basically we want to punish high coefficients. So what we do is we take the old error function and add a term which is big when the weights are big. There are two ways to do this. One way is to add the sums of absolute values of the weights times a constant lambda. The other one is to add the sum of the squares of the weights times that same constant. As you can see, these two are large if the weights are large. The lambda parameter will tell us how much we want to penalize the coefficients. If lambda is large, we penalized them a lot. And if lambda is small then we don't penalize them much. And finally, if we decide to go for the absolute values, we're doing L1 regularization, and if we decide to go for the squares, then we're doing L2 regularization. Both are very popular, and depending on our goals or application, we'll be applying one or the other. Here are some general guidelines for deciding between L1 and L2 regularization. When we apply L1, we tend to end up with sparse vectors. That means, small weights will tend to go to zero. So if we want to reduce the number of weights and end up with a small set, we can use L1. This is also good for feature selections and sometimes we have a problem with hundreds of features, and L1 regularization will help us select which ones are important, and it will turn the rest into zeroes. L2 on the other hand, tends not to favor sparse vectors since it tries to maintain all the weights homogeneously small. This one normally gives better results for training models so it's the one we'll use the most. Now let's think a bit. Why would L1 regularization produce vectors with sparse weights, and L2 regularization will produce vectors with small homogeneous weights? Well, here's an idea of why. If we take the vector (1, 0), the sums of the absolute values of the weights are one, and the sums of the squares of the weights are also one. But if we take the vector (0.5, 0.5), the sums of the absolute values of the weights is still one, but the sums of the squares is 0.25+0.25, which is 0.5. Thus, L2 regularization will prefer the vector point (0.5, 0.5) over the vector (1, 0), since this one produces a smaller sum of squares. And in turn, a smaller function.

### 08. Dropout-Ty6K6YiGdBs.en

Here's another way to prevent overfitting. So, let's say this is you, and one day you decide to practice sports. So, on Monday you play tennis, on Tuesday you lift weights, on Wednesday you play American football, on Thursday you play baseball, on Friday you play basketball, and on Saturday you play ping pong. Now, after a week you've kind of noticed that you've done most of them with your dominant hand. So, you're developing a large muscle on that arm but not on the other arm. This is disappointing. So, what can you do? Well, let's spice it up on the next week. What we'll do is on Monday we'll tie our right hand behind our back and try to play tennis with the left hand. On Tuesday, we'll tie our left hand behind your back and try to lift weights with the right hand. Then on Wednesday again, we'll tie our right hand and play American football with the left one. On Thursday we'll take it easy and play baseball with both hands, that's fine. Then, on Friday we'll tie both hands behind our back and try to play basketball. That won't work out too well. But it's OK. It's the training process. And then on Saturday again, we tie our left hand behind our back and play ping pong with the right. After a week, we see that we've developed both of our biceps. Pretty good job. This is something that happens a lot when we train neural networks. Sometimes one part of the network has very large weights and it ends up dominating all the training, while another part of the network doesn't really play much of a role so it doesn't get trained. So, what we'll do to solve this is sometimes during training, we'll turn this part off and let the rest of the network train. More thoroughly, what we do is as we go through the epochs, we randomly turn off some of the nodes and say, you shall not pass through here. In that case, the other nodes have to pick up the slack and take more part in the training. So, for example, in the first epoch we're not allowed to use this node. So, we do our feat forward and our back propagation passes without using it. In the second epoch, we can't use these two nodes. Again, we do our feet forward and back prop. And in the third epoch we can't use these nodes over here. So, again, we do forward and back prop. And finally in last epoch, we can't use these two nodes over here. So, we continue like that. What we'll do to drop the nodes is we'll give the algorithm a parameter. This parameter is the probability that each node gets dropped at a particular epoch. For example, if we give it a 0.2 it means each epoch, each node gets turned off with a probability of 20 percent. Notice that some nodes may get turned off more than others and some others may never get turned off. And this is OK since we're doing it over and over and over. On average each node will get the same treatment. This method is called dropout and it's really really common and useful to train neural networks.

### 09. Local Minima-gF_sW_nY-xw.en

So let's recall a gradient descent does. What it does is it looks at the direction where you descend the most and then it takes a step in that direction. But in Mt. Everest, everything was nice and pretty since that was going to help us go down the mountain. But now, what if we try to do it here in this complicated mountain range. The Himalayans lowest point that we want to go is around here, but if we do gradient descent, we get all the way here. And once we're here, we look around ourselves and there's no direction where we can descend more since we're at a local minimum. We're stuck. In here, gradient descent itself doesn't help us. We need something else.

### 10. Random Restart-idyBBCzXiqg.en

One way to solve this is to use random restarts, and this is just very simple. We start from a few different random places and do gradient descend from all of them. This increases the probability that we'll get to the global minimum, or at least a pretty good local minimum.

### 11. Vanishing Gradient-W_JJm_5syFw.en

Here's another problem that can occur. Let's take a look at the sigmoid function. The curve gets pretty flat on the sides. So, if we calculate the derivative at a point way at the right or way at the left, this derivative is almost zero. This is not good cause a derivative is what tells us in what direction to move. This gets even worse in most linear perceptrons. Check this out. We call that the derivative of the error function with respect to a weight was the product of all the derivatives calculated at the nodes in the corresponding path to the output. All these derivatives are derivatives as a sigmoid function, so they're small and the product of a bunch of small numbers is tiny. This makes the training difficult because basically grading the [inaudible] gives us very, very tiny changes to make on the weights, which means, we make very tiny steps and we'll never be able to descend Mount Everest. So how do we fix it? Well, there are some ways.

### 12. Other Activation Functions-kA-1vUt6cvQ.en

The best way to fix this is to change the activation function. Here's another one, the Hyperbolic Tangent, is given by this formula underneath, e to the x minus e to the minus x divided by e to the x plus e to the minus x. This one is similar to sigmoid, but since our range is between minus one and one, the derivatives are larger. This small difference actually led to great advances in neural networks, believe it or not. Another very popular activation function is the Rectified Linear Unit or ReLU. This is a very simple function. It only says, if you're positive, I'll return the same value, and if your negative, I'll return zero. Another way of seeing it is as the maximum between x and zero. This function is used a lot instead of the sigmoid and it can improve the training significantly without sacrificing much accuracy, since the derivative is one if the number is positive. It's fascinating that this function which barely breaks linearity can lead to such complex non-linear solutions. So now, with better activation functions, when we multiply derivatives to obtain the derivative to any sort of weight, the products will be made of slightly larger numbers which will make the derivative less small, and will allow us to do gradient descent. We'll represent the ReLU unit by the drawing of it's function. Here's an example of a Multi-layer Perceptron with a bunch of ReLU activation units. Note that the last unit is a sigmoid, since our final output still needs to be a probability between zero and one. However, if we let the final unit be a ReLU, we can actually end up with regression models, the predictive value. This will be of use in the recurring neural network section of the Nanodegree.

### 13. Batch vs Stochastic Gradient Descent-2p58rVgqsgo.en

First, let's look at what the gradient descent algorithm is doing. So, recall that we're up here in the top of Mount Everest and we need to go down. In order to go down, we take a bunch of steps following the negative of the gradient of the height, which is the error function. Each step is called an epoch. So, when we refer to the number of steps, we refer to the number of epochs. Now, let's see what happens in each epoch. In each epoch, we take our input, namely all of our data and run it through the entire neural network. Then we find our predictions, we calculate the error, namely, how far they are from where their actual labels. And finally, we back-propagate this error in order to update the weights in the neural network. This will give us a better boundary for predicting our data. Now this is done for all the data. If we have many, many data points, which is normally the case, then these are huge matrix computations, I'd use tons and tons of memory and all that just for a single step. If we had to do many steps, you can imagine how this would take a long time and lots of computing power. Is there anything we can do to expedite this? Well, here's a question: do we need to plug in all our data every time we take a step? If the data is well distributed, it's almost like a small subset of it would give us a pretty good idea of what the gradient would be. Maybe it's not the best estimate for the gradient but it's quick, and since we're iterating, it may be a good idea. This is where stochastic gradient descent comes into play. The idea behind stochastic gradient descent is simply that we take small subsets of data, run them through the neural network, calculate the gradient of the error function based on those points and then move one step in that direction. Now, we still want to use all our data, so, what we do is the following; we split the data into several batches. In this example, we have 24 points. We'll split them into four batches of six points. Now we take the points in the first batch and run them through the neural network, calculate the error and its gradient and back-propagate to update the weights. This will give us new weights, which will define a better boundary region as you can see on the left. Now, we take the points in the second batch and we do the same thing. This will again give us better weights and a better boundary region. Now, we do the same thing for the third batch. And finally, we do it for the fourth batch and we're done. Notice that with the data, we took four steps whereas, when we did normal gradient descent, we took only one step with all the data. Of course, the four steps we took were less accurate but in the practice, it's much better to take a bunch of slightly inaccurate steps than to take one good one. Later in this nanodegree, you'll have the chance to apply stochastic gradient descents and really see the benefits of it.

### 14. Learning Rate-TwJ8aSZoh2U.en

The question of what learning rate to use is pretty much a research question itself but here's a general rule. If your learning rate is too big then you're taking huge steps which could be fast at the beginning but you may miss the minimum and keep going which will make your model pretty chaotic. If you have a small learning rate you will make steady steps and have a better chance of arriving to your local minimum. This may make your model very slow, but in general, a good rule of thumb is if your model's not working, decrease the learning rate. The best learning rates are those which decrease as the model is getting closer to a solution. We'll see that Keras has some options to let us do this.

### 15. Momentum-r-rYz_PEWC8.en

So, here's another way to solve a local minimum problem. The idea is to walk a bit fast with momentum and determination in a way that if you get stuck in a local minimum, you can, sort of, power through and get over the hump to look for a lower minimum. So let's look at what normal gradient descent does. It gets us all the way here. No problem. Now, we want to go over the hump but by now the gradient is zero or too small, so it won't give us a good step. What if we look at the previous ones? What about say the average of the last few steps. If we take the average, this will takes us in direction and push us a bit towards the hump. Now the average seems a bit drastic since the step we made 10 steps ago is much less relevant than the step we last made. So, we can say, for example, the average of the last three or four steps. Even better, we can weight each step so that the previous step matters a lot and the steps before that matter less and less. Here is where we introduce momentum. Momentum is a constant beta between 0 and 1 that attaches to the steps as follows: the previous step gets multiplied by 1, the one before, by beta, the one before, by beta squared, the one before, by beta cubed, etc. In this way, the steps that happened a long time ago will matter less than the ones that happened recently. We can see that that gets us over the hump. But now, once we get to the global minimum, it'll still be pushing us away a bit but not as much. This may seem vague, but the algorithms that use momentum seem to work really well in practice.

### 16. Error Functions Around the World-34AAcTECu2A.en

So, in this nano degree, we covered a few error functions, but there are a bunch of other error functions around the world that made the shortlist, but we didn't have time to study them. So, here they are. These are the ones you met: there is Mount Everest and Mount Kilimanjerror. The ones you didn't meet: there is Mt. Reinerror, he's a big Seahawks fan. Straight from Italy, we got Mount Ves-oops-vius, and my favorite, from Iceland, we got the Eyjafvillajokull. This is the famous volcano that stopped all the European flights back in 2012. See what I did there is I took the original name which is Eyjafjallajokull and then I changed the word Jalla to Villa, which is Icelandic for error.



## Part 02-Module 02-Lesson 05_Embeddings  Word2Vec

### 01. M4L51 HSA Word Embeddings V3 RENDER V1-ZsLhh1mly9k.en

In this lesson, I want to talk a bit more about using neural networks for natural language processing. We'll be discussing word embedding, which is the collective term for models that learned to map a set of words or phrases in a vocabulary to vectors of numerical values. These vectors are called embeddings, and we can use neural networks to learn to do word embedding. In general this technique is used to reduce the dimensionality of text data. But these embedding models can also learn some interesting traits about words in a vocabulary. In fact, we'll focus on the Word2Vec embedding model. Which learns to map words to embeddings that contain semantic meaning. For example embeddings can learn the relationship between verbs in the present and past tense. The relationship between the embeddings for walking and walked, should be the same as the relationship between the embeddings for swimming and swam. Similarly embeddings can learn the relationships between words and common genders. Such as between woman and Queen and between man and King. You can think of these embeddings as vectors that have learned to mathematically represent the relationship between words in a vocabulary. A word of caution here. The embeddings are learned from a body of text and so any word associations in that source text will be replicated in the embeddings. If your text contains false information or gender biased associations. These traits will be replicated in your embeddings. In fact debiasing word embeddings is an active area of research and you can read more about it below. In this lesson we'll first talk about how word embedding works in theory, then a walk through a series of notebooks in which you'll learn to implement the Word2Vec model. Before we start coding, let's learn more about how embeddings can reduce the dimensionality of text data.

### 02. M4L52 HSA Embedding Weight Matrix V3 RENDER V2-KVCcG5v8fi0.en

We've talked a bit about how neural networks are designed to learn from numerical data. In our case, word embedding is really all about improving the ability of networks to learn from texted data. The idea is this, embeddings can greatly improve the ability of networks to learn from text data, by representing that data as lower-dimensional vectors. Let's think about this in an example. Usually, when you're dealing with text and you split things up into words, you tend to have tens of thousands of different words in a large data set. When you're using these words as input to a network like an R&amp;N, we've seen that we can one-hot encode them. What that means is that you have these giant vectors that are like 50,000 units long, and only one of them is set to one, and all the others are set to zero. Then, you pass this long vector as input to some hidden layer in the network. The output of this hidden layer is calculated by multiplying that input vector by some matrix of learned weights. The result is a huge matrix of values. Most of which are zero, because of the initial one-hot vector. So, all these computing resources are used on values that do not hold any information, and this is really computationally inefficient. To solve this problem, we can use embeddings, which basically provide a shortcut for doing this matrix multiplication. To learn word embeddings, we use a fully-connected linear layer like you've seen before. We'll call this layer the embedding layer, and its weights are the embedding weights. These weights will be values that are learned during training this embedding model, and they make up a useful weight matrix. With this matrix, we can skip the big multiplication step from before, by instead grabbing the values for the output of our hidden layer directly from a row in our weight matrix. We can do this because the multiplication of a one-hot encoded vector with a weight matrix, returns only the row of the matrix that corresponds to the index of the one or the on input unit. So, instead of doing matrix multiplication, we can use the embedding weight matrix as a lookup table. Instead of representing words as one-hot vectors, we can encode each word as a unique integer. As an example, say we have the word heart encoded as the integer 958. Then, to get the hidden layer values for heart, we just take the 958th row of the embedding weight matrix. This process is called an embedding lookup, and the number of hidden units is the embedding dimension. So, the embedding lookup table is just a weight matrix, and the embedding layer is just a hidden layer. It's important to know that the lookup table holds weights that are learned during training just like any weight matrix. So, this is the basic idea behind how embedding works. In the next few sections, you'll see how word "the vec" uses the embedding layer to find vector representations of words that contain semantic meaning.

### 03. 3 Word2Vec Notebook V2-4cWzv3YiF_w.en

So in this notebook, I'll be leading you through a Word2Vec implementation in PyTorch. Now, you've just learned about the idea behind embeddings in general. For any dataset with lots of classes or input dimensions like a large word vocabulary, we're basically skipping the one-hot encoding step, which would result in extremely long input vectors of mostly zeros. We're taking advantage of the fact that when one-hot vectors are multiplied by a weight matrix, we will just get one row of values back. For example, if we have a one-hot vector that has its fourth index on, and we multiply this by a weight matrix, we'll get the fourth row of weights back as a result. So, what we can do, is actually just have input numbers instead of one-hot vectors, and then we can use an embedding weight matrix to look up the correct output. In this case, we see the word heart is encoded as the integer 958, and we can look up the embedding vector for this word in the 958 row of an embedding weight matrix, this is also often called a lookup table. In text analysis, this is great. Because we already know that we can convert a vocabulary of words into integer tokens. So, each unique word will have a corresponding integer value. If we have a vocabulary of 10,000 words, we'll have a 10,000 row embedded weight matrix, where we can look up the correct output values. These output values which will just be rows in this weight matrix will be a vector representation of that input word. These representations are called embeddings, and they have as many values as is weight matrix has columns. This width is called the embedding dimension, it's usually some value in the hundreds. Now, Word2Vec is a special algorithm that basically says, "Any words that appear in the same context in a given text should have similar vector representations." So context in this case, basically means the words that come before and after a word of interest. Here are a couple of examples. In a body of text, you'll find a variety of sentences, and these ones all involve drinking some beverage. In some of these cases, even if I removed a word of interests, you may be able to guess what goes in there, just based on the context words surrounding it. So here it says, "I often drink coffee in the mornings. When I'm thirsty, I drink water, and I drink tea before I go to sleep." The mention of I drink before these words, makes these contexts similar. So, we'll expect these words coffee, water, and tea to have similar word embeddings. You can imagine that if we look at a large enough text, we may also see that coffee is more closely associated with morning time and so on. So, just by looking at a word of interest and some context words that surround it, Word2Vec can find similarities between words and relationships between them. In fact, for such similar words, Word2Vec should produce vectors that are very close in vector space, different words will be some distance away from each other. In this way, we're actually able to do vector arithmetic, and that's how Word2Vec confined mappings between words in the past and present tense for example. So, mapping the verb drink to drinking, and swam to swimming, is going to be the same transformation in vector space. In practice, Word2Vec is implemented in one of two ways. The first option is to basically give our model the context, so several words surrounding a word of interest, and have it tried to predict the missing word. So, context words in and a single word out, this is called the continuous bag of words or a CBOW model. The second option is the reverse, to input our word of interests and have our model tried to predict the context. So, one word n and a few you context words out. This is the skip-gram model, and we'll be implementing Word2Vec in this way because it's been shown to work a bit better. You'll notice that for either of these models, we'll also have to formalize the idea of context to be a window of a specified size. So, something like two words before and two words after a word of interest. Here, for an input word w at time t, we have context words from t minus two to t plus two, that is minus two words in the past and plus two in the future. Notice that the context does not include the original word of interest. So, now that you've been introduced to this notebook and the Word2Vec skip-gram model. Next, I'll show you the data that we'll be working with and give you your first exercise.

### 06. 4 Data Subsampling V1-7SJXv2BQzZA.en

Okay, let's get started with implementing the skip-gram word2vec model. The first thing you want to do is load in the necessary data. In this example, I'm using a large body of text that was scraped from Wikipedia articles by Matt Mahoney. If you're working locally, you'll actually need to click this link to download this data as a zip file, and we'll move it into our data directory, and unzip it. You should then be left with a file just called text8 in our data directory. So, I've already put that data in the data directory, and here, I'm loading that file in my name and printing out the first 100 characters. It looks like the first section of text is about anarchism and the working class. So I loaded that in correctly, and then I want to do some preprocessing. Essentially, I want to break this text up into a giant list of words so that I can build up a vocabulary. So here, I'm going to do that using a function in the provided utils.py file called preprocess. Let's actually take a look at this code. So here's our utils.py file and our preprocess function. This function takes in some text and you can see that it does a few things. First, in all of these lines, it converts any punctuation into tokens. So a period is changed to a bracketed period token and so on. Next, we see that it stores the number of times certain words appear in the text using a Counter. A Counter is a collection that will basically return a dictionary of words and their frequency of occurrence. Here, we're creating a list of trimmed words that basically cuts all words that show up five or fewer times in this dataset. This should greatly reduce issues due to noise in the data, and it should improve the quality of the vector representations. Then, finally, it returns those trimmed words. So back to our notebook, I'm going to say words equal utils.preprocess text, and I'll print out the first trim 30 words. This may take a few moments to run since our text data is quite big. Then you should see an output like this. Pretty much the same text that we saw above, only the words are split into a list. Here, I'm going to print out some statistics about this data. I'm printing out the length of the text so a word count of our data, and I'll print out the number of unique words. To get the number of unique words, I'm using the built-in Python data type set, which if you recall from the last lesson, will get rid of any duplicate words. So, we have a set of only unique words in this text. So, you can see that we have over 16 million words in this text, and over 60 thousand unique words, and these numbers will be useful to keep in mind as we continue processing. Next, I'm creating two dictionaries to convert words to integers and back again, integers to words. This is our usual tokenization step. This is again done with a function in the utils.py file, create lookup tables. So, let's take a look at what this function is doing. So, this function takes in a list of words in a text and it returns two dictionaries that map from our vocabulary to integer values and back. You may notice an interesting use of counter here. First, this is creating a sorted vocabulary. So this is a list of words from most to least frequent according to the word counts returned by counter. Then integers are assigned in descending frequency order. So the most frequent word like B is given the integer 0, and the next most frequent is 1 and so on. So in our notebook, this function returns are two dictionaries. Once we have those, the words are then converted to integers and stored in the list into words. I'll print out the first 30 tokenized words here just to check that they make sense. So, if we look at these values and back to our list of words above, we'll be able to see that 'the' and 'of' are some of the most common words in our dictionary. We can see that 'the' is tokenized as the integer 0, and it looks like 'of' is the next most frequent word tokenized as 1. We have over 60,000 words in our vocabulary, so all of these token value should be integer values in that range. Now, our goal is to implement word2vec, which relies on looking at the context around a word of interest. We want to define our context very carefully, basically looking at a window of the most relevant words around a word of interests. There are some words that are almost never going to be relevant because they're so common, words that show up anywhere and really often such as the, of, and for. These don't provide much context to other nearby words. So if we discard some of these common words, we can remove some noise from our data, and in return, get faster training and better vector representations. This process is called subsampling. This will be your first task. Subsampling works like this. For each word wi in our training set, you want to discard it with a probability given by this equation. The probability of discarding a word w is equal to 1 minus the square root of t over that words frequency, and t is the threshold value that we set. So say, we're thinking of discarding the word 'the', word index 0. Let's say, it occurs one million times in our 16-million long dataset. These are approximations but this is one million over 16 million here the frequency of occurrence. The numerator is a threshold I've set, which is 1 times 10 to the negative fifth. So, if I just run these values through our equation, I'm going to get a probability of getting rid of this word 98.7 percent of the time. Even after discarding the majority of these inner text will still leave over 12,000 of the original one million these inner text. The idea with subsampling is really to just get rid of a lot of these frequently occurring words so that they're not always affecting the context of other words while simultaneously keeping enough examples to learn a word embedding for that word. So, the subsampling equation says the probability that we discard a word is going to be higher if that word's frequency is higher. Here I provided some code, a threshold to start you out, and a dictionary of word counts. This is using the counter collection which takes in our list of encoded words, and returns how many times they appear in that list, and I can print out the first key value pair in this list. So here, I can see that the word token 5233 appears 303 times in our text. I want you to use this information to calculate the discard probability for each word in our vocabulary, then use that to create a new set of data train words, which will basically be our original list of int words only with some of our most frequent words discarded. This is more of a programming challenge rather than a deep learning task but preparing data is an important skill to have, so try to solve this task, and next, I'll show you my solution.

### 07. 5 Subsampling Solution V1-YXruURuFD7g.en

Here is my solution for creating a new list of train words. First, I calculated the frequency of occurrence for each word in our vocabulary. So, I stored the total length of our text in a variable, total_count. Then, I created a dictionary of frequencies. For each word token and count in the word counter dictionary that was given, I added an item to this dictionary, where the word was the key and the value was the count of that word over the total number of words in our text, the frequency. Then, I calculated the discard probability as p_drop. This is another dictionary that maps words to the drop probability. Here, I'm just using the subsampling equation to get that, which is 1 minus the square root of our threshold over that word's frequency. Finally, I created a new list of train words. For each word in our list of int_words, I said I'll keep this word with some probability. So, I generated a random value between zero and one, and I checked if that value was less than 1 minus the drop probability for that word. This is saying, okay, I want to keep this word with a probability of 1 minus p_drop. So, if I have a drop probability of 0.98, then the keyboard probability is 1 minus this p_drop, which will be 0.02. If I generate a value less than 0.02, which is unlikely, only then will I keep this word in my list of train words. There are other ways to solve this problem, but I like to frame this as a which words do I keep task. Okay. Then I'm printing out the first 30 words of this train data. This should look similar to the first 30 tokens in our int_words list. Only you'll notice that most of the zeros and ones are gone. These were our most common words from before, and so this is looking as I expect, and I can move on to the next step, which will be defining a context window and batching our data.

### 08. 6 Defining Context Targets V1-DJN9MzD7ctY.en

Now that our data is in good shape, we need to get it into the proper form to pass it into our network. With the skip-gram architecture for each word in the text, we want to define a surrounding context and grab all the words in a window around that word with size C. When I talk about a window, I mean a window in time. So, like two words in the past and two words in the future from our given input word. More generally than two words in the past and future, I'm going to say we want to define a window of size C. Here, I have some text from the Mikolov paper on Word2Vec, "Since the more distant words are usually less related to the current word than those close to it, we give less weight to the distant words by sampling less from those words in our training examples. If we choose C equals five, for each training word, we'll randomly select a number R in range one to C, and then use R words from history and R words from the future of the current word as correct labels." So, this is saying that we don't want to choose too big of a window because too big of a window will give us irrelevant context. In other words, good context words are usually the ones closest to the current word rather than farther away, and we want to include some randomness in how we define our context. If we define a context window of size C equals five, then we'll create a range R that's going to be a random integer between one or five. So, say we get an R equal to two as an example, then we'll define the context around a given word to be the two words that appear right before and after our word of interest. I have an example here. Say we're interested in the word at the second index in this list, 741. If we randomly generate an R equal to two, we'll be interested in the two tokens before and after this word. I want you to write a function that will return context words in a list like this. This will be the function get_target, which takes in a list of word IDs, and index of interests, and a context window size. So, the effect of getting words within a random rage R instead of a consistent larger range C is that you're more likely to get words that are right next to your current word, and less likely to get words that are further away from your current word. So, what you're really doing is going to be training on context words that are closer to your word of interests and likely more relevant, more often. So here, I've left this function for you to fill out. Now, there are some special cases. If the index that's passed in is zero or your range cannot go back in the past as far as you want, then you can start your context at the start of the past and list of words. You can test out your implementation in this cell below. Next, we'll use this function to actually batch the data, and so it's important that this is implemented correctly. As usual, if you're stuck or want to see my solution, checkout my solution video next.

### 09. 7 Batching Data Solution V1-nu2rjLzt1HI.en

Here's how I'm defining the context targets around a given word index. First, according to the excerpt from the paper, I'm going to define a range R. R is going to be a random integer in the range one to c, the window size. randint takes in a range that is not inclusive of the last number. So, that's why I have a plus one here. Then I define the start and stop indices of my context window. The start will be a range of words in the past. That is the index of my word of interest minus my range R. This will only happen as long as that doesn't get us to a negative index. If this operation does give us a negative value, then I just set my start index to the startup my list of words index zero. Then my stop index is where my feature words end. So, my word of interests index, plus our range R. Finally, I do not want my return target context to include the word at the past in index. So, I'm defining my target words as the words behind my index of interest from start to idx, plus the words in front, idx plus one to stop plus one. Then I'm returning these words as a list. Then when I go to test this out on a test set of word tokens, and I can run this a couple of times, I see that I get a variable length of words around my past an index of five. I can see the target does not include my index of interest. These line up just because I've created some input data, that's the integer zero through nine. If you run the cell multiple times, you will see a different target based on a different randomly generated R. So, this looks good. Right below this function, I've defined a generator function. This function we'll use our get_target function that we've just defined. get_batches takes in a list of word tokens a batch_size and a window_size. It makes sure that we can make complete batches of data. In this four loop, I'm iterating over our words one batch length at a time. I get a batch of words then for each word in a batch I'm calling get_target. This should return a batch of target words in a window around the given batch word. I'm calling extend here so that each batch x and y will be one row of values. Here, I'm making x the same length as y. Finally, it returns this list of input words x and target context words why using yield, which makes this a generator function. Then in the blue cell, we can test this batching out to see what it looks like when applied to some fake data here. So, I'm getting an x and y batch of data by calling next on our generator function. Here, I've passed in some int_text, a batch_size of four, and a window_size of five. When I run this cell, this output might look a little weird because everything's been extended into one row. But I can see that I've made my desired four batches because I have four different input x values; zero, one, two, and three. If we take a look at the first input zero, we see is length three. So, the target must have also been length three. The corresponding context is one, two, three. All the targets in the future window that surround the input index zero, which is what I expect. For the other input, output batches, I can see that I'm generating targets that surround the input values one, two, and three. So, we have our batch inputs, and our target context. Now, we can get to defining and training a word to vec model on this batch data, which I'll go over next.

### 10. 8 Word2vec Model V2-7BEYWhym8lI.en

Now that we've taken the time to preprocess and batch our data, it's time to actually start building the network. Here, we can see the general structure of the network that we're going to build. So, we have our inputs, which are going to be like batches of our train word tokens, and as we saw when we loaded in a batch, a lot of these values will actually be repeated in this input vector. So, we're going to be parsing in a long list of integers, which are going into this hidden layer, our embedding layer. The embedding layer, is responsible for looking at these input integers and basically creating a lookup table. So, for each possible integer value, there will be a row in our embedding weight matrix, and the width of the matrix will be the embedding dimension that we define. That dimension will be the size of the embedding layers outputs. Then these embeddings are fed into a final fully connected softmax output layer. Remember, that in the skip gram model, we're parsing in some input words and we're training this whole model to generate target context words. So, for one input value, the targets will be randomly selected context words from a window around the input word. Our output layer, is going to output the probability that a randomly selected context word is going to be the word the, or of, or nine, or any other word in our vocabulary. We're going to be trying to predict our target context words using the outputs of the softmax layer. Basically, looking at the words with the highest probability that they are context words. Then when we train everything, what's going to happen, is that our hidden layer is going to form these vector representations of the input words. So, each row in the embedding look-up table will be a vector representation for a word. Row zero, will be the embedding for the word the, for example. These vectors contain some semantic meaning, and that's what we're really interested in. We only really care about these embeddings. From these embeddings, we can do some interesting things. Performing vector math to see which of our words are most similar, or we can use these embeddings as input to another model that works with the same text input data. So, when we're done training, we can actually just get rid of this last softmax layer, because it's just there to help us train this model and create correct embeddings in the first place. Okay. So, right before we define the model, I have a function that will help us see what kind of word relationships this model is learning. When I introduced the idea of word2vec, I mentioned that representing words as vectors, gives us the ability to mathematically operate on these words in vector space. To see which words are similar, I'm going to calculate how similar vectors are using cosine similarity. Cosine similarity, looks at two vectors a and b and the angle between them, theta. It says, "Okay. The similarity between these two vectors is just the cosine of the angle between them." If you're familiar with vector math, that can also be calculated as the normalized dot product of a and b. You can really just think of it like this. When theta is zero, cosine of theta is equal to one. This is the maximum value that cosine can take. When theta is 90 degrees or rather these vectors are orthogonal to one another, then the cosine is going to be zero. So, the similarity really ends up being a value between zero and one, that indicates how similar two vectors are in vector space. So, let's look at this cosine similarity function. This function takes in an embedding layer, a validation size, and a validation window. In here, I'm getting the embeddings from the pasting layer. These are just the layer weights. Then, I'm doing some math and storing the magnitudes of these embedding vectors. That magnitude is just going to be the square root of the sum of the embedded vectors squared. Then, I'm randomly selecting some common and uncommon validation word examples. These are just integers in a range, in this case, from zero to 1,000 for common words, and for a higher range for uncommon words. Recall that lower indices indicate that a word appears more frequently. So, I'm generating half of our validation examples from a more common range, and half from a more uncommon range. These are collected in an np array and then converted into a long tensor type. Then I'm passing these validation examples into the embedding layer. In return, I get their vector representations back. So, these validation words are encoded as our vectors, a, and we're going to calculate the similarity between a, and each word vector, b, in the embedding table. We mentioned that the similarity is a dot product of a and b over the magnitude. This dot product is just a matrix multiplication between the validation vectors a and the transpose of the embedded vectors b. Here, I'm dividing by the magnitude, and this is not the exact equation here, but it will give us valid values for similarities, just scaled by a constant. This function returns the validation examples and similarities. This gives us all we need to later print out the validation words and the words in our embedding table that are semantically similar to those words. It's going to be a nice way to check that our embedding table is grouping together words with similar semantic meanings. So, this is a given function, you don't have to change anything about this. Now, on to defining the model. So, we know our model accepts some inputs, then it has an embedding layer, and a final softmax output layer. You'll have to define this using PyTorch's embedding layer, which you can read about here. Here's the documentation. So, the embedding layer is known as a sparse layer type. It takes in a number of input embeddings, which is going to be the number of rows in your embedding weight lookup matrix and an embedding dimension. This is the size of each embedding vector. The number of columns in your embedding look-up table. These two are the most important inputs when defining this layer. So, after the embedding layer, you'll define a linear layer to go from our embedding size to our predicted context words. You'll also have to apply a softmax function to the output, so that this model returns word probabilities. So, here's the skeleton code for this model, and when we instantiate this model, we're going to be parsing in input values for n_vocab, the size of our vocabulary, and n_embed, our embedding dimension. So, you should be able to complete the init and forward functions for this model. When you do that, you should be able to proceed with training using the provided training loops below. I'd really recommend training on GPU. Training this particular model takes quite a while even on GPU. So, I'd start training with maybe just one or two epics for now. All right. So, I'll leave this as an exercise, and next I'll go over one solution for defining a skip-gram model and training it.

### 11. 9 Model Validation Loss V2-GKDCq8J76tM.en

This is what we want our model to look like. It should take in some inputs and then put those through an embedding layer, which produces some embedded vectors that are sent to a final softmax output layer, and here's my model definition. You can see that it's a pretty simple model. First, I'm defining my embedding layer, the self.embed. This takes n the length of my word vocabulary. This means that it will create an embedding weight matrix that has a row for each of the words in our vocabulary, and this will output vectors of size n_embed, our embedding dimension. Then, I have a fully-connected layer that takes in that embedding dimension as input, and its output size is also the length of our vocabulary. That's because this output is a series of word class scores that tells us the likely context word for a given input word, and then I've defined a softmax activation layer here. You could have just done this in the forward function too. This is just one solution. Then at my forward function, I'm passing in my input X into the embedding layer. This returns are embeddings which moves to our fully-connected layer, which returns a series of class scores. Finally, a softmax activation function is applied and I'll be left with my log probabilities for context words. Below in this training section, I'm actually going to instantiate this model. So here, I've defined an embedding dimension and I've set this to 300, but you're welcome to experiment with larger or smaller values. The embedding dimension can be thought of as the number of word features that we can detect, like the length, the type of word, and so on. So, this takes in the entire length of our vocabulary and the embedding dimension, and I've moved this to a GPU for training. Here, you'll see that I'm using negative log-likelihood loss, and this is because a softmax in combination with negative log-likelihood basically equals cross entropy loss. So, this is a great loss for looking at probabilities of context words, and I'm using an Adam optimizer, which is just my go-to and I'm passing in my model parameters and a learning rate. Then I have my training loop and I've decided to train for five epochs. In this training, actually took a few hours even on GPU, so I'd recommend that you train for a shorter amount of time or wait until I show you how to train more efficiently. So, in my training loop, I'm getting batches of data by calling the generator function that we defined above and passing in my list of chain words and a batch size. I'm getting my inputs and my target context words, and I'm converting them into LongTensor types, and moving these two GPU if it's available, and then I'm performing backpropagation as usual, and passing my inputs into my skip-gram model to get the log probabilities for the context words. Then I'm applying my loss function to these contexts words and my targets, then performing backpropagation and updating the weights of my model, not forgetting to zero out any accumulated gradients before these two steps. Then I'm printing out some validation examples using my cosine similarity function. Here, I'm passing in my model and a GPU device, and I'm getting back some validation examples and their similarities. Here, I'm actually using topk sampling to get the top six most similar words to a given example. Here, I'm iterating through my validation examples. I'm printing out the first validation word and then the five closest words next to it after a line character, and here are some initial results. I printed a lot of data after training for five epochs. At first, these word associations look pretty random. We have and, returns, liverpudlians, and so on. But as I train, I should see that these validation words are getting more and more similar. If I scroll down all the way to the end of my training, I can see that similar words are nicely grouped together. You can see a bunch of number words are grouped together. Here, I have a bunch of animals and mammals grouped in one line, some lines that are related to states and politics, and even lines that are related to a place and a language. So, it looks like my word2vec model is learning, and I can visualize these embeddings in another way too. Another really powerful method for visualization is called t-SNE, which stands for t-distributed stochastic neighbor embeddings. It's a non-linear dimensionality reduction technique that aims to separate data in a way that cluster similar data close together and separates different data. In this case, it's an algorithm that I'm loading in from the sklearn library. I give it the number of embeddings that I want to visualize, and I get these embeddings from the weights of our embedded layer which I'm calling by name from our model. So, remember that our embedding layer was just named embed, and I can get the weights by same model.embed.weight. So here, I'm applying t-SNE to 600 of our embeddings, and this is what this t-SNE clustering ends up looking like. We can actually see that similar words are grouped together. Here we have east, west, north, and south. If we look to the right, we can see some musical terms: rock, music, album, band, and song. Lower down, we can see some religious terms, some colors over here, some academic terms: school, university, and college. On the left side here, I can see clusters of the months in the year and it looks like a few integer values here. So, this clustering indicates that my word2vec model has worked. It learns to generate embeddings that hold semantic meaning, and this also gives us a cool way to visualize the relationships between words in space. So, one problem with this model was that it took quite a while to train, and next I'm going to address that challenge.

### 12. 10 NegativeSampling V1-gnCwdegYNsQ.en

Now, the last model took quite a while to train, and there are some ways that we can speed up this process. In this video, I'll talk about one such method which is called negative sampling. So, this is a new notebook, but it contains basically the same info as our previous notebook including this architecture diagram. This is our current architecture where we have a softmax layer on the output, and since we're working with tens of thousands of words, the softmax layer is going to have tens of thousands of units. But with any one input, we're really going to have one true context target. What that means is, when we train, we're going to be making very small changes to the weights between these two layers even though we only have one true output that we care about. So, very few of the weights are actually going to be updated in a meaningful way. Instead what we can do is approximate the loss from the softmax layer, and we do this by only updating a small subset of all the weights at once. We'll update the weights for what we know to be the correct target output, but then we'll only update a small number of incorrect or noise targets usually around 100 or so as opposed to 60,000. This process is called negative sampling. To implement this, there are two main modifications we need to make to our model. First, since we're not taking the softmax output over all the words, we're really only concerned with one output one at a time. Similar to how we used an embedding layer to map an input word to a row of embedding weights, we can now use another embedding layer to map the output words to a row of hidden weights. So, we'll have two embedding layers, one for input words and one for output words. Second, we have to use a modified loss function that only cares about the true target and a small subset of noisy and correct target context words, and that's this big loss function here. It's a little heavy on notation, so I'll go over it one part at a time. Let's take a look at the first term. We can see that this is a negative log operation, and this little loop, this lowercase sigma is a sigmoid activation function. A sigmoid activation function scales any input from a range from zero to one. So, let's look at the input inside the parentheses. UW0 transpose is the embedding vector for our output target word. So, this is the embedding vector that we know as the correct contexts target for a given input word. This T here is the transpose symbol. Then we have VWI which is the embedding vector for our input word. In general, you will indicate an output embedding and V are input. If you remember from doing cosine similarity a transpose multiplication like this is equivalent to doing a.product operation. So, this whole first term is same that we take the log sigmoid of the.product of our correct output word vector with our input word vector, and this represents our correct target loss. Next, we want to sample our outputs and get some noisy target words, and that's what the second part of this equation is all about. So, let's look at this piece by piece. This capital sigma means we're going to take a sum over all of our words WI. This P and W indicates that these words are drawn from a noise distribution. The noise distribution is our vocabulary of words that are not in the context of our input word. In effect, we want to randomly sample words from our vocabulary to get these noisy irrelevant target words. So P and W is an arbitrary probability distribution which means we can get to decide how to weight the words that we're sampling. This could be a uniform distribution where we sample all words with equal probability or it could be according to the frequency that each word shows up in our text corpus, the unigram distribution UW. In fact the authors of the negative sampling paper found the best distribution to be a unigram distribution raised to the three-fourths. Then we get to this last part which looks very similar to our first term. This takes the log sigmoid of the negated.product between a noise vector UWI, and our input vector from before. To give you an intuition for what this whole loss is doing here, remember that this sigmoid function returns a probability between zero and one. So, the first term in this loss is going to push the probability that our network will predict the correct context word towards one. In the second term, since we're negating the sigmoid input, we're pushing the summed probabilities that our network will predict the incorrect noisy words towards zero. Okay. So next, I'll present your task which will be to define this negative sampling model in code.

### 15. 11 SkipGram Negative V1-e7ZrzpyXNDs.en

All right. So, we have two tasks to complete, to define a more efficient Word2vec skip-gram model. Here, I'm calling this model skip-gram neg to include negative sampling. This model takes in our usual vocab and embedding dimension. It also takes in a noise distribution, if it's provided. Okay. So, first, we want to define two embedding layers, one for input and one for output words. Here, I'm calling those in_embed and out_embed. I want you to define these layers, such that they can accept an input or output target as input and return an embedding that's a vector of dimension in_embed. I'll also suggest that you initialize the weights of these layers using a uniform distribution between negative one and one. Now, let's look at our loss function for a moment. When we think about defining a negative sampling loss, we know that this loss will take in a few things as input. It will for sure take in our input word embedding, vwi. It will also take in our correct output word embedding uw0 and several noisy incorrect embeddings uwi. So, in this model definition, I'm actually going to ask you to define three different forward functions for creating these embeddings. The first forward input should return our input embeddings, which are just going to be our input words passed through our input embedding layer. Similarly, forward output, which should return output vectors for passed and output words. Finally, a forward noise function, this one is special. It takes in a batch size and a number of noise samples to generate for performing negative sampling. This function first gets noisy samples from a passed in noise distribution. If no distribution is passed in, this will default to uniform distribution. Now, it gets a sample of noise words using torch.multinomial and gets batch size times n_samples of values. In this line, those words are being moved to a GPU, if available, and what you need to do to complete this function is pass these words through the output embedding layer to get their respective embeddings. So, you get our noise embeddings and then you should reshape these embeddings to be batch size by n_samples, by n_embed in dimension. All right. So, complete these forward functions, making sure to return correct embeddings for each forward function. If you've completed this implementation, you should be able to proceed with training this model. Next, I'll go over one solution for this model and I'll show you how I defined a custom negative sampling loss.

### 16. 12 CompleteModel CustomLoss V2-7SqNN_eUAdc.en

So, I ran all the cells in my notebook and here's my solution and definition for the SkipGramNeg Module. First, I've defined my two embedding layers, in-embed and out-embed, and they'll both take in the size of our word vocabulary and produce embeddings of size and embed. So, mapping from our vocab to our embedding dimension. Here, I'm doing an additional step which is initializing the embedding look-up tables with uniform weights between negative one and one. I'm doing this for both of our layers and I believe this helps our model reached the best way faster. Then I've defined my three forward functions. Forward input passes our input words through our input embedding layer and returns input embedding vectors. I do the same thing in forward output only passing that through our output embedding layer to get output vectors. Notice that there are no linear layers or softmax activation functions here. The last forward function is forward noise, which will return a noisy target embeddings. So, this samples noisy words from our noise distribution and returns the number of samples batch size times N samples. Then we get the embeddings bypassing those noise words through our output embedding layer. In the same line, I'm reshaping these to be the size I want, which is batch size by N samples by our embedding dimension and I return those vectors. Okay, so this completes the SkipGramNeg Module. Next, I'm defining a custom negative sampling loss. This was carefully defined above in our equations and I haven't ever gotten into the details of defining a custom loss, but suffice to say that it is really similar to defining a model class. Only in this case, the init function is left empty and we're really left with defining the forward function. The forward function should take in some inputs and targets typically and you can define what it takes in as parameters here. This should return a single value that indicates the average loss over a batch of data. So, in this case, I know I what my loss to look at an input embedded vector, my correct output embedding, and my incorrect noisy vectors. So here, I am getting the batch size and embedding dimension from the shape of my input vector, then I'm shaping my input vector into a shape that is batch first, and I'm doing something similar to my output vector here, only I'm swapping these last two dimensions one an embed size effectively making this the output vector transpose. This way, I'll be able to calculate the.product between these two vectors by performing batch matrix multiplication on them, and that's just what I'm doing here. First, I'm calculating the loss term between my input vector and my correct target vector. I'm using batch matrix multiplication and then applying a sigmoid and a log function. Here, I'm squeezing the output so that no empty dimensions are left in the output. Next, I'm doing something similar only between my input vector and my negated noise vectors. So, this is the second term in our loss function. I'm using batch matrix multiplication, applying a sigmoid and a log function, and then I'm summing the losses over the sample of noise vectors. Okay finally, I'm adding these two losses up negating them since I kept them positive during my calculations and taking the mean of this total loss. This way, I'm returning the average negative sample loss over a batch of data. Then I can move on to creating this model and training it. This training loop will look pretty similar to before, but with some key differences. First, I'm creating a unigram noise distribution that relates noisy vectors to their frequency of occurrence, and this is a value I calculated earlier in this notebook. So, I'm defining our noise distribution as the unigram distribution raised to a power of three-fourths as was specified in the paper. Then, I'm defining my model passing in the length of our vocabulary and embedding dimension which I left as 300, and this noise distribution that I've just created, and I'm moving this altered GPU. Then I have another key difference, instead of using NLL loss, I'm using my custom negative sampling loss that I defined above. In my training loop, I'll have to pass in three parameters to this loss function. So, I'm training for five epochs again, getting batches of input and target words. Then using my three different forward functions I'm getting my input embedding, my desired output embedding, and my noise embeddings. So, forward input takes in my inputs, forward output takes in my targets and forward noise takes in two parameters. It takes in a batch size and a number of noise vectors to generate. Then to calculate my loss, I'm passing in my input, output and noise embeddings here. Then, I just have the same code as before, performing backpropagation and optimization steps as usual, and I have my validation similarities that I'm going to print out along with the epoch and loss, a little more information. So, note that I chose to define my three different forward functions just so I a get the vectors that I needed to calculate my negative sampling loss here. You can try training this yourself just to see how much faster this training is. Then imprinting data less frequently because it's generated quicker. So here, after the first epoch, we see our usual sort of noisy relationships. But by the end of training, we see words grouped together that makes sense. So, we have mathematics, algebra, calculus, we have ocean, islands, Pacific, Atlantic, and some smaller words that all seem to be grouped together as well. Once again, I visualize the word vectors using T-SNE. This time I'm visualizing fewer words and I'm getting the embeddings from our input embedding layer only. Then I'm passing these embeddings into our T-SNE model and this is the result I get. I can see some individual integers grouped over here, some educational terms and war and military terms over here. I see some governmental terms and other relationships and it's pretty interesting to poke around a visualization like this. The word2vec model always makes me think about how a learned vector space can be really interesting. Just think about how you might embed images and find relationships between colors and objects or how you might transform words using vector arithmetic. Building and training this model was also quite involved and if you feel comfortable with this model code and especially manipulating models to add your own forward functions and custom loss types, you've really learned a lot about the Pythonic nature of PyTorch and model customization. In addition to implementing a very effective word2vec model. So, great job on making it this far, and I hope you're excited to learn even more.

## Part 02-Module 02-Lesson 06_Sentiment Prediction RNN

### 01. 1 SentimentRNN Intro V1-bQWUuaMc9ZI.en

Welcome to this lesson on Sentiment Analysis with an RNN. All right. So, in this notebook, I want to give you one more LSTM example, and in this case, we'll actually be training an RNN to solve our sentiment analysis task from a few lessons ago. Sentiment analysis is all about taking in some text, in this case movie reviews, and predicting the sentiment of that review. So, whether it's positive or negative. Recall that Andrew Trask showed you how to build a sentiment analysis model from scratch, and he also did some really cool visualizations with this data. What I want to do now is use this as an opportunity to show you how well an RNN versus just a feedforward network performs on this task. My thinking is that an RNN should work really well because we can include information about the sequence of words in a movie review. I've broken this notebook up into a series of exercises, and I'll leave them pretty open-ended. You'll be tasked with pre-processing some text data and building a model that includes both an embedding and LSTM layer. I'll also be showing you a few more things that are useful to know about batching data and making predictions. Next, you'll be able to access this notebook, and I'd suggest that you keep the exercise notebook open. You can have it open in one tab and watch my exercise and solution videos in another, moving back and forth between absorbing information and practicing what you've learned.

### 04. 3 Data PreProcessing V1-Xw1MWmql7no.en

So, let's get started with sentiment analysis. First, I'm going to load in data from our data directory. In here, there are two files reviews.txt and labels.txt. These are just the text files for our movie reviews data and their corresponding labels, positive or negative. So, I'm going to load these in and print out some of their contents. Here, you can see some example review text that's talking about a comedy called bromwell high. And here you see some of the text and the label's file, which just has lines positive and negative. Actually, this looks like just one review and I want to see if I can print out more than one. All right. So here, I've started printing out a second review here and you can see that these two are separated by new line characters, much like positive and negative are separated by new lines. Now, we already know that we need to pre-process this data and to organize all of the words in our vocabulary so that we have numerical data to feed to our model later. Since we're using an embedding layer, we'll need to encode each word as an integer and we'll also want to clean up our data a bit. The first pre-processing steps I want to take are turn our text to lowercase and getting rid of extraneous punctuation. Punctuation that, in this case, will not really have any bearing on whether our review is classified as positive or negative. Okay. So in this cell, I'm converting all my review text to lowercase and I'm getting rid of everything that is punctuation. And I'm using a built-in Python list here, which is from string import punctuation, and I'm going to print out what all is in there. So, punctuation is just a list of all of these punctuation characters. Then for our reviews, I'm looking at every character and if it's not and the punctuation list, I'm keeping it. This gives me a version of the review text that is all text no punctuation. So, I'm storing that in this variable all_text. Next, I know that my reviews are separated by a new line characters slash n. So, to separate out our reviews, I'm going to split the text into each review using slash n as the delimiter here. Then I can combine all the reviews back together as one big string. Finally, I get to my end goal, which is splitting that text into individual words. So, I'll run the cell and print out the first 30 words, and it looks just as I expect. Essentially, the original text that I printed out only all the punctuation is removed and we've separated everything into individual words. So, our data is in good shape, and by now you should know what's coming next. We have to take our word data and our label text data and convert this into numerical data. Your first couple of exercises will be to create a dictionary vocab_to_int that can convert any unique word into an integer token. Then using this dictionary, I want you to create a new list of tokenized words, all the words in our data but converted into their integer values. I'd also like it so that our dictionary maps more frequent words to lower integer tokens. One important thing to note here is that later, we're going to pad our input vectors with zeros. So, I actually do not want zero as a word token. I want the tokenized values to start at one. And so, the most common word in our vocabulary should be mapped to the integer value one. So, create that dictionary, use it to tokenize our words, and then store those tokens in a list, reviews_ints. Below this, I provided some code that lets you test your implementation. It'll print the length of your vocabulary and it will print the first review in your tokenized review list. Your next and similar task is going to be to encode our label text into numerical values. We saw that this text was just lines of positive or negative, and I want you to create an array encoded labels that converts the word positive to one and negative to zero. I'm not providing any testing code here, but I encourage you to get in the habit of testing out your own code piece by piece as you build. It's good practice and can be as simple as a few print statements to check that your data is converted as you expect or that it's the correct size and so on. These checks can really save some time later on because these code blocks really build on one another, and it's good to debug early and often. Okay. So, try encoding all of our words and labels on your own. And if you get stuck or want to check your solution, feel free to look at the solution video next.

### 05. 4 EncodingWords Sol V1-4RYyn3zv1Hg.en

First, here's how I went about creating a vocab to int dictionary and encoding our word data, and there are a few ways to do this. I chose to use this important counter to create a dictionary that maps the most common words in our reviews text to the smallest integers. So the first thing I'm doing is to get a count of how many times each of our words actually appears in our data using counter and passing in our words. Then with these counts, I'm creating assorted vocabulary. This sorts each unique word by its frequency of occurrence. So this vocab should hold all of the unique words that make up our word data without any repeats, and it will be sorted by commonality. I also know that I want to start encoding my words with the integer value of one rather than zero. So the most common word like be or of should actually be encoded as one. I'm making sure that we start our indexing at one by using enumerate and passing in our vocab and our starting index, one. Enumerate is going to return a numerical value, ii, and a word in our vocabulary. It will do this in order. So our first index is going to be one, and the first word is going to be the most common word in our assorted vocabulary. So to create the dictionary vocab to int, I'm taking each unique word in our vocab and mapping it to an index starting at the value one. Great. Next, I'm using this dictionary to tokenize all of our word data. So here, I'm looking at each individual review. Each of these is one item and review split from before when I separated reviews by the newline character. Then, for each word in a review, I'm using my dictionary to convert that word into its integer value, and I'm appending the token as review to reviews_ints. So the end result will be a list of tokenized reviews. Here in the cells below, I'm printing out the length of my dictionary and my first sample encoded review. I can see that my dictionary is a bit over 74,000 words long, which means that we have this many unique words that make up our reviews data. Let's take a look at this tokenized review. I'm not seeing any zero values which is good, and these encoded values look as I might expect. So I've successfully encoded the review words, and I'll move on to the next task, which is encoding our labels. So in this case, I want to look at my label's text data and turn the word positive into one and negative into zero. Now we haven't much processed our labels data, and I know much like the reviews text that a new label is on every new line in this file. So I can get a list of labels, labels_split, by splitting our loaded in data using the newline character as a delimiter. Then I just have a statement that says, for every label in this label_split list, I'm going to add one to my array if it reads as positive, and a zero otherwise. I'm wrapping this in np.array, and that's all I need to do to create an array of encoded labels. All right. This is a good start. There are still a few data clean up and formatting steps that I'll want to take before we get to defining our model. So let's address those tasks next.

### 06. 5 GettingRid ZeroLength V1-Hs6ithuvDJg.en

After encoding all our word and label data as an additional preprocessing step, we want to make sure that our reviews are in good shape for standard processing. That is, our network will expect a standard input text size, and so we'll want to shape our reviews into a consistent specific length. There are two things we'll need to do to approach this task. First, I'm going to take a look at the review data and see do we have any especially sure or longer views that might mess with our training process. I'll especially look to see if we have any reviews of length zero which will not provide any text information and will just act as noisy data. If I find any of those zero length reviews, I'll want to remove them from our data entirely. Then second, I'll look at the remaining reviews, and for really long reviews, I'll actually truncate them at a specific length. I'll do something similar for shortest reviews and make sure that I'm creating a set of reviews that are all the same length. This will be our padding and truncation step, where we basically pad our data with columns of zeros or remove columns until we get our desired input shape. Okay. Before we pad our review text, we should check for reviews of length zero. The way I'm gonna do this is to use a counter. For each review length that's currently in our data, whether that's a length of zero or thousands of words, I'll look at how many reviews are of that length. So this returns a dictionary of review lengths and account for how many our reviews fall into those lengths. So, here I'm looking at how many of our reviews are zero length and I'll also print out the longest review length just to see. So, when I run this cell, I can see that I have one review that is zero length and that my longest review has over 2,000 words in it. This zero length review is just going to add noise into our dataset. So next, your task will be to create a new list of reviews_ints and an array of encoded labels, where any reviews of zero length will be removed from this data. So, remove any zero length reviews from reviews_ints and remove that corresponding label as well. In this particular case, after running this cell, I expect to see that one of our reviews was removed. Try to solve this task on your own, and next, I'll present my solution and introduce you to your next exercise.

### 07. 6 Cleaning And Padding V1-UgPo1_cq-0g.en

So last time, we noticed that we had one review with zero length in our dataset. This review will not contribute any meaningful training information. So here, I'm removing any reviews of zero length from our reviews ends list. I'll do the same thing with their corresponding label. The way I went about this task was I thought, "Okay, I'm going to want to find any reviews of zero length and remove that data from my existing reviews ends and encoded labels data." So, I first identified the indices in our data that I want to keep which I'll call my non-zero indices. I'm checking the length of each review in our reviews end data. If the length is not equal to zero, that means I want to keep it and I'm recording its index in our list of non-zero indices. Then I'm just getting those indices from my existing reviews ends list and encoded labels array. I'm just trying the new clean data in these variables of the same name. When I do my length check, I can see that this is effectively removed one review from our data. In this case, there was only one review of zero length, so this looks good. Now, the next thing I want to deal with is very long review text data and standardizing the length of our reviews in general. We saw that the maximum review length was about 2,500 words and that's going to be too many steps for our RNN. In cases like this, I want to truncate this data to a reasonable size and number of steps. This brings me to the next exercise. To deal with both short and very long reviews, we'll either pad or truncate all reviews to a specific sequence length. For reviews that are shorter than some sequence length, we'll pad it on the left with zeros. For reviews longer than the sequence length, we can truncate them to the first sequence length worth of words. So, here's a padding example. Say, we have a short sequence of words and we specify that we want a sequence length equal to 10. The resultant padded sequence should be this, padded on the left with seven zeros and the original three-word tokens are at the end. Now in the case of a long review, it would just be cut at the sequence length of 10. This is just a small example and for our movie review data, a good sequence length is going to be around 200. An exercise, I want you to complete this function pad features. This takes in our list of reviews ends and a sequence length and it should either pad or truncate every review in the past end list. It should return an array of transformed reviews which I'll call R features which are our tokenized reviews of the same sequence length and you'll often hear transform data like this referred to as the features or input features for a model. So, at the end, each of the rows in the feature's array will be transformed review of a standard sequence length that we can then feed into a model as input. So, try to solve this and then the next cell I've included some print statements and assertions that act as tests on the shape of your feature's array. This will help you check your work and next I'll go over one solution

### 08. 7 PaddedFeatures Sol V1-sYOd1IDmep8.en

So here's my solution for creating an array of features, reviews that have either been padded on the left with zeros until their sequence length on or truncated at that length. First, I'm actually creating an array of zeros, that's just the final shape that I know I want. That is, it should have as many rows as I have reviews in the input reviews_ints data into as many columns as the specified sequence length, and this will just hold all the zero integers for now. Then for each review in my list, I'll put it as a row in my features array. The first review is going to go on the first row, and the second in the second, and so on. I started out thinking of my short review case. I want to keep a left padding of zeros, up until I reach where that review can fill the remaining values. So, I'm looking at filling my features, starting at the index that's at the end of the features row, minus the length of the input review. So, if a reviewer show, this means our features are going to keep the zeros which are padding on the left, and the review tokens will be on the right side. It turns out that I only have to add one more piece to this line to make this work for a long reviews too. Hear for annual review including those longer than the given sequence length, I'm truncating them at that sequence length, and this should fill the corresponding features row. So, this loop will do this for every review in reviews_ints, and then returns these features. Below I'm running my test code. Here I'm creating features passing in my list of reviews_ints and a sequence length equal to 200. I don't trigger any of these error messages, so I know my dimensions are correct, and then printing out the first ten values of the first three rows here. And here's what these rows look like. A lot of these start with zeros, which is what I expect for left padding, and others have filled up these rows with various token values. So, this is great. And I'll also add that. In this step, we've actually introduced a new token into our review features. Remember that before, all words in our vocabulary hadn't associated integer value, and we started organizing with the value one. So, in our vocab_to_int dictionary, we had integers from one up to 74,000 or so. And here by adding zero as padding, I've effectively inserted the zero token into our vocabulary. Okay. Now for your next and the last data transformation exercise with our data in nice shape, next, I want you to split the features and encoded labels into three different datasets, training, validation, and test sets. You'll need to create datasets for grouping our features and labels like train_x and train_y, for example. And we'll use these different sets to train and test our model. So, I've defined a split fraction, split_frac, as the fraction of data to keep in the training set. This is set to 0.8 or 80 percent of data. The 20 percent of the data that's left should be split in half to create the validation and testing data respectively. So, I'll leave this as an exercise. And next, I'll go over how I split the data and I'll show you some PyTorch resources we can use to effectively batch and iterate through these different datasets.

### 09. 8 TensorDataset Batching V1-Oxuf2QIPjj4.en

In this cell, I've split our features and encoded labels into training test and validation sets. I started by splitting our features and label data according to their split frac. So, I'm reserving 80 percent of my data for training and I'm basically getting the index at which I should split my features and label data based on this value 0.8 and actually I could have just put in this variable here. Then I'm splitting my features first, getting the features up until my 80 percent split index. This makes up my training features train_x then I'm getting the remaining data. So, after the split index and that makes up my remaining_x. Then, I'm doing the exact same thing but for my labels data, splitting it at the 80 percent index to get my training labels and my remaining data then I'm doing something similar all over again only with the remaining data. I'm getting an index to split this data in half, so at the 0.5 mark. Then each half of our remaining_x will make up our validation and test sets of features and each half of remaining_y will make up our validation and test set of labels. That's it. The last step I'm doing is checking my work and printing out the shapes of my features data. I can see that I have the largest number of reviews in my training set with a sequence length of 200 and my validation and test sets are of the same size. If you want, you can do the same thing for your labels data and you should see the same number of rows here. So, this is 80 percent of my data, 10 percent, and 10 percent. After creating training test and validation data, we want to batch this data so that we can train on batches at a time. Typically, you've seen this done with a generator function which we could definitely do here but I want to show you a really nice way to batch our datasets when we've split up our input features and labels like this. We can actually create data loaders for our data by following a couple of steps. First, we can use pytorch's tensor dataset to wrap tensor data into a known format and I can look at the documentation for this here. This dataset basically takes any amount of tensors with the same first dimension, so the same number of rows, and in our case this is our input features and the label tensors and it creates a dataset that can be processed and batched by pytorch's data loader class. So, once we create our data wrapping it in a tensor dataset, we can then pass that to a data loader as usual. Data loader just takes in some data and a batch size and it returns a data loader that batches our data as we typically might. This is a great alternative to creating a generator function for batching our data into full batches. The data loader class is going to take care of a lot of behind-the-scenes work for us and here's what this looks like in code. First, I'm creating my tensor datasets. To create my training data, I'm passing in the tensor version of my train_x and train_y that I created above and torch that from numpy just takes in numpy arrays and converts them into tensors. So, I'm doing that for my training validation and test data and if you named your data differently above, you'll have to change those names here. In fact, I could have actually done these steps the other way around. Creating a tensor dataset for all my data and then splitting the data into different sets. Both approaches work. Then for each tensor dataset that I just created, I'm passing it into pytorch's data loader or I can specify a batch size parameter equal to 50 in this case. So, without the messiness of loops and yield commands, this defines training validation and test data loaders that I can use in my train loop to batch data into the size I want. So, this gives me three different iterators and I just want to show you what a sample of data from this data loader looks like looking at our train loader and getting an iterator, then grabbing one batch of data using a call to next. So, this should return some sample input features and some sample labels. Then I'm printing out the size of my input which I can see is the batch size 50 and the sequence length 200 and the output label size which is just 50, one label for each review in the input batch and I see my tokens and the encoded labels as well. So, this is looking really great. Next, we can proceed with defining and training the model on this data.

### 10. 9 DefiningModel V1-SpvIZl1YQRI.en

By now, you've had a lot of practice with data processing and with defining RNNs. So, I'm not going to give you too much guidance here, when it comes to defining this model. Here's what it should look like generally. The model should be able to take in our word tokens, and the first thing that these go through will be an embedding layer. We have about 74,000 different words, and so this layer is going to be responsible for converting our word tokens, our integers into embeddings of a specific size. Now, you could train a Word2Vec model separately, and actually just use the learned word embeddings as input to an LSTM. But it turns out that these embedding layers are still useful even if they haven't been trained to learn the semantic relationships between words. So in this case, what we're mainly using this embedding layer for is dimensionality reduction. It will learn to look at our large vocabulary and map each word into a vector of a specified embedding dimension. Then, after our embedding layer, we have an LSTM layer. This is defined by a hidden state size and number of layers as you know. At each step, these LSTM cells will produce an output and a new hidden state. The hidden state will be passed to the next cell as input, and this is how we represent a memory in this model. The output is going to be fed into a Sigmoid activated fully connected output layer. This layer will be responsible for mapping the LSTM layer outputs to a desired output size. In this case, this should be the number of our sentiment classes, positive or negative. Then, the Sigmoid activation function is responsible for turning all of those outputs into a value between zero and one. This is the range we expect for our encoded sentiment labels. Zero is a negative and one is a positive review. So, this model is going to look at a sequence of words that make up a review. Here, we're interested in only the last Sigmoid output because this will produce the one label we're looking for at the end of processing a sequence of words in a review. So here's a little more explanation and some links to documentation if you need it. Then below, in this first cell, I'm going to check if a GPU is available for training. Then here, I want you to complete this model. It should take in all these parameters: our vocab size, output size, embedding dimension, hidden dimension, number of layers, and an optional dropout probability, and create an entire sentiment RNN model. You'll be responsible for completing the init and forward functions for this model. Remember that the output should just be the last value from our Sigmoid output layer. I'll also ask you to complete the init hidden function for the LSTM layer. This should initialize the hidden and cell state to be all zeros and move them to a GPU, if available. I'd encourage you to look at the documentation when helpful or your other code examples. You should have all the information you need to complete this model on your own. If you're confident in your model definition, later in this code, you'll be able to define your model hyperparameters and train it. Next, I'll show you one solution for defining the sentiment RNN model. But I do think this is a fun task to try out on your own in earnest too. I think it's a great exercise in thinking about how data is shaped as it moves through a model. So, good luck.




## Part 06-Module 01-Lesson 01_Descriptive Statistics - Part I

### 01. Instructors Introduction-lIvm8urf4GE.en

Statistics is at the core of analyzing data. For the stats portion of this class, you'll be learning from Sebastian Thrun and Josh Bernhard. Sebastian is a statistician and Stanford faculty member, as well as founder of Udacity and Google X. He'll be showcasing a number of examples for each of the statistical topics covered. Josh who is also a statistician teacher, has taught statistics at the University of Colorado and previously was working as a machine learning engineer for KPMG. He'll be working alongside Sebastian to assure you can implement the statistical applications in Python. With that, let's get started.

### 03. What Is Coming Up-oDJsnQcCPr4.en

A quick overview leading up to your first project. We will start with an overview of data types and the most common statistics used when analyzing data. We'll discuss measures of center and spread. The most common shapes that data takes on and how to handle outliers. You will take this farther by using spreadsheets to handle these calculations for you. You'll learn how to build visuals to better convey your message as well as how to use these features of spreadsheets to take your data game to a whole new level. All of this is aimed to help you succeed on the very first project and for analyzing your own data.

### 04. What is Data-ldTDAjrVsA8.en

The word "data" is defined as distinct pieces of information. You may think of data as simply numbers on a spreadsheet, but data can come in many forms. From text to video to spreadsheets and databases to images to audio, and I'm sure I'm forgetting many other forms. Utilizing data is the new way of the world. Data is used to understand and improve nearly every facet of our lives. From early disease detection to social networks that allow us to connect and communicate with people around the world. No matter what field you're in, from insurance and banking, to medicine, to education, to agriculture,, to automotive, to manufacturing, and so on. You can utilize data to make better decisions and accomplish your goals. We will be getting you started on the right foot to using your data in this course.

### 05. Data Types-gT6EYlsLZkE.en

In this video, we'll be taking a look at the different data types that exists in the world around us. When sitting at coffee shops, I enjoy watching the dogs pass. I often wonder, how many crossed my path? I wonder if more pass on weekdays or weekends. Maybe the number differs from Mondays to Tuesdays. I also pay attention to the breeds of the dogs. I wonder if more collies stopped by on Monday than on Wednesday. I wonder what's the most common breed. Is that breed the most common at all coffee shops? If I walked across the street to my favorite breed, would the most common breed change? This introduces two main data types: quantitative data, like the number of dogs, and categorical data, like the breed. Quantitative data takes on numeric values that allow us to perform mathematical operations. In the previous example, we saw this with the number of dogs. If I see five dogs on Monday, and six dogs on Tuesday, I've seen a total of 11 dogs so far this week. Alternatively, categorical data frequently are used to label a group or set of items. We saw this with the breeds of the dogs.

### 07. Categorical Ordinal  Nominal Data-k5bLaPGY2Vw.en

We can divide categorical data types further into categorical ordinal and categorical nominal. First, let's look at categorical ordinal data. Remember those dogs at the coffee shop? Let's say I give each a rating of how nice it is to me. Sometimes I shake hands with the dog and we become best friends. Other times, it pees on my shoe. I rate these interactions from very positive to very negative. These ranked categories are known as categorical ordinal data. Notice, this is different than the breeds, which are known as categorical nominal data, and do not have a rank order.

### 08. Continuous vs. Discrete Data-BzgZebZD9kk.en

We can also divide quantitative data types further. I assume most of my positive interactions occur with older dogs, as they've had more time to train. The age of a dog is a continuous quantitative data type, while the number of dogs I interact with is a discrete quantitative data type. In the world, we kind of make all data discrete, so it can be difficult to see the difference between discrete and continuous. Continuous data can take on any numeric value including decimal values, and sometimes even negative numbers. The age of a dog in this situation is an example of continuous data, as we could split this variable into smaller and smaller pieces, and still something exists. For example, we could talk about age in terms of years, or months, or days, or hours, or minutes, or seconds, and still there are units that are smaller. This is true of continuous data. However, discrete data like the number of dogs, only takes on countable values.

### 09. Data Types Summary-T-KrQoAJUpI.en

To summarize, we have two main data types, each with two subgroups. Quantitative data can be divided into continuous and discrete. Categorical can be split into nominal and ordinal. Identifying data types is important, as it allows us to understand the types of analyses that we can perform and the plots that we can build. Before we dive deeper, check your understanding. There are quizzes available in the next module to assure that you've mastered the concepts we've introduced.

### 12. Introduction to Summary Statistics-PCZmHCrcMcw.en

In the next lessons, we will discuss how to use statistics to describe quantitative data. You will gain insight into a process of how data is collected and how to answer questions using your data. Throughout this lesson, I hope you learn to be critical of your analysis that happened under the hood and what the numbers actually mean. As an example of an analysis that we do here at Udacity, we look at how long a nanodegree program takes students to complete. We try to provide an estimate of the number of months or hours that students will spend. One way we might start is by reporting the average amount of time it takes to complete the nanodegree program. But that doesn't tell the whole story. I'm sure there are differences in completion depending on what students knew before entering the program. The shortest amount of time needed to complete the nanodegree program might just be a few weeks. How did those people complete the course so fast? Well, the longest might be a couple of years. What proportion of students finish fast within two months? What proportion take longer than eight months? Using a variety of measures, like measures of center, give you an idea of the average student. Measures of spread give you an idea of how students differ. Visuals can provide us a more complete picture of how long it takes any student to complete a program. The material in the next sections will show you how to use these measures in a way that is informative and understandable to others.

### 13. Calculating the Mean-1nzZxmJ8xvU.en

When analyzing both discrete and continuous quantitative data, we generally discuss four main aspects. The center, the spread, the shape and outliers. In this lesson, we will focus on measures of center. There are three widely accepted measures of center. The mean, the median and the mode. To illustrate how each of these measures is calculated, consider this table of the number of dogs I see at the coffee shop in a week. From the table, we can see that on Monday, I saw five dogs. On Tuesday, I saw three dogs. On Wednesday, I saw eight dogs and so on. A friend might ask you, how many dogs would you expect to see on any given day? We might choose to respond to this in a lot of different ways. Like, it depends on the day or it depends on the week. But commonly, the word expect is associated with the mean or the average of our data set. The mean is calculated as the sum of all of the values in our data set divided by how many data points we have. As you can see calculated here, this value is 12.57 dogs. That is, the sum of the number of dogs observed on each day divided by the number of days in a week. The mean isn't always the best measure of center. For this data set, you can see that the mean doesn't really seem like it's in the middle of the data at all. There are only two of the seven days that have recorded more dogs than the reported mean. It also is splitting our dogs into decimal values which will seem strange when we're reporting back to our friend.

### 15. The Median-WlT3eeW0rb0.en

A more appropriate measure in this case, might be the median. The median is a value that divides our data set such that 50% of the values are larger while the remaining 50% are smaller. For our data set, we have a median of eight. This is a much better response than the 12 and a half dogs reported by the mean. Not only does eight sit in the middle of our data set, but it doesn't split any of our dogs in half. As a note on calculating the median, the actual calculation of the median depends on whether we're working with a data set with an even number of values or an odd number of values. Let's consider a couple of examples to illustrate this point. The first thing we should do is order the values from smallest to largest. In this top case, we are working with seven values. In notation we might say, n equals seven. Because this is an odd number, we have the median as the exact middle value, in this case, the number three. In a second case, let's consider n to be an even value. Let's find the median of the following list. Again, we should first put these values in order. Because there isn't an exact center value, we will, in this case,take the mean of the two values in the center as our median. Notice, this does not even need to be a number in our data set. Because n is equal to eight for this example, the mean of the fourth and the fifth values will provide to us the median for this data set. Moving four values from each of the sides, makes it so that we take the mean of three and five to obtain the median of four.

### 17. Measures of Center - The Mode-NE81NZgECqg.en

The third measure of center, aims at providing us the most common value in the data set. In this data set, this is the value of three. The value that occurs most often is known as the Mode. These are all three potential measures of center. The Mean or the average, the Median or the middle value, and the Mode or the most frequent value.

### 19. What is Notation-MaHV5cKfcmE.en

Previously, we listed the four main aspects of analyzing quantitative data; center, spread, shape and outliers. We also looked specifically at measures of center, by introducing means, medians and modes. Before we look at measures of spread, it's important to understand notation. You might not even know it, but you use notation all the time. Consider this example, of five plus three. Plus is an English word. This symbol is notation and its universal. Notation is a common math language used to communicate. Regardless, of whether you speak English, Spanish, Greek or any other language, you can work together using notation as a common language to solve problems. Like learning any new language, notation can be frightening at first, but it's an essential tool for communicating ideas associated with data. We will work together through some examples to assure you completely master this concept.

### 20. Notation for Random Variables-8NxTW1u4s-Y.en

As a first example, let's apply this new idea of notation to something you've used before. Spreadsheets. Spreadsheets are a common way we hold data in the real world. In our spreadsheet, we have rows and columns. To better understand how we use spreadsheets to hold data, let's work through an example. Before even collecting data, we usually start with a question or many questions. Consider I run a small blog about my best and worst adventures with the dogs at the coffee shop. Which also sells trinkets related to those adventures. Everything from fetch toys to leashes, to doggie bags, and everything in between. Questions I might have are: How many people visit my site? Or how much time do visitors spend on my site? Are there differences in traffic, depending on the day of the week? How many visitors purchase an item through the blog? In order to answer these questions, say we keep track of the date of the visit, the day of the week of the visit, the amount of time spent on the site, and whether or not an individual buys an item. We can think of each of these as a column. A column in our dataset is associated with a random variable. Explaining what a random variable is in English is complicated. But in notation, it's simple. In English, a random variable is a placeholder for the possible values of some process. In notation, it's X. For our website, the date of the visit, the day of the week of the visit, the amount of time spent on the site, and whether or not an individual buys an item are all variables. Let's say we have a visitor on Thursday, June 15. The visitor stays on our site for five minutes and doesn't buy an item. Then a second visitor, visits the site on the exact same day for 10 minutes and they do buy an item. Notice how each of these individuals has been added to our spreadsheet. We might have many more visitors, and we could update our spreadsheet accordingly. When using spreadsheets, we frequently analyze a full column to answer our questions of interest. For example, to answer the question of how much time do visitors spend on our site? We need to look at this column. To answer the question of, Are there differences in traffic, depending on the day of the week? We need to look at this column. And to answer the question of, How many purchases occur through our blog? We need to look at this column. Mathematically, we usually consider a random variable or column using a capital letter. Commonly, we use the capital letter X, but we can just as easily use Y, Z or any other capital letter. We might say, consider the random variable X, which signifies the amount of time an individual spends on our website. Therefore, X relates to this entire column. Consider we also have a random variable Y, which signifies whether or not an individual purchases an item from the website, so Y relates to this entire column.

### 22. Random  Observed Values-KFIt2OC3wCI.en

Connecting this full circle, these capital letters X and Y relate to a random variable. This is an abstract idea. How much time an individual spends on our website, can take on lots of different values. So, this capital letter X is not a number, it's an entire set of possible values. We can think of capital X as a placeholder for any of these possible values. When we look at an individual outcome of our random variable, we signify this with a lower case letter. Often the lower case letter has a subscript, that helps us attach notation to each specific value in our dataset. For this dataset, we would say the amount of time an individual spends on our site is provided by an amount, which we would notate with a capital X. The first visitor spends five minutes on the site, which we would notate with x1. Notice, this is a lowercase x. The second visitor spends 10 minutes on the site, which we wouldn't notate as x2. Again, since this is an observed value this is a lowercase x. The labeling could continue until we reach the final visitor in our dataset, who we'd called the nth visitor. We would label this value as xn. Again, this is a lowercase value. Notation is an essential tool for us to communicate mathematical ideas. We have now discussed the idea that capital letters are used as notation for random variables. When we observe a particular value of the random variable we use a lowercase letter, with a subscript that signifies the specific value of the random variable we are considering. Notation can be tricky. Before we dive deeper, check your understanding. There are quizzes available in the next module to assure you've mastered the concepts we've introduced.

### 24. There Must Be A Better Way-oBp8YX2AgJw.en

In the next concepts, we are going to combine what we know about how to calculate the mean with notation. The purpose of this video is not to relearn how to calculate the mean but rather to introduce notation using a measure you already know. Let's consider the amount of time someone spends on our website in minutes. Imagine we collect the amount of time spent on our site for five individuals. From the last video, we saw that we could label the values in this way, x1 for the first value, x2 for the second value, x3 for the third value and so on. Imagine we want to add the first two numbers together. We could write this in notation as x1 plus x2 which in this example, would be the same as 15 plus 10. If we wanted to add more than just two values, it would be tedious to continue the same process. Imagine if we had 100 values. We would have a, x1 plus x2 plus x3 plus and we'd have to continue this process until x 100. Someone must have come up with a better way to notate this. And it turns out, there is a better way.

### 25. Aggregations-ADx1x2ljFB4.en

There are common ways to notate most aggregations. An aggregation is just a way to turn multiple numbers into fewer numbers, usually just one number. Common aggregations include the measures of center we introduced earlier, like the mean, the median, and the mode. Each of these takes many numbers and provides a single value to give information about the data set. In this example, we want to aggregate all the values into a single sum. In other words, add them up. The Greek alphabet is a popular place to pull notation. Similar to English, there are both lower and upper case letters in the Greek alphabet. For summing many values together, we use a Greek symbol known as Sigma. Specifically, we use the uppercase Sigma. We generally use the symbol in the following way: you will notice that instead of writing multiple x values, each with a different subscript, we write a single x with a subscript of i. Here, the I is a placeholder that tells us which x values we'll be summing up. In our first case, summing only the first two values, we want this i to be the value of either one or two. Now you might be asking, how does this notation show which values we're summing together. This is a great question. If we would like to sum just the first two values, we would write something like this. Notice at the bottom, we are giving our starting point. That is, we would like the first x value to be where i equals one. You can imagine replacing the one into this subscript. Then the value at the top gives us an ending point for where we stop. Here, our ending is two. The Sigma tells us we would like to sum from this bottom value up through all the values until we hit this top value. The new notation, with the summation symbol, is the same as our original notation of x1 + x2.

### 26. Notation for the Mean-3EF15AoRxyM.en

But now, if we want to sum all the values in our original example, we no longer need to write out all of the xs. Instead, we can write our summation starting at i=1 and ending at the fifth value. This is way better when wanting to extend to 10 or 20 or even 100 values. We no longer need to write out all of those xs. However, we can be even more efficient than this. Each time our data set changes, I have to change this number at the top to indicate how many values I'm summing. If we really just want to sum all of our values, we can replace this top value with n. Now our notation will work for any data set. So far, we have a way to sum all of our values regardless of how many are in our data set. In order to finish this calculation of the mean, we need to divide our sum by how many values are in our data set. But this is just n. You would commonly see the mean of the data set notated like this, which we pronounce as x-bar, and it is calculated with this notation which says you sum all of the values in the data set and then divide by the number of values in the data set. Learning notation can be tough. Don't be afraid to watch this video more than once. Before we move on to the next sections, there are quizzes available in the next module to assure you've mastered the concepts we've introduced here.



## Part 06-Module 01-Lesson 02_Descriptive Statistics - Part II

### 01. 26 Spread Part 1-zb76Z_viYLY.en

That last section was intense. I hope the quizzes reinforce what was shown in the videos. I know the first time I saw a notation, it totally went over my head. We will begin this lesson looking at the second aspect with regard to analyzing quantitative variables, the spread. When we discussed measures of spread, we are considering numeric values that are associated with how far points are from one another. Common values of spread include the range, the inter-quartile range, the standard deviation, and the variance.

### 02. Histograms-4t10RgUv2Fc.en

It is easy as to understand the spread of our data visually. The most common visual for quantitative data is known as a histogram. In this video, we will take a look at exactly how histograms are constructed. In order to understand how histograms are constructed, consider we have the following data-set. First, we need to bin our data. You as the histogram creator ultimately choose how the binning occurs. Here, I have chosen our bins as one to four, five to eight, 9-12 and 13-16. Because these first four values are between one and four, they go into the first bin. These next three values are between five and eight, so they fall in the next bin, then these two values fall in this bin and 15 falls into our last bin. The number of values in each bin determine the height of each histogram bar. Changing the bins will result in a slightly different visual. There really isn't a right answer to choosing our bins, and in most cases software will choose the appropriate bins for us. But it is something to be aware of.

### 03. What is the Difference-I3tQvrCgNrQ.en

Here, are two histograms comparing the number of dogs that I saw on weekdays to the number of dogs I saw on weekends from last year. You will notice that the tallest bins for both weekdays and weekends are associated with 13 dogs. So the number of dogs I expect to see are essentially the same for weekdays as on weekends. And the measures of center would basically be the same here. Both have a mean, median, and mode that are about 13 dogs. But something is different about these two distributions. So what's the difference? Well, the difference is how spread out the data are for each group. You can see that the number of dogs I see on weekdays, ranges from 10-16. While on weekends, it ranges from 6-18. In the upcoming sections, we will look at the most common ways to measure the spread of our data.

### 04. 29 Number Summary-gzUN5zKLHjQ.en

One of the most common ways to measure the spread of our data is by looking at the Five Number Summary, which gives us values for calculating the range and interquartile range. The Five Number Summary consists of five values, the maximum, the minimum, the first quartile, the second quartile which is also the median, and the third quartile. Consider we have the following dataset. The first thing we need to do to calculate the Five Number Summary is to order our values. Once ordered, the minimum and the maximum values are easy to identify as the smallest and largest values. As we calculate it in the section on measures of center, the median is the middle value in our dataset. We also call this Q2 or the second quartile because 50% of our data or two quarters fall below this value. The remaining two values to complete the Five Number Summary are Q1 and Q3. These values can be thought of as the medians of the data on either side of Q2. That is the median of these data points is Q1, this value is such that 25% of our data fall below it, and the median of these data points is Q3 or the third quartile. This value is such that 75% of our data fall below this mark. Notice, Q2 was not an either a set of these points used to calculate Q1 or Q3. This provides our Five Number Summary as the following. Let's consider another example for in our dataset has an even set of values. Again, we first need to order the values. We can quickly identify the maximum and the minimum. Remember, with an even number of values, the median or Q2 is given as the mean of these two values here. In order to find Q1 and Q3, we divide our dataset between the two values we use to find the median. This provides these two datasets. Finding the median of each of these will provide Q1 and Q3. For this dataset, Q1 will be the mean of these two values, and Q3, will be the mean of these two values. This provides our Five Number Summary as the following. Once we've calculated all the values for the Five Number Summary, finding the range and interquartile range is no problem. For the first dataset, the range is calculated as the maximum minus the minimum. For the first dataset, this was 10 minus 1 or 9. And the interquartile range is calculated as Q3 minus Q1, which is 8 minus 2 or 6.

### 06. 5 Number Summary to Variance-Ljhau0hrZ1g.en

Looking back at the distributions we found for the number of dogs I see, we can mark the values of our five number summary like this. If we take just these marks, this makes a common plot for data known as a box plot. Though I prefer a histogram in most cases, a box plot can be useful for quickly comparing the spread of two data sets across some key metrics, like our quartiles, and the maximum and minimum. From both the histogram and the five-number summary, we can quickly see that the number of dogs I see on weekends varies much more than the number of dogs I see on weekdays. We can also visualize the distance from here to here as the range, while the distance from here to here is the interquartile range. There are lots of useful metrics we can get from these box plots. But what if we want to compare the spreads of these distributions without having to carry around all five of these values for each distribution? What if I wanted just a single value to be able to compare the two distributions spreads?

### 07. What is the Standard Deviation Measuring-IbwUJ3ORZ5s.en

The most common way that professionals measure the spread of a data set with a single value is with the standard deviation or variance. Here, we will focus on the standard deviation, but we will actually learn how to calculate the variance in the process. If you have never heard of these measures before, this calculation will probably look pretty complex. When all's said and done with this calculation, the standard deviation will tell us on average how far every point is from the mean of the points. As a quick mental picture, imagine we wanted to know how far employees were located from their place of work. One person might be 15 miles, another 35, another only one mile, and another might be remote and is 103 miles. We could aggregate all of these distances together to show that the average distance employees are located from their work is 18 miles. But now, we want to know how the distance to work varies from one employee to the next. We could use the five number summary as a description. But if we wanted just one number to talk about the spread, we'd probably choose the standard deviation. The standard deviation is on average how much each observation varies from the mean. For this example this is, how much on average the distance each person is from work differs from the average distance all of them are from work. So, this one is three miles farther from work than the average while this individual is four miles closer to work than the average. The standard deviation is how far on average are individuals located from this mean distance. So, it is like the average of all of these distances. We will take a closer look at this but hopefully this gives you a strong conceptual understanding of what we'll be calculating in the next sections.

### 08. Standard Deviation Calculation-H5zA1A-XPoY.en

In the last video, we got an idea of what the standard deviation is measuring. In this video, we will look at the math that actually occurs when calculating this measure. We will work with data to calculate this measure as well as associate notation with it. It is worth noting that after this lesson, you probably won't calculate this measure by hand ever again, because you'll learn software to do it for you. The calculating it yourself will give you intuition behind what it's actually doing. And this intuition is necessary to become good at understanding data and choosing the right analysis for your situation. Imagine we have a data set with four values, 10, 14, 10, and 6. The first thing we need to do to calculate the standard deviation is to find the mean. In notation, we have this as X bar. For our values, the sum is 40, and we have four numbers. So the mean is 40 over 4 or 10. Then we want to look at the distance of each observation from this mean. Two of these observations are exactly equal to the mean. So the distance here is zero. One is 4 larger the 14, while the other is 4 smaller the 6. In notation, each of these is XI minus X bar. Then, if we were to average these distances, the positive would cancel with the negative value. And the value of zero isn't a great measure of the spread here. Zero would suggest that all the values are the same or that there's no spread. So instead, we need to make all of these values positive. The way we do this when calculating the standard deviation is by squaring them all. If we do that here, our negative and positive 4 values will become 16s. Now, we could average these to find the average squared distance of each observation from the mean. This is called the variance. Finding the average, just as we did before, means adding all of these values and dividing by how many there are. In our case, we had 0, 16, 0, 16 and we divide by 4 because we have four observations. However, this is an average of squared values which we only did to get positive values in the first place. So to get our standard deviation, we take the square root of this ending value. Here, our standard deviation is 2.83. So this is on average how far each point in our data set is from the mean, which is the definition of the standard deviation.

### 11. Why the Standard Deviation-XlTBvjQ2t8w.en

So it might seem absurd to do this calculation of the standard deviation. I mean, it's such a complicated way to measure the spread of our data compared to the five number summary we saw earlier. But it turns out that the standard deviation is used all the time to get a single number to compare the spread of two data sets. It is kind of nice to be able to talk about how spread out our data are from one another without having to report an entire table of values. We can just compare the standard deviation for one group to the standard deviation of another group. And we have a way to tell which dataset is more spread out. Having one number simplifies the amount of information that the person you're reporting to needs to consume. Having the single value also has other advantages with regard to what is known as inferential statistics, but that's beyond what we need to know now. For now, we just need to know that we have a way to take all of our values and get a single number that tells us how spread out they are from one another.

### 12. Variance  Standard Deviation Final Points-vXUgp2375j4.en

A few quick final points to keep in mind about the standard deviation. First, the standard deviation is frequently used to compare the spread of different groups to determine which is more spread out. Second, when data pertains to money or the economy, having higher standard deviation is associated with having higher risk. In comparing stock prices, a stock price that changes with higher standard deviation over time is considered more risky than a stock price that fluctuates with lower standard deviation. Third, for a comparison to be fair, all data must be in the same units. If you're measuring in dollars for one data set, but euros for another, it's not fair to compare these data sets to determine which has the greatest spread. Finally, the variance has squared units of your original measurements. For example, if you were measuring revenue in dollars, the variance has units of dollars squared, which isn't particularly useful. For this reason, the standard deviation, which is the square root of the variance, is often deemed a more useful measurement of spread as it shares the units of the original data set. If you measure revenue in dollars, the standard deviation also has units of dollars.

### 17. Shape of Distributions-UnN99AAYf8k.en

Now that we've discussed how to build a histogram, we can use this to determine the shape associated with our data. Here we have three histograms, showing the shape for three different data sets. The histogram that has shorter bins on the left and taller bins on the right, is considered a left skewed shape. This histogram that has shorter bins on the right and taller bins on the left, is considered a right skewed shape. Any distribution where you can draw a line down the middle and the right side mirrors the left side is considered symmetric. One of the most common symmetric distributions, is known as a normal distribution and it's also called the Bell Curve. The shape of the distribution can actually tell us a lot about the measures of center and spread. Symmetric distributions, like this one have a mean that's equal to the median, which also equals the mode. Each of these measures sits here in the center. The mode is essentially the tallest bar in our histogram. When we have skewed distributions, it's the case of the mean is pulled by the tail of the distribution, while the median stays closer to the mode. For example, in this right skewed distribution, the mean would be pulled higher, resulting in a mean that's greater than our median. Alternatively, in a left skewed distribution, our mean is pulled down here, resulting in a mean that's less than our median. In order to relate this to the visual of a histogram, back to the five number summary we saw on the earlier lessons, here are the corresponding box plots below each histogram. Notice how the whiskers stretch in the direction of the skew, for each of the skewed distributions. That is the longer whisker is on the left, for a left skewed distribution and it's on the right, for the right skewed distribution. Alternatively, the symmetric histogram also has a symmetric box spot.

### 18. Data in the Real World-HmipezTjTDY.en

If you're working with data, you can always build a Quick Plot to see the shape. Just to apply some context, some examples of approximately Bell-Shaped data include heights and weights, standardized test scores, precipitation amounts, the mean of a distribution, or errors in manufacturing processes. Common data that follow Left Skewed Distributions include GPAs, the age of death, and asset price changes. Common data that follow approximately Right Skewed Distributions include the amount of drug left in your bloodstream over time, the distribution of wealth, and human athletic abilities. There are links below in the instructor notes in case you want to learn more about each of these cases. Though these three, Right Skewed, Left Skewed and Symmetric, are the most common distributions, data in the real world can be messy and it might not follow any of these distributions. We will talk about this more in the next section.

### 20. Outliers-HKIsvkZUZfo.en

In this video, we want to look at the final aspect used to describe quantitative variables, Outliers. Outliers are data points that fall very far from the rest of the values in our data set. In order to determine what is very far, there are a number of different methods. My usual method for detecting outliers isn't very scientific. Usually, I just look at a histogram and see if the point is really far from any of the other data points. Again, a quick plot of your data can often help you understand a lot in a short amount of time. In order to illustrate the impact that outliers can have on the way we report summary statistics, let's consider the salaries of entrepreneurs. Imagine I select ten entrepreneur earnings and I pull these nine values here as earnings in thousands of dollars, and the tenth is the CEO of Facebook. According to a post by CNN in 2016, he earned 4.4 million dollars per day. Here, we can calculate the mean of these salaries for entrepreneurs based on this data to be approximately 160 million dollars. This is incredibly misleading. Literally zero of the entrepreneurs earned this salary. None of the ten salaries are even close to this amount. A better measure of center would certainly be the median. The median here is calculated at 62,000 dollars a year and is a better indication of what an entrepreneur is likely to earn based on our data. Our standard deviation is also not a great measure in this case. At approximately 482 million dollars, all this suggests is that our earnings for entrepreneurs are really spread out, but that really isn't fair either. Just one point is really far from the rest. Like really really far.

### 21. Working with Outliers-4RnQjtJB8t8.en

How should we work with these outliers in practice? At the very least, we should note that they exist. We need to realize the impact they have on our summary statistics. In this case, greatly increasing our mean and our standard deviation. If the outliers are typos or data entry errors, this is a reason to remove these points. Or if we know what they should be, we could change them with the correct values. In cases like the example above, we might try to understand, what was so different about the outlier when compared to the other individuals? How did this entrepreneur become so successful? And why are the earnings so large in comparison? There is an entire field aimed at this idea called the anomaly detection. You will notice measures associated with the five number summary, like the minimum, the maximum and the quartiles one, two, three, are more informative when outliers are present. This example shows that you need to be careful about how we share our results and state our conclusions using summary statistics when we have outliers. A single number can be very misleading about what is actually happening in our data. Some statistics are more misleading than others. If you are the consumer of information based on data, which we all are, it's important to know how to ask the right questions regarding the statistics around you.

### 22. Outliers Advice-BhhDoTgYQmI.en

If you're the one doing the reporting, here are some of my personal guidelines when analyzing data. First, plot your data. Second, if you have outliers, determine how you should handle them. This might require a domain expert of the field. Should you remove them? Should you fix them? Should you keep them? Third, if you're working with data that are normally distributed, the bell shape that we saw before, you can find out every little detail about the data with only the mean and the standard deviation. This may seem surprising but it's true. However, if our data are skewed, the five-number summary provides much more information for these data sets than the mean and the standard deviation can provide. Again, the most useful and informative summary you can get is frequently a visual. In upcoming lessons, we will focus specifically on the visuals that will best convey our message.

### 27. Descriptive vs. Inferential Statistics-XV9pd8-RZ78.en

The topics covered this far have all been aimed at descriptive statistics. That is, describing the data we've collected. There's an entire other field of statistics known as inferential statistics that's aimed at drawing conclusions about a population of individuals based only on a sample of individuals from that population. Imagine I want to understand what proportion of all Udacity students drink coffee. We know you're busy, and in order to get projects in on time, we assume you almost drink a ton of coffee. I send out an email to all Udacity alumni and current students asking the question, do you drink coffee? For purposes of this exercise, let's say the list contained 100,000 emails. Unfortunately, not everyone responds to my email blast. Some of the emails don't even go through. Therefore, I only receive 5,000 responses. I find that 73% of the individuals that responded to my email blast, say they do drink coffee. Descriptive statistics is about describing the data we have. That is, any information we have and share regarding the 5,000 responses is descriptive. Inferential statistics is about drawing conclusions regarding the coffee drinking habits of all Udacity students, only using the data from the 5,000 responses. Therefore, inferential statistics in our example is all about drawing conclusions regarding all 100,000 Udacity students using only the 5,000 responses from our sample. The general language associated with this scenario is as shown here. We have a population which is our entire group of interest. In our case, the 100,000 students. We collect a subset from this population which we call a sample. In our case, the 5,000 students. Any numeric summary calculated from the sample is called a statistic. In our case, the 73% of the 5,000 that drink coffee. This 73% is the statistic. A numeric summary of the population is known as a parameter. In our case, we don't know this value as it's a number that requires information from all Udacity students. Drawing conclusions regarding a parameter based on our statistics is known as inference.

### 31. Descriptive Statistics Summary-Fe7Gta2SfLA.en

Congratulations on making it through the statistics portion of this program. This foundation in working with data will make later sections using Spreadsheets, SQL and Tableau more intuitive. I hope this section reinforce some ideas you're already familiar with, while also introducing you to some new ones that you've now mastered.



## Part 06-Module 01-Lesson 03_Admissions Case Study

### 01. Admissions Case Study Introduction-FGbxq1hQgtk.en

Welcome to the first lesson in the practical statistics course. In this case study, you're going to witness an instance of Simpson's paradox. A phenomenon that shows how powerful and dangerous statistics can be. Sometimes just grouping your data differently for your analysis can make your conclusions disappear or even be reversed. Let's walk through an interesting case study that demonstrates this.

### 02. Admissions 1-CLgVLQAEYw8.en

The problem I'd like to tell you about is motivated by an actual study the University of California Berkeley, which many years back wanted to know whether it's admissions procedure is gender biased. I looked at various admission statistics to understand whether than admissions policies had a preference for a certain gender. And while the numbers I'll be giving you are not the exact same that UC Berkeley found, the paradox is indeed the same and is often called, "Simpson's Paradox." I'm just giving you a simplified version of the problem. Here is the data. Among male students, we find that from 900 applicants in major A 450 are admitted. Please tell me what the acceptance rate is in percent.

### 02. Admissions 1-f3y_weFskL4.en

Obviously, it's 50%.

### 03. Admissions 2-o91iPvtqt78.en

And the answer is 10%.

### 03. Admissions 2-pJrwiukN3Ls.en

In a second major B 100 students applied, of which 10 were admitted. What is the acceptance rate?

### 04. Admissions 3-iKTYAsZLbhc.en

The same statistic was run for female students. Again, I made up the data to illustrate the effect. Females tended to apply predominantly for major B with 900 applications for major B and just 100 for major A. The university accepted 80 out of 100 applications in major A and 180 out of 900 in major B. Please tell me the rate of acceptance in percent for major A for the females student population.

### 04. Admissions 3-rDw0TIpwJ-c.en

Of course it's 80%--80/100.

### 05. Admissions 4--GMhV1twy6Y.en

Please do the same for the major B in the female population over here.

### 05. Admissions 4-GD6cQhkoqS4.en

180 is 20% of 900.

### 06. Gender Bias-DeWp0hnRq4g.en

So, just looking at these numbers for the two different majors, would we believe--in terms of the acceptance rate-- is there a gender bias?  Yes or no?

### 06. Gender Bias-JWl8lPGhlbY.en

And I would say yes, in part because the acceptance rate is so different for the different student populations, even though  the numbers are relatively large. So, it doesn't seem just like random deviations. But the thing that will blow your mind away is a different question.

### 07. Aggregation-55eZrE82TqA.en

Who is being favored--the male students or the female students? And looking at the data alone, it makes sense to say the female students are favored because for both majors, they have a better admission rate than the corresponding male students. But now, let's do the trick. Let's look at the admission statistics independent of the major. So, let's talk about both majors, and I would wonder how many male students applied. And of course, the answer is 1000. How many were admitted?

### 07. Aggregation-8j5hria6Rc8.en

And the answer is 460.

### 08. Aggregation 2-udXhxyls5Dw.en

And the answer is, of course, 46%. It's 460/1000 x 100%.

### 08. Aggregation 2-xhpEqsHTf3g.en

So, what is the admissions rate for male students across both majors in percent?

### 09. Aggregation 3-YkaVgZ-yFrM.en

Now, do the same for the female student population, and we had a 1000 applicants, same number as in the male case, and 260 students admitted. So, what's the percentage rate for admission?

### 09. Aggregation 3-tPSj6_m-0_M.en

The answer is 26%.

### 10. Gender Bias Revisited-4YY-hmqSz30.en

So, across both majors, I'm asking you the same question again now. Who is actually being favored? Males or females?

### 10. Gender Bias Revisited-dOa4Cl0wM0s.en

And surprisingly, when  you look at both majors together, you find that males have a much higher admissions rate than females. I'm not making this up. These numbers might be fake, but that specific affect was observed at the University of California at Berkley many years ago. But when you look at majors individually, then you find in each major individually the acceptance rate for females trumps that of males, both in the first major and the second major. Going from the individual major statistics to the total statistics, we haven't added anything. We just regrouped the data. So how come, when you do this, what looks like an admissions bias in favor of females switches into admissions bias in favor of males?

### 11. Dangers Of Statistics-UYZXqP562qg.en

As you've seen in this example, on Simpson's paradox, the way you choose to look at your data can lead to completely different results. And often, you can majorly impact what people believe to be true with how you choose to communicate your findings. You can guess how people intentionally or unintentionally come to false conclusions with these choices. Next, you're going to walk through a similar example of Simpson's paradox on a full dataset in a Jupyter notebook.

### 14. Conclusion-XiR_37bYA84.en

I hope this example made you think and learn to be skeptical, of your own results and the results from others. Moving forward even when you feel very confident about the statistics you use for your analysis, take a moment to reconsider other ways of looking at your data and whether you chose wisely. Stay tuned as we dive into the basics of statistics. We'll begin with probability theory.



## Part 06-Module 01-Lesson 04_Probability

### 01. Introduction to Probability-HeoQccoqfTk.en

Statistics and probability are different but strongly related fields of mathematics. In probability, we make predictions about future events based on models or causes that we assume whereas in statistics we analyze the data from the past events to infer what those models or causes could be. There's almost an opposite relation between these two. In one, you're predicting data and in the other, you're using data to predict. Although not all topics and both fields require an understanding of the other, you'll need a good understanding in probability for the foundation you'll be building in statistics. With that in mind, let's get started with the basics of probability.

### 02. Flipping Coins-OpNufHYgJCg.en

I have here a U.S. dollar coin. It has two sides, one showing a head and one showing what's called tails. In probability, I'm giving a description of this coin, and I'm making data. We just make data. [sound of coin spinning] So if we look at the coin, it came up heads. So I just made a data point of flipping the coin once, and it came up heads. Let me do it again. [sound of coin spinning] And--wow! It came up heads again. So my new data is {heads, heads}. And you can see how it relates to the data we studied before when we talked about histograms and pie charts and so on. Let me give it a third try. [sound of coin spinning] And, unbelievably, it comes up once again heads. So let me ask a statistical question to test your intuition. Do you think if I twist this coin more frequently will it always come up heads? And say I try to twist it as fairly as I possibly can.

### 02. Flipping Coins-lgUDXtUyLLg.en

And you can debate it, but I think the best answer is no. This is what's called a fair coin, and that means it really has a 50% chance of coming up tails. So let me spin it again. [sound of coin spinning] And, not surprisingly, it actually came up tails this time. So probability is a method of describing the anticipated outcome of these coin flips.

### 03. Fair Coin-9LrlrexpW_o.en

Let's talk about a fair coin. The probability of the coin coming up heads is written in this P notation. This reads probability of the coin coming up heads. And in a fair coin, the chances are 50%. That is, in half the coin flips, the coin should come up heads. In probability we often write 0.5, which is half of 1. So a probability of 1 means it always occurs. A probability of 0.5 means it occurs half the time. And let me just ask you what do you think, for this coin, is the probability of tails?

### 03. Fair Coin-fSKL742j-zk.en

And I would say the answer is 0.5. Let me now go to a coin that is what is called "loaded."

### 04. Loaded Coin 1-T0EjWSjLGjQ.en

A loaded coin is one that comes up with one of the two much more frequently than the other. So, for example, suppose I have a coin that always comes up heads. What probability would I assess for this coin to come up heads? What would be the right number over here?

### 04. Loaded Coin 1-sNvQeSikRFY.en

And the number is 1. That's the same as 100%. 1 just means it always comes up in heads.

### 05. Loaded Coin 2-Y7tnbth-gag.en

And, given that, what number would you now assess the probability of tails to be?

### 05. Loaded Coin 2-dGffszQYzqc.en

And, yes, the answer is zero. And we find a little law here we just want to point out, which is the probability of heads plus the probability of tails equals 1. And the reason why that's the case is the coin either comes up heads or tails. There is no other a choice. So no matter what happens, if I look at heads and tails combined the chances of either of those occurring is 1, because we know it's going to happen. So we can use this law to compute the probability of tails for other examples.

### 06. Loaded Coin 3-HohMRlmHoMQ.en

And the answer is 0.25, which is 1 - 0.75 using the law down here. As you can verify, 0.75 + 0.25 =1.

### 06. Loaded Coin 3-P4uljJ_OP6I.en

So suppose the probability of heads is 0.75, that is, 3 out of 4 times we're going to get heads. What is the probability of tails?

### 07. Complementary Outcomes-YseJqD-1oUg.en

So we just learned something important. There's a probability for an outcome; I'm going to call it A, for now. And we learned that the probability of the opposite outcome, which we're going to call A (this over here just means "not") is 1 minus the probability as expressed right over here. That's a very basic law of probability, which will become handy as we go forward, so please remember it.

### 08. Two Flips 1-1txkcmxk3vU.en

That was a tricky question, and you couldn't really know the answer if you've never seen probability before, but the answer is 0.25. And I will derive it for you using something called a truth table. In a truth table, you draw out every possible outcome of the experiment that you conducted. There were two coin flips--flip 1 and flip 2--and each had a possible outcome of heads, heads; heads, tails; tail, heads; and tail, tail. So when we look at this table, you can see every possible outcome of these two coin flips. There happens to be four of them. And I would argue because heads and tails are equally likely, each of these outcomes is equally likely. Because we know that the probability of all outcomes has to add up to 1, we find that each outcome has a chance of a quarter, or, 0.25. Another way to look at this is the probability of heads followed by heads is the product . What are the chances of the first outcome to be heads multiplied by the probability of the second outcome to be heads? The first is 0.5, as is the second. And if you multiply these two numbers, it's 0.25, or, a quarter.

### 08. Two Flips 1-yUIz7SgUwJg.en

In our example, we observed heads twice. So now I want to ask you a really tricky question: What's the probability of observing heads and heads if you flip the same unbiased coin twice? This means in each flip we assume the probability of heads is 0.5. Please answer here.

### 09. Two Flips 2-pT0FXiH_5nI.en

And the answer is 0.4 because heads comes up 0.6, and 1 - 0.6 = 0.4.

### 09. Two Flips 2-uhrL5fatt3E.en

Let me now challenge you and give you a loaded coin I flipped twice. And for this loaded coin, I assumed the probability of heads is 0.6. That really changes all of the numbers in the table so far, but you can apply the same method of truth tables to arrive at an answer for what is the probability of seeing heads twice under the assumption that the probability of heads equals 0.6? And I want to do this in steps, so rather than asking the question directly, let me help you derive it by first asking: What's the probability of tails?

### 10. Two Flips 3-3NSPqjp6pFY.en

And the answer using our product rule is heads, heads comes out to 0.6  0.6, which is 0.36. Heads followed by tails is 0.6  0.4, which is 0.24. Tails followed by heads is, again, 0.24. And tails followed by tails is 0.16, which is 0.4  0.4.

### 10. Two Flips 3-uimwo-puQWY.en

And now please fill out the entire truth table. There are four values over here, so please compute them for me.

### 11. Two Flips 4-bNoS6LQEFrI.en

And, not surprisingly, it's 1. That is, the truth table always has a probability that adds up to 1 because it considers all possible cases, and all possible cases together have a probability of 1. So we just check this and make sure it's correct. Reading from this table, we find that the probability of (H,H) is 0.36. And you can do the same over here. 0.6  0.6 = 0.36 So that's our correct answer.

### 11. Two Flips 4-rRPwknIDuI0.en

If you add up these numbers over here-- please go ahead and add them up and tell me what the sum of those numbers is.

### 12. Two Flips 5-G28YyiGFGWA.en

Let's now go to the extreme, and this is a challenging probability question. Suppose the probability of heads is 1, so my coin always comes up with heads. What is the probability of (H,H)?

### 12. Two Flips 5-HB8b7sZQFGs.en

And the answer is 1. To see this, we know that the probability of tails is 0. All the probability goes to heads. 1  1 = 1 1  0 = 0 0  1 = 0 And 0  0 = 0. And it's easy to verify that all these things add up to 1. Our (H,H) is just 1.

### 13. One Head 1-T4A5uyqesjo.en

And the answer shall be 0.5. And this is a nontrivial question. Let's do the truth table. So, for flip-1, we have the outcomes of heads, heads, tails, tails. For flip-2, heads and tails and heads and tails. These are all possible outcomes. And we know for the fair coin each outcome was equally likely. That is, exactly one quarter.

### 13. One Head 1-lHuZpDkfwq8.en

The truth table gets more interesting when we ask different questions. Suppose we flip our coin twice. What we care about is that exactly one of the two things is heads, and thereby exactly the other one is tails. For a fair coin, what do you think the probability would be that if I flip it twice we would see heads exactly once?

### 14. One Head 2-64EjAbqrtmo.en

And, yes, it's in the second case and in the third case. The extreme cases of heads, heads and tails, tails don't satisfy this condition. So the trick now has been to take the 0.25 probability of these two cases and add them up, which gives us 0.25 + 0.25 = 0.5. This is the number which is correct for this inquiry.

### 14. One Head 2-JHx3ucNS9f4.en

Given that, we now have to associate a truth table with the question we're asking. So where exactly is, in the outcome, heads represented once? Please check the corresponding cases.

### 15. One Of Three 1-bDCXSxkochE.en

And this answer is tricky. We will derive it through the truth table. Now there's eight possible cases. Flip one can come of heads or tail; same for flip two, heads, tail, heads, tail; and the same for flip three and if you look at this every possible combination is represented. For example, these are heads, tail, tail. Now each of those outcomes has the same probability of an eighth, because it's eight cases. So 8 x 1/8 sums up to 1. In how many cases do we have exactly one H? It turns out that it's true for only three cases. The H could be in the first position, in the second position, or in the third position. So three out of eight cases have a single H. Each of those carries a probability so we sum those cases up to carry a total of 3/8 of a probability. These are the same as 0.375.

### 15. One Of Three 1-rxfHfjy9Mm4.en

Let me now make it really, really challenging for you. I take a fair coin and flip it 3 times, and I want to know the probability that exactly 1 of those 3 flips comes up heads.

### 16. One Of Three 2-27Ed1GI4j84.en

And my answer is 0.288. How do I get that? Let's look at the three critical cases. H T T is 0.6 for H times 0.4 for tails times another 0.4 for tails again and it gives me 0.096. Now it turns out this case over here has the same probability because all we do is we order it 0.4 x 0.6 x 0.4 and we know that in multiplication the order doesn't matter, so you get the same 0.096, and by the same logic, if third one also gets me 0.096. So adding this 0.096s together, if we get them, gives me 0.288. So I did not have to fill the entire truth table, which you might have done in the derivation. I only have to fill out the cases I care about, yet they give me their correct result.

### 16. One Of Three 2-gGgqTGZ9TKg.en

Now that was a challenging question. I'm going to make it even more challenging for you now. I'll give you a loaded coin--the probability for H is 0.6. I expect this will take you awhile on a piece of paper to really calculate this probability over here. But you can do exactly the same thing. You go through the truth table. You apply the multiplication I showed you before to calculate the probability of each outcome; they're not the same anymore. H, H, H is clearly more likely than T, T, T. And when you've done this, add the corresponding figures up, and tell me what the answer is.

### 17. Even Roll-DrnAR4SqlEE.en

So let's do one final exercise. Now I am throwing dice. The difference between dice and coins is that there are now 6 possible outcomes. Let me just draw them, and say it's a fair die, which means each of the different sides comes up with a probability over 6 for any of the numbers you can plug in over here. What do you think the probability is the die comes up with an even number? I'm going to write this as the outcome of the die is even. And you can once again use a truth table to calculate that number.

### 17. Even Roll-M3L0a5V4Nf0.en

In truth table-speak, there are 6 outcomes, 1 to 6. Each has the same probability over six. Half of those numbers are even--2, 4,  and 6, so if we add those up, we get 3  1/6--the same as a half. The outcomes is 0.5. Now I'm finally going to make, as my final quiz, a really challenging question for you.

### 18. Doubles-On_Guw8wac8.en

Suppose we throw a fair die twice. What do you think the probability of a double is? Double means both outcomes are identical with the same number regardless of what that number is. The actually an important number because in many games involving two dice, have different rules when these come up with the same number. So, it might be important to know what the probability is.

### 18. Doubles-fkUyTJNbdzU.en

And once again, we can answer this using a truth table. Now the truth table will have 36 different entries, six for the first throw times six for the second throw, and there isn't enough space on this tablet to draw all the 36 entries. So, let me just draw the ones that really matter, one-one, two-two, and so on all the way to a six-six. So, each one of those is a probability of 1/6 for the first outcome times 1/6 for the second, which gives me 1/36, and the same logic applies everywhere. So, for all of these six outcomes, I have 1/36 of a chance this outcome would materialize. Adding them all up gives me 1/6, why? Because, I get 6 times 36 and I can simply this back to 1/6 that's just the same as 0.16667. So, 1/6 times, you will get a double Now, when you're play a game like backgammon, which is played with two dice, it might not feel like this, I can swear I don't get a double of 1/6 moves, but it's actually true that that's the right--that's the correct probability.

### 19. Probability Conclusion-dsVKoXymYDU.en

You now have a basic understanding of probability. Great job! Let's quickly summarize what was covered in this lesson. You learned about the probability of an event. Such as the outcome of a coin flip. You learned that the probability of the opposite event is one minus the probability of this event. And you learned about the probability of a composite event, which was in the form of P times P times P and so on. This rule is true because the events we've observed so far are independent of one another, which means the outcome of one event doesn't affect the outcome of another, which is also the case in the next lesson on binomial distributions. However, if we're looking at the probability of dependent events, say the probability of rain tomorrow given that it rained today, this rule doesn't hold. Dependent events where the outcomes of later events depend on what has already happened are at the heart of the following topics, unconditional probability and Bayes Rule.



## Part 06-Module 01-Lesson 05_Binomial Distribution

### 01. Binomial-3koDdc9r73E.en

So we talked about coin flips, and we flipped some coins. Now, I want to flip many coins including this 2-dollar coin whose country of origin I just don't remember. Perhaps you can post on the forum where this 2-dollar coin might be from. So let's ask an easy quiz. Suppose we do 2 coin flips. I would like to know how many outcomes of these 2 coin flips are there and which number of heads equals the number of tails which in this example means you would have head exactly once and tails exactly once. Give me that number.

### 01. Binomial-x1yamZeOMPY.en

And the answer is 2. If you look at the truth table head-head, head-tail, tail-head, and tail-tailthese are the four possible outcomes. Those two outcomes over here yield an equal number of heads and tails.

### 02. Heads Tails-iyX0-eXStbw.en

In going through the truth table, there's a bit more info. So let's find the one where the number of heads and tails are the same. Those three on the left side and those three on the right side for a total of 6.

### 02. Heads Tails-yo55zJtJQwo.en

Let's now go to 4 coins and ask the same question.

### 03. Heads Tails 2-S87Z5DgPJeo.en

Now. Let's go to 5 coin flips. How many outcomes have the same number of heads and tails? This is a trick question.

### 03. Heads Tails 2-vLhdJtXx060.en

And the answer is 0. With an odd number of coin flips, one has to be more than the other. There's no other way. Okay. I tricked you a little bit.

### 04. 5 Flips 1 Head-4LVRNqpdxsw.en

The answer is 5. There's 5 different ways in these 5 outcomes. To place heads--could be first, second, third, fourth or fifth. So these are 5 different ways.

### 04. 5 Flips 1 Head-VEfOdACY9rA.en

With 5 coin flips, how many outcomes will have exactly 1 heads, hence 4 tails.

### 05. 5 Flips 2 Heads-69je8wHh2mQ.en

And the answer is 10 and this is a non-trivial answer. So you could go and place the first heads anywhere in these five elements--say here-- and there's five different ways to place heads. You can now place the second heads among the remaining four--for example, you could place it over here and it gives you a factor of four different ways of placing the second heads. But when you do this, you over count--you over count exactly by a factor of 2 in the business. You place the first heads over here, the second over here, but if it was chosen to place the first head on the right side and the second head over here on the left side, and the outcome would've been exactly the same, so you counted the one twice which means we have to divide it by 2--5 times 4/2 is 10

### 05. 5 Flips 2 Heads-lhhUjxnbad8.en

Let's now make you think really hard. In 5 coin flips, how many outcomes will you have 2 heads. This is a serious and non-trivial question.

### 06. 5 Flips 3 Heads-1PHs2w_NNTg.en

Let's now go to 3 heads if you think the result is no.

### 06. 5 Flips 3 Heads-pOKmt4w8T3g.en

And that log is 10 again. There's two proofs. One is I can just flip heads and tails. So three heads means two tails. I can do the exact same game as before where I placed tails as opposed to heads and it gives me the same equation as before, but let's do it the new way, three heads. I can place 5*4*3--the first heads, the second and the third.** For the first, I have five positions, for the second--four, and for the third--three are left. This gives me the combinatorics for those heads, but now I'm over counting. How much am I over counting? Well, suppose I'm committed to put the three heads into the three slots over here and that's not given. And I just wonder in which order I've put them in, so I might put the first one here, the first one here, the first one here. Then for the first one placed in here, there's now three different ways of placing it. For the second one, there's two different ways of placing it. For the third one, it's not deterministic--there's just one slot left. So I over count this by a factor of 6--there are 6 different ways of placing these three heads into these three slots, so the result is 543/321 producing the 5*2=10. And that is insightful.

### 07. 10 Flips 5 Heads-Qm4KTLfFMzo.en

I was given 10*9*8*7*6 which is 30,240 with a pick of those five heads and we're over counting by 5*4*3*2*1 which is 120 and this gives us as a quotient 252.

### 07. 10 Flips 5 Heads-mOPFQlKBg2M.en

Let's say I have 10 coins. You just get 10 of these shiny silver coins here. So we got 10 coins. And I want about 4 heads in these 10 coins, and just apply the same logic to find an answer. The first time I'll do it for you. I can place those heads in 1098*7 different slots if they were counted. Now that I'm committed to having chosen these slots, the implementations of those are 432*1. That gives me 5,040/24, also known as 210, so that's 210 outcomes out of 2 to 10th outcomes which is 1024 in which exactly 4 heads are observed and 5 tails. Now it's your turn. 5 heads out of 10 coins. One of them coming up to heads.

### 08. Formula-DTdS-LlMTQ0.en

So let me give you the mathematical formula that you might be familiar with. n! for any number n is the same as n(n-1)(n-2) all the way to times 1. Let's call it factorial. So 10 factorial for example will be 109876 and so on. If you look at this equation over here, I'll give you a couple of choices how to write it. It could be n!; it could be n!/k! where this over here is k which is 5 and this over here is n. It could be n!/k!*k! or it could be n!/k!*(n-k)! These are four choices. One of this is actually correct for the formula that we've computed before. Pick the correct one.

### 08. Formula-yTr8zCHdo5M.en

And this one is the correct one and I have to admit I made a mistake a little bit by taking a symmetric example where k is exactly half of n.

### 09. Arrangements-GeINbOOYkF8.en

The key observation is that this thing over here is n!/(n-k)! and to see this let's go back to the case where we had 4 heads. In the 4 heads case, we multiplied all the way to times 5 over here. We only multiplied all the way to 7. These are the 4 heads that we placed. So n! goes from 10 all the way to 1. (n-k)! goes from 10-4 and that's 6 all the way to 1. So this blue expression over here will go from 10 x 9 x 8 and so on all the way to 1 over now n-k, 10-4 is 6. 6*5*4 and so on. When you look at this, the 6*5*4 occurs on top as well. So we can cut those out and what remains is 10*9*8*7. This one over here. Now once we place these 4 heads, we have to divide by a 4 factorial which is different ways of placing those 4 points into the predefined bins. So put differently, this is k! so if we put all this together you get n!/(n-k)!*k! This is the expression over here. Let's practice this one last time. Say you have 125 coins and you ask how many ways exists in which 3 coins come up heads. What is the resulting outcome? Be careful when you use your calculator. It might result in an overflow, but the answer is easy to compute.

### 09. Arrangements-NRPcnpmFCg8.en

So, I get 317,750, and the logic is I just plug in these numbers, 125!/122! 3!. When I evaluate this, I find that these guys can be easily reduced to 125*124*123. From 122 on, these factors exactly cancel. 3 expands to 3*2*1 also known as 6. When you divide these two things, you get 317,750.

### 10. Binomial 1-07vOaYwecII.en

Now, we know from our previous consideration that there are five ways in which the number of heads could be one. 5!/4! 1! happens to be 5. We also know that there are 32 possible outcomes. It is 2=32 outcomes. This is the size of a truth table. So, 5 out of 32 outcomes has exactly 1 head. So, I would suggest that the answer is 5/32, and my calculator tells me this is 0.15625. So, that is actually interesting. If you flip a coin five times, there's a chance that it only comes at head exactly once and that is the probability 0.15625.

### 10. Binomial 1-RBfFHxEjsIU.en

And let's really ask about probabilities. Let's say for now, we have a fair coin with a probability of heads is 0.5. If I flip a coin five times, what's the probability the number of heads is exactly 1. You should be able to compute this.

### 11. Binomial 2-Uy7b3aMPnEY.en

And the answer is 5!/5-3 is 2!/3! and that gives us 10. 10/32 gives us the probability of 0.3125.

### 11. Binomial 2-d4LWnxyvrTQ.en

Let's now modify it and ask, what are the chances it becomes a head three times? What's the probability for that to happen?

### 12. Binomial 3-Jp2xJOtNQZ0.en

Now, I'm going to make it really difficult. I'm going to give you a coin--let's call it loaded. So, the probability for heads will now be 0.8 and therefore the probability for tails is 0.2. To make it easier, assume only a 3 coin flips and ask the probability of heads coming up exactly once. What is that probability for the loaded coin that I gave you here. I recommend answering it using a truth table.

### 12. Binomial 3-YIELbuet-ZE.en

So here is my truth table of these eight different outcomes. The ones that has head exactly once are this one, this one, and this one, but they're not all equally likely. Heads, heads, heads is much more likely than say tails, tails, tails because heads has a probability of 0.8^3. This one here has an outcome probability of 0.8 while this one has an outcome probability of 0.2^3. All of the green ones have the same outcome probability. They all have exactly one head 0.8 and two tails. So as before, we took the probability to be one of these heads in the truth table. This time each of those has a probability of 0.032. That's each one of the three. Now, we have to consider all of these three outcomes, which means you're going to add 0.032 for each one of those three guys over here and this gives me as an answer 0.096. That's a nontrivial question.

### 13. Binomial 4-lPrKmvckG4E.en

So interestingly enough, we can do the trick as before 5!/4!*1!, which is the number of outcomes at exactly 4 heads and we know that's 5. Four heads means one tails. There's five ways to place one tails and this one over here. The question now what's the probability of those? Well, they have heads four times, (0.8) and tail once to (0.2). So we do is we multiply the total number of outcomes that have this property with each one probability, which happens to be the same because we get exactly four times heads and 1 times tail and multiplying these things together gives us 0.4096. This over here is indeed 0.4096 and the 5*0.2 cancels the others out, so that's the result in this specific case. The cancellation doesn't always happen. It's a rare circumstance.

### 13. Binomial 4-mvJUNYfHngY.en

Let's now say for the same loaded coin, we flip that coin five times with that same load of probability over here and we care about the out of five times heads comes exactly four times. Now, I want you to compute this probability over here, It's a completely nontrivial question. If it's too hard, hit the next button.

### 14. Binomial 5-8jcCGD986jk.en

And here is my answer 5!/3!*2! is 10, and we have 3 heads, so we put 3 in here and 2 tails, so we put 2 in here. Putting these all together gives us 0.2048, which is half the probability of the previous question.

### 14. Binomial 5-yof0QiP2mzk.en

So, this does not go and get the same for 3 heads. I leave this here, but obviously, the numbers aren't correct anymore.

### 15. Binomial 6-CQHRYIU6v9Q.en

Now, you're ready for the real challenge. You flip the coin 12 times in the care about how likely it is to get heads 9 times out of the 12. This is not a trivia question, but you should be able to get it right.

### 15. Binomial 6-n_OrWrZ8tKY.en

And the approximate answer is 0.236. That's the probability of exactly 9 heads out of 12 coin flips for this heavily loaded coin that mostly gives you heads. And again, the answer is 12!/12-9. This is 3! 9!. Then we have to compute the probabilities being (0.8) and 1-0.8 is 0.2 and that is the number over here.

### 16. Binomial Distribution Conclusion-9gjCYs8f_PU.en

In this lesson, we found the probability that a coin would land on heads k times out of n flips. If the probability of heads for the coin is p, we get the following formula for the probability that the number of heads will be k. The first half is n factorial over n minus k factorial times k factorial. This part keeps track of the total number of ways we can get k heads and n total flips of the coin. For example, if we were looking at three heads and four flips, we could get this set of flips, or this set, or this set, or this set. There are four possible ways we can get three heads out of four flips. This part of the formula keeps track of how many possible ways we can get these k heads and n flips. In this case four. Each one of these ways have the following probability of event occurring. P to the k times 1 minus p to the n minus k. The p to the k keeps track of the probability associated with the number of heads. And this part keeps track of the probability associated with the number of tails. Altogether, this formula gives the probability of what's called the binomial distribution. You've now seen how you can take very large experiments with many coin flips and compute the probability that the coin will land heads a certain number of times with this formula. Although the examples in this lesson use coin flips, you can really perform these calculations with any events that have two outcomes. Does a customer buy or not? Is a transaction fraud or not? Or if a coin flip is heads or tails. Binomial distributions are used to give us insight about all of these. Later in the course, you will explore large experiments again using the normal distribution. But first, let's move on to conditional probability where events are no longer independent. Let's explore cases where the outcome of an event can affect the outcome of the events that follow.



## Part 06-Module 01-Lesson 06_Conditional Probability

### 01. Introduction to Conditional Probability-Ok8948Wcbmo.en

In real life, events often depend on each other. Say you can be an early bird or night owl. And for the sake of simplicity, let's assume there's a 50 percent chance for each one. Now whether you decide to go for a run at 5:00 a.m. tomorrow is not entirely independent. I would argue that going for a run at 5:00 a.m. is not likely for most people. But it is certainly more likely for individuals who are actually awake at that hour. If you're an early bird, the probability might be a two percent chance, while if you're a night owl, the probability might be zero. You can think of these events as two consecutive coin flips. The first coin determines whether you're an early bird, and the second coin is only flipped given that you are an early bird, and it determines if you go for the run. We are no longer observing independent events, like we did in the last lesson, where the outcome of the first coin flip didn't affect the outcome of the second. Starting with this lesson, we're going to look at case studies where the outcome of the first does impact the outcome of the second.

### 02. Medical Example 1-E1ph6NP3_v4.en

And the answer is 0.9 with just 1 minus the cancer.

### 02. Medical Example 1-mFfbts1lAEo.en

To do so, let's study a medical example--supposed there's a patient in the hospital who might suffer from a medical condition like cancer. Let's say the probability of having this cancer is 0.1. That means you can tell me what's the probability of being cancer free.

### 03. Medical Example 2-FV_hc3MzS_8.en

Of course, in reality, we don't know whether a person suffers cancer, but we can run a test like a blood test. The outcome of it blood test may be positive or negative, but like any good test, it tells me something about the thing I really care about--whether the person has cancer or not. Let's say, if the person has the cancer, the test comes up positive with the probability of 0.9, and that implies if the person has cancer, the negative outcome will have 0.1 probability and that's because these two things have to add to 1. I've just given you a fairly complicated notation that says the outcome of the test depends on whether the person has cancer or not. That is more complicated than everything else we've talked about so far. We call this thing over here a conditional probability, and the way to understand this is a very funny notation. There's a bar in the middle, and the bar says what's the probability of the stuff on the left given that we assume the stuff on the right is actually the case. Now, in reality, we don't know whether the person has cancer or not, and in a later unit, we're going to reason about whether the person has cancer given a certain data set, but for now, we assume we have god-like capabilities. We can tell with absolute certainty that the person has cancer, and we can determine what the outcome of the test is. This is a test that isn't exactly deterministic--it makes mistakes, but it only makes a mistake in 10% of the cases, as illustrated by the 0.1 down here. Now, it turns out, I haven't fully specified the test. The same  test might also be applied to a situation where the person does not have cancer. So this little thing over here is my shortcut of not having cancer. And now, let me say the probability of the test giving me a positive results--a false positive result when there's no cancer is 0.2. You can now tell me what's the probability of a negative outcome in case we know for a fact the person doesn't have cancer, so please tell me.

### 03. Medical Example 2-VLLG0rYC7To.en

And the answer is 0.8. As I'm sure you noticed in the case where there is cancer, the possible test outcomes add up to 1. In the where there isn't cancer, the possible test outcomes add up to 1. So 1 - 0.2 = 0.8.

### 04. Medical Example 3-Iz4ViIg9ZlQ.en

Look at this, this is very nontrivial but armed with this, we can now build up the truth table for all the cases of the two different variables, cancer and non-cancer and positive and negative tests outcome. So, let me write down cancer and test and let me go through different possibilities. We could have cancer or not, and the test may come up positive or negative. So, please give me the probability of the combination of those for the very first one, and as a hint, it's kind of the same as before where we multiply two things, but you have to find the right things to multiple in this table over here. This is not an easy question.

### 04. Medical Example 3-Rf6WfB_1EJQ.en

And the answer is probability of cancer is 0.1, probability of test being positive given that he has cancer is the one over here--0.9, multiplying those two together gives us 0.09.

### 05. Medical Example 4-pL8Bf6tck_A.en

Moving to the next case--what do you think the probability is that the person does have cancer but the test comes back negative? What's the combined probability of these two cases?

### 05. Medical Example 4-udduksMWMB4.en

And once again, we'd like to refer the corresponding numbers over here on the right side 0.1 for the cancer times the probability of getting a negative result conditioned on having cancer and that is 0.1  0.1, which is 0.01.

### 06. Medical Example 5-fqt7NIvMB0s.en

Moving on to the next case. What do you think the answer is?

### 06. Medical Example 5-ys9w-NNKCcU.en

And here the answer is 0.18 by multiplying the probability of not having cancer, which is 0.9, with the probability of getting a positive test result for a non-cancer patient 0.2. Multiplying 0.9 with 0.2 gives me 0.18.

### 07. Medical Example 6--lC9xztr4zA.en

Here you get 0.72, which is the product of not having cancer in the first place 0.9 and the probability of getting a negative test result under the condition of not having cancer.

### 07. Medical Example 6-iyE5h48qPFQ.en

Let's just quickly do the final one, because it's the most likely one.

### 08. Medical Example 7-cw_zgQbAWNU.en

Now quickly, do me a favor and add all of those up. What do you get?

### 08. Medical Example 7-jPspIs-fNxg.en

And as usual, the answer is 1. That is, we study in the truth table all possible cases. and when we add up the probabilities, you should always get the answer of 1.

### 09. Medical Example 8-7k5oAaZamCA.en

Now let me ask you a really tricky question. What is the probability of a positive test result? Can you sum or determine, irrespective of whether there's cancer or not, what is the probability you get a positive test result?

### 09. Medical Example 8-btGdX0ZpkNU.en

And the result, once again, is found in the truth table, which is why this table is so powerful. Let's look at where in the truth table we get a positive test result. I would say it is right here, right here. If you take corresponding probabilities of 0.09 and 0.18, and add them up, we get 0.27, and that's the correct answer for getting a positive result.

### 10. Total Probability-YSYpzFR4k1I.en

Putting all of this into mathematical notation we've given the probability of having cancer and from there, it follows the probability of not having cancer. And they give me 2 conditional probability that are the test being positive. If we have have cancer, from which we can now predict the probability of the test being negative of having cancer. And the probability of the test being positive can be cancer free which can complete the probability of a negative test result in the cancer-free case. So these things are just easily inferred by the 1 minus rule. Then when we read this, you complete the probability of a positive test result as the sum of a positive test result given cancer times the probability of cancer, which is our truth table entry for the combination of P and C plus the same given we don't have of cancer. Now this notation is confusing and complicated if we ever dive deep into probability, that's called total probability, but it's useful to know that this is very, very intuitive and to further develop intuition let me just give you another exercise of exactly the same type.

### 11. Two Coins 1-QIQBb4nLsHc.en

This time around, we have a bag, and in the bag are 2 coins,coin 1 and coin 2. And in advance, we know that coin 1 is fair. So P of coin 1 of coming up heads is 0.5 whereas coin 2 is loaded, that is, P of coin 2 coming up heads is 0.9. Quickly, give me the following numbers of the probability of coming up tails for coin 1 and for coin 2.

### 11. Two Coins 1-SYnYIjLpbjE.en

And the answer is 0.5 for coin 1and 0.1 for coin 2, because these things have to add up to 1 for each of the coins.

### 12. Two Coins 2-hoVOT8qcQ7c.en

And lets do the truth table. You have a pick event followed by a flip event We can pick coin 1 or coin 2. There is a 0.5 chance for each of the coins. Then we can flip and get heads or tails for the coin we've chosen. Now what are the probabilities? I'd argue picking 1 at 0.5 and once I pick the fair coin, I know that the probability of heads is, once again, 0.5 which makes it 0.25 The same is true for picking the fair coin and expecting tails but as we pick the unfair coin with a 0.5 chance we get a 0.9 chance of heads So 0.5 times 0.95 gives you 0.45 whereas the unfair coin, the probability of tails is 0.1 multiply by the probability of picking it at 0.5 gives us 0.05 Now when they ask you, what's the probability of heads we'll find that 2 of those cases indeed come up with heads so if you add 0.25 and 0.45 and we get 0.7. So this example is a 0.7 chance that we might generate heads.

### 12. Two Coins 2-tI0J14yQr1s.en

So now what happens is, I'm going to remove one of the coins from this bag, and each coin, coin 1 or coin 2, is being picked with equal probability. Let me now flip that coin once, and I want you to tell me, what's the probability that this coin which could be 50% chance fair coin and 50% chance a loaded coin. What's the probability that this coin comes up heads? Again, this is an exercise in conditional probability.

### 13. Two Coins 3-GO6kbL3QRBE.en

Now let me up the ante by flipping this coin twice. Once again, I'm drawing a coin from this bag, and I pick one at 50% chance. I don't know which one I have picked. It might be fair or loaded. And in flipping it twice, I get first heads, and then tails. What's the probability that if I do the following, I draw a coin at random with the probabilities shown, and then I flip it twice, that same coin. I just draw it once and then flip it twice. What's the probability of seeing heads first and then tails? Again, you might derive this using truth tables.

### 13. Two Coins 3-JIWv5fU3GLA.en

This is a non-trivial question, and the right way to do this is to go through the truth table, which I've drawn over here. There's 3 different things happening. We've taken initial pick of the coin, which can take coin 1 or coin 2 with equal probability, and then you go flip it for the first time, and there's heads or tails outcomes, and we flip it for the second time with the second outcome. So these different cases summarize my truth table. I now need to observe just the cases where head is followed by tail. This one right here and over here. Then we compute the probability for those 2 cases. The probability of picking coin 1 is 0.5. For the fair coin, we get 0.5 for heads, followed by 0.5 for tails. They're together is 0.125. Let's do it with the second case. There's a 0.5 chance of taking coin 2. Now that one comes up with heads at 0.9. It comes up with tails at 0.1. So multiply these together, gives us 0.045, a smaller number than up here. Adding these 2 things together results in 0.17, which is the right answer to the question over here. That was really non-trivial, and I'd be amazed if you got this correct.

### 14. Two Coins 4-9R44IyZ-aQI.en

And the answer is depressing. If you, once again, draw the truth table, you find, for the different combinations, that if you've drawn coin 1, you'd never see tails. So this case over here, which indeed has tails, tails. We have 0 probability. We can work this out probability of drawing the first coin at 0.5, but the probability of tails given the first coin must be 0, because the probability of heads is 1, so 0.5 times 0 times 0, that is 0. So the only case where you might see tails/tails is when you actually drew coin 2, and this has a probability of 0.5 times the probability of tails given that we drew the second coin, which is 0.4 times 0.4 again, and that's the same as 0.08 would have been the correct answer.

### 14. Two Coins 4-cDub-OOrIRE.en

Let me do this once again. There are 2 coins in the bag, coin 1 and coin 2. And as before, taking coin 1 at 0.5 probability. But now I'm telling you that coin 1 is loaded, so give you heads with probability of 1. Think of it as a coin that only has heads. And coin 2 is also loaded. It gives you heads with 0.6 probability. Now work out for me into this experiment, what's the probability of seeing tails twice?

### 15. Summary-yepMH9VswI8.en

So there're important lessons in what we just learned, the key thing is we talked about conditional probabilities. We said that the outcome in a variable, like a test is actually not like the random coin flip but it depends on something else, like a disease. When we looked at this, we were able to predict what's the probability of a test outcome even if we don't know whether the person has a disease or not. And we did this using the truth table, and in the truth table, we summarized multiple lines. For example, we multiplied the probability of a test outcome condition on this unknown variable, whether the person is diseased multiplied by the probability of the disease being present. Then we added a second row of the truth table, where our unobserved disease variable took the opposite value of not diseased. Written this way, it looks really clumsy, but that's effectively what we did when we went to the truth table. So we now understand that certain coin flips are dependent on other coin flips, so if god, for example, flips the coin of us having a disease or not, then the medical test again has a random outcome, but its probability really depends on whether we have the disease or not. We have to consider this when we do probabilistic inference. In the next unit, we're going to ask the real question. Say we really care about whether we have a disease like cancer or not. What do you think the probability is, given that our doctor just gave us a positive test result? And I can tell you, you will be in for a surprise.



## Part 06-Module 01-Lesson 07_Bayes Rule

### 01. Bayes Rules-CohZnkZMOxE.en

So this unit is a tough one. We're going to talk about perhaps the holy grail of probabilistic inference. It's called Bayes Rule. Bayes Rule is based on Reverend Thomas Bayes, who used this principle to infer the existence of God, but in doing so, he created a new family of methods that has vastly influenced artificial intelligence and statistics. So let's dive in!

### 02. Cancer Test-CNpSrdnYvbo.en

Let's use the cancer example from my last unit. There's a specific cancer that occurs in 1% of the population, and a test for this cancer and with 90% chance it is positive if they have this cancer, C. That's usually called the sensitivity. But the test sometimes is positive, even if you don't have C. Let's say with another 90% chance it's negative if we don't have C. That's usually called the specificity. So here's my question. Without further symptoms, you take the test, and the test comes back positive. What do you think is the probability of having that specific type of cancer? To answer this, let's draw a diagram. Suppose these are all of the people, and some of them, exactly 1%, have cancer. 99% is cancer free. We know there's a test that if you have cancer, correctly diagnose it with 90% chance. So if we draw the area where the test is positive, cancer and test positive, then this area over here is 90% of the cancer circle. However, this isn't the full truth. The test sent out as positive even if the person doesn't have cancer. In fact, in our case, it happened to be in 10% of all cases. So we have to add more area, because as big as 10% of this large area is as big as 10% of this large area where the test might go positive, but the person doesn't have cancer. So this blue area is 10% of all the area over here minus the little small cancer circle. And clearly, all the area outside these circles corresponds a situation of no cancer, and the test is negative. So let me ask you again. Suppose we have a positive test, what do you think? Would a prior probability of cancer of 1%, a sensitivity and specificity of 90%, Do you think your new chances are now 90% or 8% or still just 1%?

### 02. Cancer Test-FnNveASivMA.en

And I would argue it's about 8%. In fact, as we see, it will come out at 8 1/3% mathematically. And the way to see this in this diagram is this is the region that should test as positive. By having a positive test, you know you're in this region, and nothing else matters. You know you're in this circle. But within this circle, the ratio of the cancerous region relative to the entire region is still pretty small. It increase, obviously, having a positive test changes your cancer probability, but it only increases by a factor of about 8, as we will see in a second.

### 03. Prior And Posterior-GlmS_jox08s.en

Obviously, P(C) is 0.01  (times) 0.9 is 0.009, whereas 0.99  (times) 0.1, this guy over here, is 0.099. What we've computed is here is the absolute area in here which is 0.009 and the absolute area in here which is 0.099.

### 03. Prior And Posterior-o2Tpws5C2Eg.en

So this is the essence of Bayes Rule, which I'll give to you to you in a second. There's some sort of a prior, of which we mean the probability before you run a test, and then you get some evidence from the test itself. and that all leads you to what's called a posterior probability. Now this is not really a plus operation. In fact, in reality, it's more like a multiplication, but semantically, what Bayes Rule does is it incorporates some evidence from the test into your prior probability to arrive at a posterior probability. So let's make this specific. In our cancer example, we know that the prior probability of cancer is 0.01, which is the same as 1%. The posterior of the probability of cancer given that our test is positive, abbreviate here as positive, is the product of the prior times our test sensitivity, which is what is the chance of a positive result given that I have cancer? And you might remember, this was 0.9, or 90%. Now just to warn you, this isn't quite correct. To make this correct, we also have to compute the posterior for the non cancer option, which there is no cancer given a positive test. And using the prior, we know that P of not C is 0.99. It's minus P of C Times the probability of getting a positive test result given not C. Realize these 2 equations are the same, but I exchanged C for not C. And this one over here takes a moment to computer. We know that our test gives us a negative result if it's cancer free, 0.9 chance As a result, it gives us a positive result in the cancer free case, with 10% chance. Now what's interesting is this is about the correct equation except the probabilities don't add up to 1. To see I'm going to ask you to compute those, so please give me the exact numbers for the first expression and the second expression written over here using our example up there.

### 04. Normalizing 1-5Tbd3_a5Vug.en

And, yes, the answer is 0.108. Technically, what this really means is the probability of a positive test result-- that's the area in the circle that I just marked. By virtue of what we learned last, it's just the sum of two things over here, which gives us 0.108.

### 04. Normalizing 1-9SbUxcyDTaQ.en

The normalization proceeds in two steps. We just normalized these guys to keep ratio the same but make sure they add up to 1. So let's first compute the sum of these two guys. Please let me know what it is.

### 05. Normalizing 2--pOzdj6pnbA.en

The answer is 0.0833.

### 05. Normalizing 2-WYA5Zbf8HC4.en

And now finally, we come up with the actual posterior, whereas this one over here is often called the joint probability of two events. And the posterior is obtained by dividing this guy over here with this normalizer. So let's do this over here--let's divide this guy over here by this normalizer to get my percent distribution of having cancer given that I received the positive test result. So divide this number by this.

### 06. Normalizing 3-V96RcbbVP7Q.en

Let's do the same for the non-cancer version, pick the number over here to divide and divide it by this same normalizer.

### 06. Normalizing 3-etrUbOAoh1U.en

The answer is 0.9167 approximately.

### 07. Total Probability-_hXCgF-aMB0.en

Why don't you for a second add these two numbers and give me the result?

### 07. Total Probability-fAaE5K9OZJc.en

And the answer is 1 as you will expect. Now this was really challenging. You can see a lot of math in this slide. Let me just go over this again and make it much, much easier for you.

### 08. Bayes Rule Diagram-b8M9CWxRyQ4.en

Well, we really said that we had a situation where the prior P(C), a test with a certain sensitivity (Pos/C), and a certain specificity (Neg/C). When you receive, say, a positive test result, what you do is, you take your prior P(C) you multiply in the probability of this test result, given C, and you multiply in the probability of the test result given (Neg/C). So, this is your branch for the consideration that you have cancer. This is your branch for the consideration of no cancer. When you're done with this, you arrive at a number that now combines the cancer hypothesis with the test result. Look for the cancer hypothesis and the no cancer hypothesis. Now, what you do, you add those up and then normally don't add up to one. You get a certain quantity which happens to be the total probability that the test is what it was in this case positive. And all you do next is divide or normalize this thing over here by the sum over here and the same on the right side. The divider is the same for both cases because this is your cancer branch, your non-cancer branch, but this score does not depend on the cancer variable anymore. What you now get out is the desired posterior probability, and those add up to 1 if you did everything correct, as shown over here. This is the algorithm for Bayes Rule.

### 09. Equivalent Diagram-aUFWZ2uJuBE.en

Now, the same algorithm works if your test says negative. We'll practice this in just 1 second. Suppose your test result says negative. You could still ask the same question: Now, what's my probability having cancer or not? But now all the positives in here become negatives. The sum is the total probability of negative test results,  and we may now divide by this score, you now get the posterior probability for cancer and non-cancer assuming you had a negative test result, which of course to be much, much more favorable for you because none of us wants to have cancer. So, look at this for a while and let's now do the calculation for the negative case using the same numbers I gave you before, and with the step by step this time around so it can really guide you through the process.

### 10. Cancer Probabilities-7ZLe_JP5wRY.en

And obviously this is still 0.99 as before 0.1 and 0.1. I hope you got this correct.

### 10. Cancer Probabilities-CMQBKuYjPBM.en

We begin with our prior probability, our sensitivity and our specifitivity, and I want you to begin by filling in all the missing values. So, there's the probability of no cancer, probability of negative, which is negation of positive, given C, and probability of negative-positive given not C.

### 11. Probability Given Test-41HCYR-NW-w.en

Now assume the test comes back negative, the same logic applies as before. So please give me the combined probability of cancer given the negative test result and the combined probability of being cancer-free given the negative test result.

### 11. Probability Given Test-omC0zbJyzUY.en

The number here is 0.001 and it's the product of my prior for cancer which is 0.01, and the probability of getting a negative result in the case of cancer which is right over here, 0.1. If I multiply these two things together, I get 0.001. The probability here is 0.891. And when I'm multiplying is the prior probability of not having cancer which is 0.99 with the probability of seeing a negative result in the case of not having cancer, and that is the one right over here, 0.9. So, we'll multiply 0.99 with 0.9, I actually get 0.891.

### 12. Normalizer-G9yQ_URDrDQ.en

Let's compute the normalizer. You now remember what this was.

### 12. Normalizer-W5i-gRAvZxs.en

And the answer is 0.892. You just add up these two values over here.

### 13. Normalizing Probability-V_Gqm42WodI.en

And now finally tell me what is posterior probability of cancer given that we know we had a negative test result and the probability of negative cancer given there is a negative test result. Please give me the numbers here.

### 13. Normalizing Probability-yYqN9Mf4jqw.en

This is approximately 0.0011, which we get by dividing 0.001 by the normalizer 0.892, and the posterior probability of being cancer-free after the test is approximately 0.9989, and that's obtained by dividing this probability over here by the normalizer and not surprisingly, these two values indeed add up to 1. Now, what's remarkable about this outcome is really what it means. Before the test, we had a 1% chance of having cancer, now, we have about a 0.9% chance of having cancer. So, a cancer probability went down by about a factor of 9. So, the test really helped us gaining confidence that we are cancer-free. Conversely, before we had a 99% chance of being cancer free, now it's 99.89%. So, all the numbers are working exactly how we expect them to work.

### 14. Disease Test 1-05upwXtARuo.en

Let me now make your life harder. Suppose our probability of a certain other kind of disease is 0.1, so 10% of the population has it. Our test in the positive case is really informative, but there's a 0.5 chance that if I'm cancer-free the test, indeed, says the same thing. So the sensitivity is high, the specificity is lower. And let's start by filling in the first 3 of them.

### 14. Disease Test 1-qDGSvvabN18.en

Obviously, these are just 1 minus those: 0.9, 0.1, and 0.5. Notice that these two numbers may very well be different. There is no contradiction here. These guys have to add up to 1, so given C, the probability of positive and negative have to add up to 1, but these guys don't. It takes a lot of practice to understand which numbers have to add up to 1. But I set it up in a way that you should have gotten it right.

### 15. Disease Test 2-FQM7i07EqGo.en

And the answer is 0.01. P(C) = 0.1, and P(NegC) is also 0.1, so if you multiply those two they are 0.01.

### 15. Disease Test 2-GsneDVJB75E.en

Now comes the hard part: What is P(C, Neg)?

### 16. Disease Test 3-PfEYA6z-19w.en

And what's the same for P(C, Neg).

### 16. Disease Test 3-a61GPGk-Qy4.en

And the answer is 0.45. P(C) is 0.9, and P(NegC) is 0.5. So 0.9 * 0.5 = 0.45.

### 17. Disease Test 4-UERKMwmkAsM.en

Well, you just add up these two numbers to get 0.46.

### 17. Disease Test 4-ztkKTrMZHXg.en

What's the score over here?

### 18. Disease Test 5-4qW7a5E74No.en

So tell me what the final two numbers are.

### 18. Disease Test 5-nUxwwMNKIYo.en

The first one is 0.01 divided by normalized 0.46 and that gives us 0.0217, and the second one is called over here 0.45 divided by 0.46 and that gives us 0.9783 and uses the correct posteriors, restarted our chance of 10% of having cancer. We had a negative result. We're down now to about 2% of having cancer.

### 19. Disease Test 6-OdVAt79eQak.en

So once again, we have 0.9, 0.1, and 0.5 over here. Very quickly multiplying this guy with this girl over here 0.09. This guy with this girl over here 0.45. Adding them up gives us 0.54, and dividing those correspondingly 0.9 divided by 0.54 gives us 0.166 and so on and 0.833 and so on for dividing 0.45 by 0.54. And with this means, with the positive test result, our chance of cancer increased from 0.1 to 0.16. Obviously, our chance of having no cancer decreased accordingly. You got this, so let's just summarize.

### 19. Disease Test 6-cdFrLeXIkZU.en

Let's now consider the case that the test result is positive, and I want you to just give me the two numbers over here and not the other ones.

### 20. Bayes Rule Summary-RgXQ8GRsjfc.en

In Bayes rule, we have a hidden variable we care about--whether they have cancer or not. But we can't measure it directly and instead we have a test. We have a prior of how frequent this variable is true and the test is generally characterized by how often it says positive when the variable is true and how often it is negative and the variable is false. Bayes rule takes a prior, multiplies in the measurement, which in this case we assume to be the positive measurement to give us a new variable and does the same for all actual measurement, given the opposite assumption about our hidden variable of cancer and that multiplication gives us this guy over here. We add those two things up and then it gives us a new variable and then we divide these guys to arrive the best estimate of the hidden variable c given our test result. And this example, I used the positive example is a test result but it might do the same with a negative example. This was exactly the same as in our diagram in the beginning. There was a prior of our case, we have this specific variable to be true. We noticed inside this prior, it can cover the region for which our test result applies. We noticed that test result also apply when the condition is not fulfilled. So, this expression over here and this expression over here corresponds exactly to the red area over here and the green area over here. But then we noticed that these two areas don't add up to 1. The reason is that's lots of stuff outside, so we calculated the total area which was this expression over here, pPos. And then we normalized these two things over here by the total area to get the relative area that is assigned the red thing versus the green thing and at this time by just dividing by the total area in this region over here; thereby, getting rid of any of the other cases.

### 21. Robot Sensing 1--TBAfU1cjRU.en

In this example, it gives us funny numbers. It was 3 for red as 0.8 and one for the green as 0.2. And it's all to do with the fact that in the beginning where there had no clue where it is. The joint for red after seeing red is 0.4. The same for green is 0.1. 0.4+0.1, S to 0.5. If you normalized 0.4 divided by 0.5, you get 0.8, and if you normalized 0.1 by 0.5, you get 0.2.

### 21. Robot Sensing 1-_DjfTytro6I.en

Now, I should say if we got this, you don't find any immediate significant about statistics and probability. This is totally nontrivial, but it comes in very handy. So, I'm going to practice this with you using a second example. In this case, you are a robot. This robot lives in a world of exactly two places. There is a red place and a green place, R and G. Now, I say initially, this robot has no clue where it is, so the prior probability for either place, red or green, is 0.5. It also has a sensor as it can see through its eyes, but his sensor seems to be somewhat unreliable. So, the probability of seeing red at the red grid cell is 0.8, and the probability of seeing green at the green cell is also 0.8. Now, I suppose the robot sees red. What are now the posterior probabilities that the robot is at the red cell given that it just saw red and conversely what's the probability that it's at the green cell even though it saw red. Now, you can apply Bayes Rule and figure that out.

### 22. Robot Sensing 2-aBBmlnd7okQ.en

And the answer is, the prior isn't affected by the measurement, so the probability of 0 is at red, and the probability of 1 at green, despite the fact that it's all red. To see this, you find the joint of seeing it red and seeing red is 0 times 0.8, that's 0. That's the same join for green is 1 times 0.2. So you have to normalize 0 and 0.2. The sum of those is 0.2. So let's divide 0 by 0.2, gives us 0, and 0.2 divided by 0.2 gives us 1. These are exactly the numbers over here.

### 22. Robot Sensing 2-t22oDruXhuo.en

If I now change some parameters--say the robot knows the probability that it's red, and therefore, the probability 1 is under the green cell as a prior. Please calculate once again using Bayes rule these posteriors. I have to warn you--this is a bit of a tricky case.

### 23. Robot Sensing 3--6l4_oprDOk.en

To change this example even further. Let's make this over here a 0.5 and revert back to a uniform prior. Please go ahead and calculate the posterior probability.

### 23. Robot Sensing 3-m1LSU9SPZ2k.en

Now the answer is about 0.615 or 0.385. These are approximate. Once again, 0.5 times 0.8 is 0.4. 0.5 minus this guy is again 0.5. 0.25, add those up, 0.65, normalizing 0.4 divided by 0.65 gives approximately 0.615. 0.25 divided by 0.65 is approximately 0.385, so now you you've got it.

### 24. Robot Sensing 4-d_fbDqAGVdE.en

And just like before, we multiply the prior, this guy over here, that gives you 0.3.

### 24. Robot Sensing 4-vasdN2Gol0M.en

I will now make your life really hard. Suppose there are 3 places in the world, not just 2. There are a red one and 2 green ones. And for simplicity, we'll call them A, B, and C. Let's assume that all of them have the same prior probability of 1/3 or 0.333, so on. Let's say the robot sees red, and as before, the probability of seeing red in Cell A is 0.9. The probability of seeing green in Cell B 0.9. Probability of seeing green in Cell C is also 0.9. So what I've changed is, I've given the hidden variable, kind of like the cancer/non cancer variable, 3 states. There's not just 2 as before, A or B. It's now A, B, or C. Let's solve this problem together, because it follows exactly the same recipe as before, even though it might not be obvious. So let me ask you, what is the joint of being in Cell A after having seen the red color? This is the joint as before.

### 25. Robot Sensing 5-PGG9agooCvw.en

Well, the answer is you multiply our prior of 1/3 with the probability of seeing red in Cell B, as seeing green at 0.9 probability, so red is 0.1. So 0.1 times this guy over here gives 0.033.

### 25. Robot Sensing 5-tIrqdYTT_9Q.en

What's the joined for Cell B?

### 26. Robot Sensing 6-Se-ddM2Wdac.en

Finally, probability of C and Red. What is that?

### 26. Robot Sensing 6-hXyXlk0gYzk.en

And the answer is exactly the same as this over here, because the prior is the same for B and C, and those probabilities are the same for B and C, so they should be exactly the same.

### 27. Robot Sensing 7-clFL503NPyY.en

And the answer is, you just add those up.

### 27. Robot Sensing 7-goEMc0w58xM.en

So here's the $100,000 question. What is our normalizer?

### 28. Robot Sensing 8-hyAQ28MYmc4.en

And now we calculate the desired posterior probability for all 3 possible outcomes. So please plug them in over here.

### 28. Robot Sensing 8-lmuonrQp_lM.en

As usual, we divide this guy over here by the normalizer, which gives us 0.818. Realize all these numbers are a little bit approximate here. Same for this guy, it's approximately 0.091. And this is completely symmetrical, 0.091. And surprise, these guys all add up to 1.

### 29. Generalizing-SdMk3aROgSc.en

So what have you learned? In Bayes Rule, there will be more than just 2 underlying causes of cancer/non cancer. There might be 3, 4, or 5, any number. We can apply exactly the same math, but we have to keep track of more values. In fact, the robot might also have more than just 2 test outcomes. Here was red or green, but it could be red, green, or blue. And this means that our measurement probability will be more elaborate. I have to give you more information, but the math remains exactly the same. We can now deal with very large problems that have many possible hidden causes of where the world might be, and we can still apply Bayes Rule to find all of these numbers. Let me give you one final test.

### 30. Sebastian At Home-R4zq6mPPMxs.en

And I get 0.0217, which is a really small thing. And the way I get there is what taking home times the probability of rain at home normalizing it using the same number of a year plus the calculation for the same probability of being gone is 0.6 times the rain I've been gone has a probability of 0.3 and that results is 0.0217 or the better of 2%--did you get this? If so, you now understand something that's really interesting. You're able to look at a hidden variable, understand how a test can give you information back about this hidden variable and that's really cool because it allows you to apply the same scheme to great many practical problems in the world--congratulations! In our next unit, which is optional, I like you to program all of this so you can try the same thing in an actual program interface and writes software that implements things such as Bayes rule. But not to worry, this is optional. If you don't know how to program just skip the next unit.

### 30. Sebastian At Home-TtmQ7YCw_1Y.en

This test is actually directly taken from my life and you'll smile when you see my problem. I used to travel a lot. It was so bad for a while. I would find myself in a bed not knowing what country I'm in. I kid you not. So let's say, I'm gone 60% of my time and I'm at home only 40% of my time. Now at summer, I live in California and it truly doesn't rain in the summer. Whereas in many of the countries I have traveled to, there's a much higher chance of rain. So let's now say, I lie in my bed, here I am lying in bed, and I wake up and I open the window and I see it's raining. Let's now apply Bayes rule--What do you think is the probability I'm home now that I see it's raining--just give me this one number.

### 32. Reducing Uncertainty-zuFMhmKQ--o.en

Let's talk a bit more about why uncertainty is so important in the field of robotics and self-driving cars. We know that measurements like the speed, the direction, and the location of a car are challenging to measure and we can't measure them perfectly. There's some uncertainty in each of these measurements. We also know that many of these measurements affect one another. For example, if we are uncertain about the location of a car, we can reduce that uncertainty by collecting data about the car's surroundings and its movement. Self-driving cars measure all of these things from a car's speed, to the scenery and objects that surround it, with sensors. And though these sensor measurements aren't perfect, when the information they provide is combined using conditional probability and something called Bayes rule, they can form a reliable representation of a car's position, its movement and its environment. Let's take a look at Bayes Rule.

### 33. Bayes' Rule and Robotics-meNSO42JF6I.en

Bayes rule is extremely important in robotics and it can be described in one sentence. Given an initial prediction, if we gather additional related data, data that our initial prediction depends on, we can improve that prediction. For example, let's say our initial prediction also known as a prior belief, is an estimate of a car's location on a road. This might be the location given by a slightly inaccurate GPS signal. Then, we use sensors to gather data about the car's surroundings and how the car is moving. How do you think this sensor data can help us improve our initial location prediction?

### 35. Using Sensor Data-vhl-SADfti8.en

Once we gather sensor data about the car's surroundings and its movement, we can then use this information to improve our initial location prediction. For example, say we sense lane markers and specific terrain, and we say, hmm. Actually, we know from previously collected data that if we sense landlines close to the sides of the car, the car is probably located in the center of the lane. We also know that if we sense that our tires are pointing to the right, we're probably on a curved section of the road. So this sensor data, combined with what we already know about the road and the car, gives us more information about where our location is most likely to be. So using the sensor information, we can improve our initial prediction and better estimate our car's location. Bayes rule gives us a mathematical way to correct our measurements, and let's us move from an uncertain prior belief to something more and more probable. You'll see Bayes rule come up again and again in robotics. And in this lesson, you'll gain a greater understanding of Bayes rule.

### 37. Bayes Rule Conclusion-vlfDGCD8w0s.en

Great job finishing this lesson on Bayes rule. At this point, you've now gained a ton of valuable knowledge about probability, conditional probability and Bayes rule. To reinforce your understanding of these topics, let's go through some probability practice using Python. You'll do this in the next lesson.



## Part 06-Module 01-Lesson 08_Python Probability Practice

### 01. Python Probability Introduction-tFMdvAN7WDY.en

Now that you've seen some examples and the math involved with them, you're going to apply this knowledge to problems using Python. This lesson includes screencast and Jupyter notebooks to help you practice using Python to explore the topics of probability you just learned. Let's get started.

### 02. Simulating Coin Flips-7YtQNZ3iy6o.en

NumPy has a module for random sampling, that makes it really easy for us to simulate random events like coin flips and Python. We're going to start with a simple example, simulating a single coin flip. For this, we'll use a function called randint. This generates however many random integers, we specify between a lower bound inclusive and upper bound exclusive. We can use these integers to represent the outcomes of our events, like coin flips. Let's try this out. We call randint from NumPy's random sampling module like this. Let's use zero to represent heads and one to represent tails. Let's make this function randomly produce zero or one. The lower bound would be zero and the highest would be two, because it's exclusive. Since the lower bound is zero, this is actually the default. We don't need to include it. If we run the cell again and again, we'll keep getting a random outcome of zero or one or, we can specify a size to just give us more events. Cool. Now, we have the results of 10,000 random coin flips. The average of these outcomes produced here should be very close to 0.5, since right now there's an equal probability of getting a zero or a one. But what if we want to flip a biased coin, that had a higher probability of landing on heads. There's actually another function for this called random.choice. This function works a little differently. It randomly chooses a number of values from an array that you provide and you can also give it a set of probabilities for each value in that array. Let's call random.choice and give it the array of the possible outcomes zero and one. If we don't specify probabilities, it gives us each value and an equal probability by default. We can run this similar to the above example in this way. So the average is very close to 0.5 Again. To make this a biased coin, we can specify the parameter p with an array of probabilities. Say 0.8 for heads and 0-2 for tails. Now, you can see that the mean is close to 0.2 which makes sense because the zero or heads should be chosen 80% of the time. You probably noticed that the mean values we get from these outcomes don't always reflect the true probabilities perfectly. However, they do tend to reflect the true probability more closely as we increase the number of flips. You'll learn more about this later.

### 04. Simulating Many Coin Flips-AqpWQIj2V5Y.en

So far, we've been stimulating event outcomes by generating random numbers with NumPy's random.randint and random.choice. However, there is a better function for simulating large binomial experiments like coin flips. Here, you see NumPy's random.binomial function, which simulates a number of events n, which each have probability of success p. Success just represents one of the two outcomes of the event. Really, either outcome could be the success. For example, if our event is flipping a fair coin 10 times, we could define success as the number of heads. n would be the number of flips. In this case, 10, and p would be the probability of heads for each flip which is 0.5. To try this out, let's set n equal to 10 and p equal to 0.5. Notice, this returns 1 integer instead of an array of 10 outcomes. Since this function is only for binomial outcomes, it can simplify the output by just returning the number of successes. In this case, 4. This is the number of heads. Again, we can run the simulation many times. Let's run this 20 times. Each number in this array represents the number of heads that resulted from each test of 10 coin flips. This is the number of heads on the first 10 coin flips, while this one is the number on the second 10 coin flips and so on, for 20 different flippings of the coin. Let's find the mean number of heads for these tests. Since this is a fair coin, we would expect this mean to be close to 5. You can see that this is a little bit of a ways away, probably because we only ran 20 tests. Let's see what happens if we increase the number of tests. Since these events are random, it's not guaranteed that each simulation will perfectly average five heads. However, as this number of tests increase, the simulation more closely reflects the fairness of the coin. With 10,000 tests, the mean is much closer to five heads. Let's use matplotlib to plot a histogram of the outcomes in this simulation. As expected, the distribution is centered around five heads.

### 08. Python Probability Conclusion-4JYar5GykXk.en

In this lesson, you put your new probability skills to practice using Python. In order to gain an understanding of some of the complex topics in the next sections, we'll be working with Python more and more. It's often easier to understand these complex ideas through simulations and Python than it is to prove them mathematically. You will see a bit of both in the upcoming lessons. So your practice here will definitely come in handy.

## Part 06-Module 01-Lesson 09_Normal Distribution Theory

### 01. Maximum Probability-5zkupL6EWh8.en

I just programmed and ran the experiment, and the answer is 10. The reason why the answer is 10 is because the number of combinations to place 10 positives and 10 negatives into our list of 20 is larger than any other number. This term over here is maximized when k is exactly half of N--so 10.

### 01. Maximum Probability-b2zvrFL8AUw.en

What you are about to see is one of the most transformative parts of modern statistics and it uses things if like we've never seen before. We start out with the binomial distribution that you're familiar with from our last unit and then we move into the central limit theorem which basically means we take a number of coin flips to infinity. From that, we arrive at the normal distribution which is basis to so much in statistics-- all of testing and confidence intervals are defined though the normal distribution. And the reason why this matters is much of what we've done in coin flips had one or two coin flips but in statistics experiments, you often have 1000 of patients or 1000 of data points and then starting the normal distribution as an approximation to the binomial distribution is much more practical. So let's start--so when we start with our now well established formula for binomial distributions where N is the number of coin flips, k is how often it comes up with heads and p is the probability that the coin comes up with heads, usually 0.5. And I want to graph for fix n this function here. K can go from 0 all the way to 20. Instead of letting you compute this thing over here for all those different values of k, I'm going to ask a different question--suppose you have a fair coin, what do you think this value takes on its maximum value, which value of k maximizes our expression. I know you can't really know this but with some thought, I believe you'll arrive at the correct answer.

### 02. Shape-DjsL64Kjr1Q.en

The other interesting thing is things fall down in the interesting fashion as you deviate from 10, 11, 12, 13, 14 all the way to 0 or 20. Obviously we got a curve that looks a bit like this. This curves is often called a bell curve because it's quite feasible to think of it as a church bell-- that's move left and right and rings the bells. I did a related experiment--actually I bought a piece of software to flip a coin 1000 times and if you did the last optional units on programming, you wrote a piece of software to flip a coin a 1000 times and from that, I looked at the empirical frequency which is the same as that count of heads divided by a 1000, but this one scales between 0 and 1. I called this thing an experiment--I flip the coin a 1000 times, out comes a one singular number which is the ratio of heads to the total number of experiments. It should be 0.5 in the ideal case but often it's a little bit off 0.5. I repeated this experiment 1000 times and that means I've got 1000 samples of this ratio over here. When I do this, I got a whole bunch of means. When I run a histogram over those, I might see a curve. What shape do you think the curve has--is it going to be like this, is it going to be like this, like this or like this, all are focused on 0.5. Which one do you think is it?

### 02. Shape-w5qcGO8krMw.en

And it's this one. Let me show you. Here's a typical one, and I apologize the axis over here can't really be read, but you can take with faith that the center is 0.5, and you can see the characteristic bell curve for this simple coin-flipping experiment. For this run the mean was 0.50006. If I run it again, I get a different sample. And there is some randomness involved as this bar over here illustrates, but over the randomness you can clearly see the bell-shaped curve that flattens off on the sides.

### 03. Better Formula-vMAl1m8ZtoI.en

The answer is a resounding yes. Even more so, what I'm going to show you doesn't just apply to binomial distributions with fair coins. It applies to almost any distribution that is sampled many, many times, which is a  very deep statistical result. I will construct for you the formula that is being used.

### 03. Better Formula-z2xsu2Kehyo.en

The question really is can we find a better formula for this bell-shaped curve. The answer is, well, take your guess.

### 04. Quadratics-1R44jvxIPJY.en

I will define for you a normal distribution with a specific mean that's often called , Greek letter , and a variance that's often called . We already know that variance is a quadratic expression. In normal land we often use  and . Let's do this. The very first element is that for any outcome x we write the quadratic difference between this outcome x and . This is indeed a function in x. So, look at this. Here are four possible hypotheses of what this function might look like. Each case is  is on the right side some where. The horizontal axis is x, and we're graphing f(x). The first I'll give you is a triangular function. The second is a quadratic function. The third one is a negative quadratic function. And the fourth one is a quadratic function that doesn't quite touch . So, which one in your opinion best describes this formula over here?

### 04. Quadratics-GzRNoodJZxk.en

I would submit that it's this one over here. The reason is this expression is 0 when x = . As a result, it can't be the fourth of these choices. It's strictly non-negative, so it can't go down into the negative area, so this one is being out ruled. It's quadratic. Hence, a function like this doesn't make sense, so it must be this one over here.

### 05. Quadratics 2-HjpgML5zsUE.en

The next thing I'll do is I'll divide it by . Without telling you why I'm doing this, I want to see what the effect is. Suppose  = 4. That means we have a variance of 4 and a standard deviation of 2. I've given you already the quadratic function when it isn't divided by . It's the same as saying  = 1. What I'd like to know is whether our new version where  = 4 makes this quadratic wider or whether it makes it narrower, assuming that this is our new function f(x). So, pick one of the two, or perhaps it stays the same. Then pick the third.

### 05. Quadratics 2-N-wpkttwcoA.en

The answer is it makes it wider. To see why is this affects the vertical dimension--the output and scales it down by a factor of 4. So that we said before gets dragged down by a factor of 4. That means this point over here finds itself here. This one over here finds itself here. That means we'll widen out the quadratic. Observe that large variances yield wide quadratics. Small or tight variances yield sharp quadratics.

### 06. Quadratics 3-Ny2vcRZ6Aws.en

In particular, if we now look at the quadratic over here, which is much tighter, which of the following potential  would you think is best representative of this narrow function over here, provided that this is the quadratic that corresponds to  = 1. Check one of those four--4, 1, , and 0.

### 06. Quadratics 3-YSMWpFM92S0.en

It follows it's got to be something like a quarter. We already learned that 4 widens the quadratic, so that can't be it. One is already shown over here. Zero makes no sense because we have division by 0 which you can think of as a quadratic that shoots up almost like a straight line, but honestly it doesn't make any sense. A quarter is the one that really describes this particular quadratic the best. Now, that's great. Now we understand this expression over here.

### 07. Quadratics 4-yimIE9fCvi8.en

Let's go further, and let's now take this function and multiply it by -. Again, I ask you what the affect it. If this is your original quadratic, then what do we get? We already know that it's going to flatten it, because you are dividing the f value by half, but are we going to get something like this or perhaps something like this? Pick one of those two choices.

### 07. Quadratics 4-zB2Y-5YEIec.en

And quite interestingly, we inverted the sign, so all of a sudden the function is negative. Quite obviously, green is the correct answer. So, now we have a quadratic that points down into the negative space whose maximum value is 0 and otherwise, it's strictly negative. That's this function f over here.

### 08. Maximum-02v8ui9riew.en

to understand the solution, it's useful to draw the exponential function. e is 1 and then it goes up exponentially to really large numbers. If you've ever heard Ray Kurzweil  talk about the future of society, you've seen these curves--everything goes up exponentially. Everything is just exponential. And further, if you go back in time to negative values, this thing slowly drifts down to 0. Of course, that's not very exciting, so we never talk about exponential in the negative space. However, it turns out that all the arguments of the exponential are at best 0 and otherwise are negative, because the exponential is monotonic-- that is the larger its argument, the larger its exponential value. It ought to be optimized where this thing over here is the largest, and where is that the case? Well, it's exactly where  hits 0.

### 08. Maximum-MZoYGBZTh-g.en

And now I'm going to take the most extreme of all steps. I'm going to make this the exponent of the e function. Remember, the inner argument is a quadratic that points down. This a bit does depend on . This mean is  so I call this f(x) where f(x) maximize. And I'll give you several choices for x=, x=0, x=-infinity, or x=+infinity. Where will this thing be the largest?

### 09. Maximum Value-rjpcSymYulE.en

Quite interestingly, even though this formula looks complex, it a really easy answer, which is when x =  this thing here is 0. That makes the entire thing 0. e--any value to the 0--is going to be 1.

### 09. Maximum Value-z_eElEkVOPY.en

Let me ask you another question. What is the value of this function if we go to the point where it's maximum, which is x = ? That's the way to write this. Compute for me in your head this what this thing will be when x = .

### 10. Minimum-MEbJxfw3NVs.en

The answer is now  . If you look at this, if you put a really large positive or negative value in, the difference to any  will be enormous. The square will be even more enormous. Therefore, this entire expression on the right side will be huge. Put a minus sign in front of it, and you have a hugely negative number. You have e^-. e^- drive the e curve all the way to the left where this just ends up to be minimized.

### 10. Minimum-tiv8VKPL7jg.en

Next I'd like to know where is f(x) minimized? For what value of x would we get the possible smallest value of this entire expression over here? Again, pick one or more of those choices over here.

### 11. Minimum Value-LNzmJUj8K8w.en

In fact, what do you think is the value of this where x = ?

### 11. Minimum Value-LconwqN7hJs.en

The answer is 0. As x goes to infinity, this expression goes to negative infinity. The exponential of negative infinity converges to 0. So, now we've got basically the normal distribution function.

### 12. Normalizer-mQ_IjrtmmAk.en

We have a function f that assumes the value 1 when x =  that goes to 0 when x goes to . It so happens that it looks like a bell curve. The fact that it looks like a bell curve is not entirely obvious, but you have to take my word for it. This--what I would consider a relatively simple formula-- describes the limit of making infinitely many coin flips. In fact, it describes the limit of computing a mean over any set of experiments. This is a very powerful result. No matter what you do when you drive n to very large numbers you get a bell curve like this. There is one flaw here, and I'll tell you about the flaw without going into detail. That is the area underneath this curve doesn't always add up to 1. In fact, without proof, it adds up to 2. The reason why this matters is deeply buried in probability theory. But it turns out we want all these areas to add up to 1 just as much as we wanted a coin flip and its complement to add up to 1. The true normal distribution is normalized by just the inverse of this thing over here--1/2. So, that is the normal distribution of any value x indexed by the parameter  and . So, this is a very deep piece of mathematics. Now, we will apply it a little bit for you to practice how the normal distribution looks in the field.

### 13. Formula Summary-zqo1RJEHT_0.en

So, here is our normal distribution again. I'm going to write it as "exp" for exponential         {- (x - )/}. The truth is when you're new to this this looks really cryptic. When you're with statistics for many years as I have been, you wake up in the middle of the night and you can recite this formula. It's as normal as getting breakfast in the morning or having a beer after dinner. What I want to get into your brains is not the complexity of the formula. I want you to really understand how this formula is constructed. I you to understand the quadratic penalty term of deviations from the expectation of the mean of this expression. Then the exponential that squeezes it back into the curves. That's basically what it is. We can draw values from this normal distribution just the same way as we flipped coins before. The way to look at this is any value x has this probability up here. This is nothing else but a notation of the probability of x for a normal distribution with  and . So, a value x that has twice the high bar than some other value x' will have twice as much of a probability of being drawn. Now, obviously, the normal has an entire continuous space of outcomes. And obviously that renders each individual outcome of probability 0. But, in essence, you can think of the height of this thing over here as being proportional to the probability that this value is being drawn.

### 14. Central Limit Theorem-36KLIHioAvA.en

As I'm sure you've guessed this is the probability for a single coin flip. This is the formula we can use for multiple coin flips and as we go to very large numbers, the [UNKNOWN] is often a good approximation for the outcome of many coin flips. I should warn you, if the coin flip is zero to one, then the mean is always in the positive. This can assume negative values, but it's an approximation

### 14. Central Limit Theorem-9I8ysrRlmbA.en

So let's look for a second at different ways to compute probabilities for coin flips. You have a coin that has probability P of coming up heads. I will give you now 3 very different formulas. Here's a single probability, here's our common [UNKNOWN] formula and here's our Gaussian exponential. So what I want to know from you is which one of these formulas is best suited to compute a probability for a single coin flip, a few coin flips, or many coin flips, possibly even infinity coin flips as a limit value? So click exactly three of those buttons, one each row and one each column for the best correspondence of the formula to the case on the left

### 15. Summary-VP-PMcgqhc8.en

What I've shown you in the beginning of class have from a coin flip to a binomial distribution all the way to a normal distribution, and you might think that this was challenging and indeed it was. As it turns out, you can treat all this things about the same. In fact, if you're a medical doctor and you have one patient, you might think of it as a coin flip. If you have 10 patients, you might think of it as binomial distribution. If you do what's normally done, when your test say a new drug and you have, say 10,000 patients then this thing over here is a beautiful and very compact representation of it. Otherwise, it would be almost impossible to compute. That's the purpose of normal distribution for the sake of this class. As you go forward and look into hypotheses testing and confidence then develops. You don't do this for this relatively complicated expressions over here. We just do it for the normal distribution that I think relatively easy to compute. Welcome to the world of normal distributions.



## Part 06-Module 01-Lesson 10_Sampling distributions  and the Central Limit Theorem

### 01. Introduction-SvdlBB-ZjcQ.en

In this lesson, you'll be learning about sampling distributions. In order to gain a firm grasp of how sampling distributions work, it's important to first have a strong grasp of inferential statistics. We will do a recap in the next concepts to make sure you're comfortable with the ideas surrounding inferential statistics. This is also a shift from earlier content, where you were thinking of probability functions. From this point, you'll be thinking more about statistics. That is, we'll be learning from data to draw our conclusions, rather than using probability to draw our conclusions.

### 02. Descriptive vs. Inferential Statistics-XV9pd8-RZ78.en

The topics covered this far have all been aimed at descriptive statistics. That is, describing the data we've collected. There's an entire other field of statistics known as inferential statistics that's aimed at drawing conclusions about a population of individuals based only on a sample of individuals from that population. Imagine I want to understand what proportion of all Udacity students drink coffee. We know you're busy, and in order to get projects in on time, we assume you almost drink a ton of coffee. I send out an email to all Udacity alumni and current students asking the question, do you drink coffee? For purposes of this exercise, let's say the list contained 100,000 emails. Unfortunately, not everyone responds to my email blast. Some of the emails don't even go through. Therefore, I only receive 5,000 responses. I find that 73% of the individuals that responded to my email blast, say they do drink coffee. Descriptive statistics is about describing the data we have. That is, any information we have and share regarding the 5,000 responses is descriptive. Inferential statistics is about drawing conclusions regarding the coffee drinking habits of all Udacity students, only using the data from the 5,000 responses. Therefore, inferential statistics in our example is all about drawing conclusions regarding all 100,000 Udacity students using only the 5,000 responses from our sample. The general language associated with this scenario is as shown here. We have a population which is our entire group of interest. In our case, the 100,000 students. We collect a subset from this population which we call a sample. In our case, the 5,000 students. Any numeric summary calculated from the sample is called a statistic. In our case, the 73% of the 5,000 that drink coffee. This 73% is the statistic. A numeric summary of the population is known as a parameter. In our case, we don't know this value as it's a number that requires information from all Udacity students. Drawing conclusions regarding a parameter based on our statistics is known as inference.

### 06. Example of Sampling Distributions - Part I-1XezzP6kxUE.en

In this video, we'll be defining the term Sampling Distribution, and looking at an example of one specific sampling distribution. A sampling distribution is the distribution of a statistic. This could be any statistic, but what does it really mean, to look at the distribution of a statistic? Consider again the coffee drinking habits of all Udacity students. Let's say, each of these cups represents a student, and a green cup represents a student that drinks coffee, while a red cup represents a student that doesn't drink coffee. And even though there are more students than we represent here, pretend that this represents all Udacity students. If we were to select this group of students to ask about their coffee drinking habits, what would our population, parameter, sample and statistic be? Use the quiz below to answer. You may want to pause the screen here to return and check your answers.

### 07. Example of Sampling Distributions - Part II-PKf3Nu6zAxM.en

If we wanted to identify the sample and statistic, from this visual, we would only use these cups, which give a sample of five students where four of them don't drink coffee. This gives us a statistic that 20% of students drink coffee. Remember, a population is our entire group of interest. Therefore, we have a population of 21 students, and a portion that drink coffee of 71%. Moving back to our sample and statistic, what if we didn't select these five individuals, but instead we selected these five here? How would you now answer the questions regarding the population, parameter, sample and statistic?

### 08. Example of Sampling Distributions - Part 3-E_4lvTWkSNI.en

So you probably notice that though our sample is still five students, our statistic changed, because we chose five different students than were chosen in the first sample. We could select all possible combinations of five cups, and we could recompute the proportion of coffee drinkers for each of these samples. If we were to look at how these statistics change from one sample to the next, that is, if we looked at the distribution of the proportions across all samples of size five, this is what is known as the sampling distribution.

### 11. Introduction To Notation-ISkBSUVH49M.en

It's important to understand notation. You might not even know it, but you use notation all the time. Consider this example of five plus three. Plus is an English word. This symbol is notation, and it's universal. Notation is a common math language used to communicate. Regardless of whether you speak English, Spanish, Greek, or any other language, you can work together using notation as a common language to solve problems. Like learning any new language, notation can be frightening at first. But it's an essential tool for communicating ideas associated with data. We will work together through some examples to assure you completely master this concept.

### 12. Notation Parameters vs. Statistics-webref_dLrA.en

There are common ways to notate parameters that are different than the way we notate statistics. In general, parameters are notated with Greek symbols, where statistics are either notated by lower case letters or the same Greek symbol with a hat on it. Here, this symbol called mu represents the mean of a population, while these represent the mean of a sample. Similarly, this Greek symbol called sigma represents the standard deviation of a population, while either of these can be used to represent the standard deviation of a sample. This same pattern continues. Here, you can see common parameters and statistics. You can see the parameter and statistic values for not only the mean and standard deviation but also for the variance, proportions, and the values of coefficients and regression. You'll see all of these in later lessons in the statistics class. This notation will be consistently used throughout these lessons, as well as beyond this course, in books, blogs, and other use cases.

### 14. Other Sampling Distributions-Bxl0DonzX8c.en

You now have seen how a sampling distribution provides how a statistic varies. As we saw, the proportions change with different samples of students. However, we might also look at the distribution of other statistics, like how the sample standard deviation, the variance, difference in mean or any other statistic varies from one sample to the next. Notice, we are not looking at the distribution parameters. As you saw in the first example, the parameter that is a numeric summary of a population is a fixed value, so these values do not change. However, statistics will change based on a sample you select from the population. So what are the common traits of sampling distributions??

### 15. Two Useful Theorems-jQ5i7CALdRQ.en

There are two mathematical theorems that are commonly discussed when looking at sampling distributions, the Law of Large Numbers and the Central Limit Theorem. First, let's go through the Law of Large Numbers. This theorem makes a lot of sense and it tells us that if we choose the right statistics to estimate a parameter, the larger our sample size, the closer the statistic gets to the parameter. Makes a lot of sense, right? If you want to know the mean of a population and you estimate it with the sample mean, the larger the sample size, the better your sample mean will be, estimating the population mean. So you might be wondering, what makes a statistic the right estimate of a parameter? There are a number of ways to estimate parameters. In the instructor notes below, I've provided links to some of the most popular methods.

### 17. Two Useful Theorems - Central Limit Theorem-L79u8ywRmG8.en

The second theorem is one of the most popular theorems in all of statistics. And it pertains specifically to the sample mean and sample proportions statistics. The central limit theorem states, that with a large enough sample size, the sampling distribution of the mean will be normally distributed, since a proportion is like a mean of zero and one data values, it also abides by the central limit theorem. To give you some practice with this, work through the notebook and quiz questions that follow.

### 20. When Does the CLT Not Work-uZGTVUEMfrU.en

You now have gained some intuition for how the Central Limit Theorem works. But it doesn't work for all sampling distributions. Sure, a mean and proportion are normally distributed with large enough sample sizes. But what does it mean for a sample size to be large enough? Is a sample size of 10 large enough? Or 30? Or 100? You might memorize a list of statistics for which the Central Limit Theorem applies. It applies for the mean, and for proportions, and applies for the difference in means, and difference in proportions. However, the Central Limit Theorem doesn't apply to the sampling distribution for the variance, or for the correlation coefficient. It doesn't apply for the sampling distribution of the maximum value in a data set. With all the data being collected in the world, and tons of compute power available at our fingertips, the Central Limit Theorem is still useful for some sampling distributions. But instead of relying on this theorem, what if we just simulated whatever sampling distribution we were interested in instead?

### 22. Bootstrapping-42j3YclcZ4Q.en

So in the last video, we talked about how relying on mathematical theorems, like the central limit theorem leads to gaps in whether we've achieved a large enough sample size or, which statistics the theorem applies to, and that instead of relying on theorems we could simulate the sampling distribution. This introduces a technique known as bootstrapping. Bootstrapping in statistics, means sampling with replacement. If we want to bootstrap sample five individuals from this group, we could randomly sample these five here. However, in bootstrap sampling, we are sampling with replacement. So we could actually end up sampling these four, and this one might end up being sampled again. We could look at this a bit slower. In sampling, we might end up choosing this individual. And immediately after choosing it as a part of the sample, it goes back to potentially be chosen again. It is possible, though unlikely, in bootstrap sampling to choose the same individual for every random draw. We could end up choosing this individual or any of these five times in a row. After a little more practice with this, you'll be ready to apply bootstrapping to simulate sampling distributions.

### 23. Bootstrapping  the Central Limit Theorem-GJGUwNr_82s.en

Here's the idea of using bootstrapping to simulate the sampling distribution for any statistic. This might take more than one time through to fully grasp. So bear with me. We know that in inferential statistics, we want to use a statistic to try and say something about the corresponding population parameter. Imagine we treat our sample as if it were the entire population. So although these 21 cups represent only a sample of Udacity students, imagine we treat them as though they were in our entire population. If these 21 individuals are truly representative of our population, we can bootstrap sample from them to understand how the proportion of coffee drinkers might change from one sample to the next. Here, I set up an array of 21 ones and zeroes to represent the cups and the previous image. I also set the seed, in case you want to follow along and get the same results. We can calculate the proportion of coffee drinkers in our original sample. Now, let's take our first bootstrap sample of the same size as the original sample and calculate the proportion of coffee drinkers. Notice, this proportion doesn't match the original because we're not just sampling the original observations again but rather, we're sampling them with replacement. We could write a loop to reform the same sampling 10,000 times to see how the proportion will change. Finally, let's plot the proportions to look at the sampling distribution. Use your sampling distribution results to answer the questions in the next concept.

### 25. Background Of Bootstrapping-6Vg5kGoDl7k.en

If bootstrap sampling seems pretty amazing, that's because it kind of is. But the application of bootstrap sampling actually goes beyond even the use cases here. Bootstrapping techniques have been used for leading machine learning algorithms. More on this as provided in the instructor notes below. This technique is credited to Bradley Efron in 1979, a Minnesota born statistician. The name bootstrapping was given to the technique because of the amazement and how well it works is similar to the idea of being able to walk into quicksand and pull yourself out of the quicksand to safety by your own bootstraps. The way we can draw inference about a population parameter by only performing repeated sampling within our existing sample is just as amazing. We actually gain confidence about where parameter is likely to exist without having to collect any additional data. This is an amazing concept and it's wowed people for years.

### 26. Why Are Sampling Distributions Important-aDFDOCJKoH0.en

In this lesson, you've looked a lot at sampling distributions. We might still not understand the use of this idea in practice. In the next lessons, you're going to learn more about inference. Specifically, you'll be learning about Confidence Intervals and Hypothesis Testing. When looking at these techniques online, you might find a lot of formulas and built-in calculators for computing the final results for these techniques. However, these built-in calculators hide the assumptions and potential biases. With your new understanding of sampling distributions and bootstrapping, you're ready to tackle not only using the built-in techniques, but to extend these techniques to a multitude of other situations.

## Part 06-Module 01-Lesson 11_Confidence Intervals

### 01. Confidence Intervals Introduction-crleT4000ak.en

In this lesson, you'll be learning about confidence intervals. When we try to estimate a population parameter based on a sample statistic, this is like fishing with a fishing pole. And there's nothing wrong with fishing this way. It's a perfectly good way to fish. However, you'd be much more likely to catch a fish if you cast an entire net. How much more likely are you that you'll catch a fish? Well, that depends on the size of the net. The larger the net, the more confident you can be that you'll actually capture a fish. This net is much more likely to catch a fish than the smaller net. Though this example isn't exactly the same as building a confidence interval, because we consider a parameter to be a non-moving number and not a moving fish amongst water. This idea that providing an interval and not just a single estimate will help us gain confidence in our ability to capture a population parameter is at the core of confidence intervals. And the wider our interval, the more confident we can be that we capture our parameter of interest. Let's take a closer look.

### 02. Sampling To Distributions To Confidence Intervals-QYMLkDToigc.en

In the previous lesson, you saw how we can do sampling distributions and bootstrapping to understand the values of a statistic that are possible. It turns out that we can use these sampling distributions to understand the most likely values for a parameter as well. In the real world, we don't usually know the value of a parameter as our populations of interest tend to be things like everyone in the world or all the past, present, and future transactions of a company. We just don't have all the information we might like to about these populations. So now, the question is, how do we use what we do know about sampling distributions to infer something about these parameters for these large populations? And here is where the interesting part starts. Imagine this distribution here is the sampling distribution for some statistic, any statistic of interest. We can actually use this sampling distribution to build a confidence interval for our parameter of interest. If we want a 95 percent confidence interval, we could cut off two and a half percent from here and another two and a half percent from up here. These values would then give the range where we believe the parameter would be, with 95 percent confidence. If alternatively, we wanted a 99 percent confidence interval, we would cut off a half a percent from each side, and these values would then give the range where we believe the parameter to be with 99 percent confidence.

### 03. Sampling Distributions  Confidence Intervals-gICzUhMVymo.en

Let's work together through an example of building a confidence interval. Specifically, we'll be looking at how heights range based on coffee drinking habits and based on age. Here, I have two datasets, which I have labeled coffee full, and coffee red for coffee reduced. Notice that the second is just a subset of the first. The csv is linked in the resources so you can follow along, as well as a workspace on the next concept. I set the seed, which you can do to follow exactly my steps. Consider that the larger dataset is all of the individuals in our population. While the smaller dataset is just a random sample that we selected. In the real world, we wouldn't necessarily have all the information in the full dataset, but it will be useful for some of the examples we'll be working through. Let's calculate some statistics about the individuals who drink coffee in our sample. We can see that we have approximately 57 percent of individuals that drink coffee in our sample, and we can see the average height of these individuals is approximately 68.52 inches. Now, let's boot strap from the sample to build a confidence interval. Remember, we don't have the population data to work with. So, we cannot use that to build the interval. By cutting off the bottom two and a half and the top two and a half percent, we built 95 percent in the middle portion. We can interpret these values as the bounds where we believe the mean height of all coffee drinkers in the population to be, with 95 percent confidence, between 68.06 and 68.97 inches tall. Let's go back and see what the population mean actually was. Looks like in this case, we were successful using our confidence interval. But that's not always the case. Your turn. Use the notebook and prompts in the next concept to answer the quiz question that follow.

### 05. Confidence Interval for a Difference In Means-8hrWGzjyhck.en

Now that you've seen how you can use sampling distributions to build confidence intervals for a single parameter, let's take a look at how we might do something similar to estimate the difference in two parameters. Here, we might have a question. What is the difference in average heights for those who drink coffee versus those who do not? Do we have evidence of a difference in the average height? In order to build a confidence interval for the difference in the average heights for these two groups, we can do something similar to what you've already done. But for each iteration of taking the mean for each group, we're also going to take the difference. Now, we can iterate this process some large number of times and use the resulting differences to build the confidence interval for the difference in the means. Here, I've set up a difference list and we'll append the differences into it. We could again plot the difference in the means of the two groups and we could cut off the bottom two and a half and top two and a half percent, to build the 95 percent confidence interval for where we believed the difference in the means of the two groups to exist. In this case, you saw that our confidence interval doesn't contain zero. And therefore, this would suggest that there is a difference in the population means. Further, we would suggest that on average, coffee drinkers are actually taller than non-coffee drinkers.

### 07. Confidence Intervals Applications-C0wgmeRx9yE.en

In the previous concepts, you saw how we could build confidence intervals for different parameters, like the mean or difference in means. So what are some of the scenarios for which we would want to build a confidence interval for the difference in means? Well, we could look at the effectiveness of different drugs by comparing two groups who take two different drugs, or comparing a group that takes a drug to a group that did not take a drug at all. This type of testing is common for any of the health conditions around the world. Another use case specific to learning could be to implement two different ways of teaching the same topic and see which way improves retention. One of the most common use cases for comparing two groups in this way is known as A/B testing, where we compared different webpages to one another to determine which web designs drive the largest amount of traffic. The last topic will be covered in more detail in a later lesson, as it's the key to survival for many online companies.

### 08. Statistical vs. Practical Differences-RKHD1wzxxPA.en

You're now been introduced to a few different applications for confidence intervals, as well as a bootstrapping approach to creating these confidence intervals. We found that creating a confidence interval for the differences in means of two groups, rather than just comparing two point value estimates, is important to assure that the differences are not just occurring due to the randomness associated with the sample that was chosen. Confidence intervals are certainly useful. But there are issues that can arise when exclusively using confidence intervals to make decisions. This brings up the ideas of Practical and Statistical Significance. Let's consider an example to illustrate the difference between these two terms. Imagine that I own a dog walking business and I'm advertising online. I would like to know which of these two ads would help me generate the most interest in my business. I send out each ad to the same number of randomly selected users and I build the confidence interval, that suggests that more people click this ad. Based on my confidence interval, I have statistical evidence to suggest that the second ad is better. This evidence we get from the confidence interval is just this, Statistical Significance. Let's say that both ads generate enough interest to achieve more dogs than I can even handle watching. And that the second ad is much more expensive and time-consuming to create. If a friend of yours decides that they would like to build a similar ad campaign to start their own dog walking business, which type of ad would you recommend to them? In this case, you might suggest something like the first ad, even though you statistically prove that the second ad was better. The first ad will generate enough interest in their business and be less expensive and time intensive. Your suggestion for them to use the first ad is an example of Practical Significance. Practical significance takes into consideration additional aspects and the world around us, rather than just the numbers. And it's critically important to making your decisions.

### 10. Traditional Confidence Interval Methods-DmZwYHuz2eM.en

The way we've been building confidence intervals thus far is based on bootstrapping and our knowledge of sampling distributions. This is an extremely effective method for building confidence intervals for essentially any parameters we might be interested in. However, if you've taken a statistics course elsewhere, you might be confused as to why this notation is different than what you've done in those other courses. You might have seen equations like these for capturing a population mean or proportion. Similarly, you might have seen equations like these for capturing the difference in means or the difference in proportions. All of these formulas have underlying assumptions that may or may not be true. If you truly believe that your data are representative of your population of interest, the bootstrapping method should provide a better representation for where the parameter is likely to be. However, with large enough sample sizes, these formulas should provide very similar results to those that we've seen in bootstrapping methods. In the next concept, we'll take a look at a few examples.

### 11. Traditional vs. Bootstrapping Confidence Intervals-eZ8lyiumXDY.en

In this video, we're going to do a quick comparison of the traditional approaches for building confidence intervals, which are actually built in the Python, and the approaches you've already seen using bootstrapping in this lesson. There are lots of different names for hypothesis tests and the way that we build confidence intervals. Like a one sample T- test, which is used for the population mean, or two sample T- test, which is used for comparing two means. There is also a paired T- test often used for comparing an individual to themselves, or a Z- test, or a chi-square test, or an F-test. There are so many hypothesis tests which are linked to the way that we create the confidence intervals. And the bootstrapping approach can actually be used in place of any of these. In this video, we will illustrate this by example. First, let's read in our data and the libraries. I've also set the seed so you can follow along. Next, let's look at a confidence interval for the difference in means. Here's the bootstrap approach that you did in an earlier quiz. While that runs, I'm going to go look at StackOverflow for a post that could be helpful for finding the difference in means. By finding confidence interval for a T-test, difference in comparison in means, I came across the stack overflow post. If we scroll down, you can see that this is the documentation that they used to get their confidence interval. So we already read in Numpy, I'm going to pull this part out. And here, you can see they're just setting up some random data to work with. So let's actually pull that. We don't need it. We want our data to be a comparison of those who drink coffee and the heights who don't drink coffee. So we just want the heights of each of those groups, so actually, it's going to look a lot like this. But instead of a bootstrap sample, we just want it from our original sample set. And this print statement will only work in 2.7. Notice that intervals for the bootstrapping method and the built in using the traditional method are nearly identical.

### 12. Other Language Associated With Confidence Intervals-9KYVRx7-llg.en

In this video, you'll learn a few more terms associated with confidence intervals, as well as some of the relationships you can expect between what happens to the inputs of a confidence interval and how this affects the interval you get as a result. There are a few terms that we should discuss that are true for any confidence interval. In the literature, you will frequently see terms like the margin of error and the confidence interval width. A common way that we see political results is in the following way. Candidate A has 34 percent of the vote plus or minus 3 percent. And Candidate B has 22 percent of the vote plus or minus 3 percent. Then in small print, you might see something like, "These figures are based on the 95 percent confidence interval." In the sample, each candidate has this respective 34 and 22 percent, and this 3 percent is called the margin of error. In order to build a confidence interval, we actually add and subtract this amount. So our confidence interval for the true proportion that Candidate A controls for the population is at 31 to 37 percent, and for Candidate B, it would be 19 to 25 percent. If this confidence interval for each is larger than we want it, we could actually collect a larger sample size. Because of the Law of Large Numbers, we know that the larger the sample size, the better our estimates will be at approximating our parameter and therefore, this will narrow our interval.

### 14. Correct Interpretations of Confidence Intervals-IhYv_SlN7e8.en

You've already learned a ton about how to build, interpret, and use confidence intervals in practice. This video is just a reminder on the types of conclusions we can make with confidence intervals and the types of conclusions we cannot make, which are commonly confused. When we build confidence intervals, they're aimed at parameters. That is, they're aimed at a single numeric value in our population. These values include the population mean, or the population standard deviation, potentially the difference in two population means, or any other numeric summary in the population. Notice that confidence intervals do not allow us to say something specific about any individual in our population. More advanced techniques in Machine Learning do aim at giving us information about every individual in a population, but commonly, confidence intervals are not aimed at solving these types of problems. Confidence intervals are commonly aimed at telling you about aggregate values in a population.

### 16. Confidence Intervals And Hypothesis Tests-T2d9AUnWl-I.en

This concludes this lesson on confidence intervals. You've learned a ton about how you can build and interpret confidence intervals. In the next lesson, you will learn another technique that's very similar to confidence intervals called hypothesis testing. The topics of confidence intervals and hypothesis testing essentially do the same thing. But depending on who you talk to or what source you're reading from, it's important to understand both. With that, check out the recap on this lesson on the next concept and let's get ready for hypothesis testing.



## Part 06-Module 01-Lesson 12_Hypothesis Testing

### 01. Hypothesis Testing Introduction-Qi6F2rJAmrA.en

Welcome to this lesson on hypothesis testing. Hypothesis testing is one of my favorite topics, because it really does bring an art to the way that we think about statistics. This can bring a lot of complexity. And you may need to watch videos in the section multiple times, or practice the quizzes multiple times. But this is a really rewarding subject to master.

### 02. Hypothesis Testing-9GbHHpiK6wk.en

Academic and industry professionals have questions about, well, just about everything. As data analysts, we try to help them answer these questions. But first, we need to translate the questions into what are known as hypothesis. Then, we need to be able to collect data to justify which hypothesis are likely to be true. As an example, just the other day, I was in a debate with a friend, about what the most popular ice cream flavor in the world is? Where I assumed, the most popular was chocolate, and they were sure it was vanilla. In this case, we could generate hypothesis where the most favorite ice cream is chocolate, and collect data to see if this hypothesis is actually supported by the data. But how can we truly know this unless we talk to everyone? How do we know if our conclusions are reliable? Well, it turns out, you can use hypothesis testing, or confidence intervals which you just saw in the last lesson, to draw conclusions about a population, only using sample data. Not all hypothesis testing is this straightforward. Within medical studies, say I wanted to test if a cancer drug is effective at helping patients. Well now, the hypothesis could go lots of different ways. Is the drug helpful if it makes patients feel better? Is it helpful if they live longer? Is it helpful if it reduces the tumor sizes? In this lesson, you will learn about how to set up and evaluate the results of hypothesis testing. Hypothesis testing is all about helping businesses make better and more informed database decisions. So let's get started.

### 03. Setting Up Hypotheses - Part I-NpZxJg4S6X4.en

When performing hypothesis testing, the first thing we need to do is translate a question into two competing hypotheses. One of these hypotheses is called the Null, and it's associated with the symbol, and the other is called the Alternative and it's commonly notated in this way. Setting up these hypotheses can be a bit subjective, but here are a few general rules that will go through. The null hypothesis is the condition we believe to be true before we collect any data. Mathematically, the null hypothesis is commonly a statement of two groups being equal or of an effect being zero. The null and the alternative hypotheses should be competing and non overlapping hypotheses. The alternative hypothesis is often associated with what you want or what you want to prove to be true. Mathematically, the null hypothesis tends to hold an equal sign while the alternative holds a greater than, less than, or not equal to sign. We will go through a couple of examples to connect these ideas to. In the US judicial system we say "Innocent until proven guilty." This is actually a setup for a hypothesis test. In a judicial case, every individual is either innocent or guilty of an act. This statement of "Innocent until proven guilty" is a statement that says "We believe everyone to be innocent initially" that is the null hypothesis for every individual is innocent. It is this statement we believe to be true before we collect any data. Therefore, the competing alternative hypothesis is that an individual is guilty. We then collect evidence or data and use this data to see which hypothesis is supported.

### 05. Setting Up Hypotheses - Part II-nByvHz77GiA.en

In the previous video, you were introduced to the terms of the null and alternative hypotheses and came up with a few guidelines for setting these up as shown here. You also saw how the claim of innocent until proven guilty relates to the null and alternative hypotheses. In this video, I want to provide a second example. Imagine you create a new web page layout and we'd like to know if this new page drives more traffic than the existing page. We could set this up to specifically ask, "Does the average web traffic increase with the new web page as compared to the existing page?" Before we even implement this test, we might hope that the new page is better. That's why we built it after all. But we need to prove this. This is an indication that the new page being better than the existing page belongs in the alternative hypothesis. Therefore, the null would be that the average web traffic is the same for each of these groups or that the old page is actually better. Mathematically, we could set this up in this way, where the average traffic for the existing page is equal to the average traffic for the new page. Then, the alternative would look like this, where the average traffic for the new page is greater than the average traffic for the existing page. And again, we can collect data to see which hypothesis is supported. Here, our guidelines served very useful in defining the null and alternative hypotheses. Setting up hypothesis tests can be tricky because there isn't really one right answer. In the following concepts, you will see why this really matters for our decision-making process. For now, use this example along with these four guidelines to get some practice setting up hypotheses.

### 07. Types Of Errors - Part I-aw6GMxIvENc.en

Now that you've had some practice with setting up hypotheses, you might be asking yourself, why does all this null and alternative stuff really matter? Well, it actually matters a lot. Let's consider again the judicial example from before. There are four potential reality decision combos that could be made. In order to look at each one of these outcomes, consider this grid where we have the truth of whether or not someone is innocent or guilty represented by this axis, and then the decision made by the jury represented as either innocent or guilty on this axis here. This creates a grid of four potential outcomes. In this corner, the truth is that an individual is innocent, and the jury also believes that individual to be innocent. While in this corner, the truth is that the individual is guilty, and the jury believes them to be guilty. In either of these other two, the jury has made a mistake as the truth doesn't match the jury decision. This provides us with two potential errors that are possible in hypothesis testing. The first error that is possible is that the jury might consider an individual innocent when they are truly guilty. This error sets guilty people free. The second type of error is that the jury considers an individual guilty when the truth is that they're innocent. This error punishes innocent people.

### 09. Types Of Errors - Part II-mbdSQ5CjdFs.en

From the quizzes, you've now been introduced to two types of errors. Type one and type two errors. Correctly setting up the null and alternative hypotheses is important for exactly this reason. They define the importance of the errors that we're making. In the previous example, a type one error is one where we decided that an individual is guilty, but they're actually innocent. The definition of a type one error is an error where the alternative hypothesis is chosen, but the null hypothesis is actually true. You might also hear this called, a false-positive, and it is frequently denoted with a symbol alpha. Type one errors are considered the worse of the two possible errors. The other type of error that might occur, is that we might set a guilty individual free. This is a type two error. By definition, a type two error is when the null hypothesis is chosen, and the alternative is actually true. What we can see in this example is that there are two potential extremes. The jury might decide that they never want to commit a type one error, in which case, regardless of the evidence, they just always set everyone as innocent, and then they're going to commit many more type two errors. Alternatively, if the jury just decided that everyone was guilty, then they would never commit a type two error, but they would commit many more type one errors. Because of this relationship between type one and type two errors, professionals frequently just set a threshold for how many type one errors they're willing to commit. And then, they try to keep the type two errors as low as possible while still meeting this threshold. Common type one error rates are one percent from medical field, while they're five percent for research journals and other business applications. But really, this rate should be contingent on your application. In the next concept, you will see one application where neither five percent nor a one percent error rate makes very much sense.

### 12. Types Of Errors - Part III-Z-srkCPsdaM.en

Hopefully, you're starting to feel more comfortable identifying hypotheses and the types of errors that you can make. In this video, we'll go through one more example to iterate on these ideas. This example is one that really helped me connect all the dots of hypotheses testing and type I and type II errors. I hope it does the same for you. Imagine you own a skydiving shop. And as a part of your job, you must check parachutes to assure they worked correctly. There are two potential outcomes, either a parachute works or it doesn't. You can create these as our two potential hypotheses. You know that there are four potential outcomes for each skydiver-parachute combination. First, you check each parachute to make a decision. Either the parachute works or it doesn't. If you determine that it works, you put it on the shelf for skydivers to use. If it doesn't, you throw it out. Now, there are two possible truths, either the parachute actually works or it doesn't. For the parachute that we threw out, if it doesn't work, that's great, but if it did, then we probably lost like 30 bucks. Now, for the parachute we put on the shelf, if it works, then the skydiver uses it to jump out of a plane and land safely on the ground. However, if it doesn't, we clearly just committed the worst type of error possible. This should clearly be the type I error in this example. So, the other type of error is the type II error. This helps us align our null and alternative hypotheses to look like this. As we know that type I error rate should be choosing the alternative when the null is true. And it helps us align an appropriate type I error rate as one or five percent as entirely too high. Committing five or even one of these errors out of every 100 individuals would just be unacceptable.

### 14. Common Types of Hypothesis Tests-8hv8KnvQ6JY.en

Now that you've had a chance to gain some familiarity with setting up hypothesis test, let's look at some of the most common tests that are done in practice. One common test is to test the mean or proportion of a population being equal to some value. For example, in finance, we could have the question of, if you can expect a return greater than six percent on an investment. So we could set up the hypothesis test in the following way, where the null is that you earn less than or equal to six percent, and the alternative is that you earn greater than six percent. Another common hypothesis test is to ask which of two marketing campaigns might drive more traffic to our website? In which case, we could set up a null and alternative that looks like this, where the null suggests that the proportion of individuals that click through to our page is the same for each campaign, and the alternative is that one page drives more traffic. In which case, the proportion of traffic is different from one page to the next. If we really wanted to test if a new campaign was better than an old campaign, we might use a one sided hypothesis test like this one, where the greater than here, suggests that the proportion of individuals that travel to our site will be higher for the new campaign. You can do some algebra to change the exact same logic to look like this. Notice that all of these hypothesis tests are regarding parameters. They are not about statistics. This is always the case. There is no need to do hypothesis testing on statistics as they are exact values in our data set. Our questions are about the entire population and therefore, so are our hypotheses.

### 16. How Do We Choose Between Hypotheses-JkXTwS-5Daw.en

Once we set up our null and alternative hypotheses, we need used data to figure out which hypothesis we actually think is more likely to be true. And there are two ways we might approach choosing one of these hypotheses. One is the approach that we saw using confidence intervals, where we simulate the sampling distribution of our statistic, and then we could see if our hypothesis is consistent with what we observe in the sampling distribution. The second way we could approach choosing a hypothesis is simulating what we believe to be possible under the null and then seeing if our data is actually consistent with that. This is what professionals tend to do in a hypothesis testing.

### 16. Using A Confidence Interval to Make A Decision-MghT95b6LbQ.en

I have attached the data for this example in the Resources tab. And you also can find it in the works pace on the next concept of our work through. This is the same coffee data you saw in the previous lesson on confidence intervals. So you might already have it saved on your local as well. What if we wanted to ask the question of if the average height for all coffee drinkers is greater than 70 inches. We could set up our known alternative hypothesis in the following way. Here, we have that the average height of all coffee drinkers is less than or equal to 70 in the null while in this statement we have that the average height is greater than 70 in the alternative. Notice, we are always testing a parameter. So I'm using new here to represent the mean of all coffee drinkers. Based on what we just did with confidence intervals, you can imagine a very intuitive approach for determining if the null is possible, is just to bootstrap a sample set of data and compute the sample mean again and again. And build the sampling distribution and corresponding confidence interval to determine what are the reasonable values for the population mean with some level of confidence. Let's put this to practice, imagine from our dataset we achieve this sample. Then we can bootstrap this in the following way. Now, let's bootstrap a number of times and compute the mean for each bootstrap sample. Here, we have our bootstrap sample, and here, I've created an empty vector of means that we're going to append each of our bootstrap means into. Now, we have all of our means and we can create our confidence interval. Here, I have the lower bound, and here is the upper bound. Additionally, we might choose to plot those. Here's what it looks like.

### 17. Simulating From the Null-sL2yJtHZd8Y.en

Consider the same example as earlier, where we asked if the mean height for all coffee drinkers was greater than 70 inches. We could again set up a null and alternative hypotheses like these. A second approach that is commonly done for making decisions in hypothesis testing is the following, we assume that the null is true and we know what the sampling distribution would look like if we were to simulate from the closest value under the null to the alternative. That is, this value of 70. That's the closest value under this hypothesis to our alternative hypothesis. We could use the standard deviation of the sampling distribution to determine what the sampling distribution would look like if it came from the null hypothesis. We'll simulate from a normal distribution in this case. I'm going to pull over the code that we used before to get the standard deviation of our sampling distribution. So the standard deviation of our sampling distribution is equal to 0.2658. And we know that if it came from this null hypothesized value of 70 what it would look like. By the central limit theorem, we know that it would follow a normal distribution. Now, from the NumPy documentation on normal distributions, we see we can simulate draws from the normal using the hypothesized mean at 70 and the standard deviation of our sampling distribution in the following way. So here, the loc tells us that the mean of 70 will go in this value, and the scale is the standard deviation that we want to use. So that's the standard deviation of our sampling distribution. And we can simulate, say, 10,000 values from that. Each of the simulated draws here represents a possible mean from the null hypothesis. We can now ask the question of where the sample mean falls in this distribution. If we go back and look at what our sample mean was, we can see that it falls far below this distribution from the norm. If our sample mean were to fall closer to the center value of 70, it would be a value that we would expect from the null hypothesis and therefore, we think the null is more likely to be true. In this case, with our sample means so far out in the tail, it's far enough that we don't think it probably came from this null hypothesized value.

### 19. What Is A P-value Anyway-eU6pUZjqviA.en

In the previous video, you saw two methods for how we might choose between competing hypotheses. In the second method, we ask the question if the null hypothesis is true, what is the probability of obtaining the statistic we observed in our data or one more extreme in favor of the alternative hypothesis? This probability is what is called a P-value. Finding the P-value involves a mix of ideas that you've learned about, sampling distributions and conditional probability. Imagine we have a null that a population mean is equal to zero. Then we collect sample data and we find the sample mean to be five and the sample standard deviation to be two. Assuming the sampling distribution of your statistic follows the null hypothesis. What is the probability of observing the actual value of the statistic from your data in this distribution? If we want to know the probability that the population mean is actually greater than zero, you could update the hypothesis. Values like 6, 7, 10 all fall out here and are even more of an indication that the alternative is true, that our population mean is greater than zero. Additionally, what is the probability you observed something even more in favor of the alternative hypothesis? This shaded region here provides that probability, the P-value. Notice the P-value is dependent on your alternative hypothesis as it determines what is considered more extreme. If your alternative hypothesis is that your parameter is greater than zero, then we shade greater than the statistic as shown here. However, if we change our null and alternative to look like this, then we shade to the left of our statistic. Our shading for the P-value would now look like this. Notice that this would create a very large probability in this case as almost the entire distribution is shaded. And if you have a not equal sign in the alternative, then your shading is associated with extremes that are just far from the null in either direction. In these cases, we just care about statistics that are far from the null in either direction. So we end up marking on both sides and shading away from the null hypothesis to find our P-value. This often takes people a while to wrap their heads around. And there are two parts to understand. First, you have to fully conceptualize the definition of a P-value, which is the conditional probability of your data given that the null hypothesis is true. Then, you need to figure out what to compute, which is visually summarized by these three images. You will get some practice with both of these before moving on and applying the ideas of P-value to make decisions.

### 20. Calculating the p-value-_W3Jg7jQ8jI.en

In the last programming video, you saw how we could simulate draws from the null hypothesis, and that if our statistic was in the bulk of the distribution, this suggested that the statistic was likely from that null. However, if the statistic was farther out from the bulk of the distribution, this suggested that the null wasn't likely to have generated our statistic. Then you saw that we can calculate p-values based on the shaded region starting at the value of our observed statistic through the tail of the distribution, where the shaded region is dependent on our alternative. Based on the previous video, you had simulated values of the sampling distribution from the null, like this. Imagine that we have the alternative hypothesis that the population mean is greater than 70, then we could calculate the p-value as the proportion of the simulated draws that are larger than our sample mean. Here, you can see that that would give us a p-value of one. Remember, large p-value suggests that we shouldn't move away from the null hypothesis. In this case, that suggests that we should stay with the mean being less than 70. Here, we've calculated the null values that are greater than our sample mean. Since this is one, our p-value is large and therefore we wouldn't move away from the null hypothesis that suggests that our population mean is truly less than or equal to 70. If our new null and alternative hypotheses look like this instead, we would calculate our p-value a little differently. Here, because our alternative is less than 70, we would now look at the shaded region to the left of our statistic. This would change our p-value to the following. Now that our p-value is zero, this suggests that we reject the null hypothesis in favor of an alternative, suggesting that the population mean is less than 70. So if this was our null and alternative hypothesis, we would now want to look at the values that are more extreme than our sample mean in either direction away from the null hypothesis. That looks like this equation here. Let's take a look at where these values fall on the above histogram. You can see that if we were to shade more extreme than either of these regions, there are essentially no data points from our null hypothesis that fall outside of this region. Again, we would have evidence to suggest that the null hypothesized value did not generate our sample statistic.

### 23. Connecting Errors and P-Values-hFNjd5l9CLs.en

In the last video, we saw that a p-value is the probability of obtaining our data or more extreme values from the null hypothesis. So how does this connect to making decisions and the types of errors that we can make? If the p-value is really small, this suggests it's less likely to observe our statistic from the null. And it's more likely that it came from the alternative. But how small does the p-value need to be before we no longer stay with the null hypothesis? Well, I guess that depends on how willing you are to make certain types of errors. If you are willing to commit five percent of errors, where you choose the alternative incorrectly, then your p-value needs to be smaller than this threshold in order to choose the alternative. However, if your probability of getting the data from the null is, say, an eight percent chance, this is enough of a chance that you would stay with the null under a five percent type one error threshold. The fast rule is that if our p-value is less than a type one error rate, then professionals say we reject the null. That is, we choose the alternative hypothesis. If the p-value is greater than our type one error rate, then we fail to reject the null. That is, we stay with the null hypothesis as our decision. What's actually true? We can't really be sure in practice, but we now have a database way to make our decisions.

### 24. Conclusions In Hypothesis Testing-I0Mo7hcxahY.en

When making a decision about whether you're choosing the null or alternative hypothesis, you might see certain professionals, specifically statisticians, cringe, with concluding remarks like, "So based on the data, we accepted the null hypothesis to be true," or, "Based on the data, we accepted the alternative." Remember, when setting up our null and alternative hypotheses, we automatically set the null to be true before any data were collected. Therefore, this statement is a default. It isn't like we're uncertain about what hypothesis we'll choose and then we choose one, rather, you are by default choosing the null. Going back to the judicial example, everyone is innocent until proven guilty. You don't choose someone as innocent. By default, everyone is innocent. Therefore, in hypothesis testing, we say, "Based on the data, we have evidence to reject the null hypothesis," or alternatively, if we don't have enough evidence to reject the null, we say, "Based on the data, we fail to reject the null hypothesis." Many people will only care about making the right decision. So this distinction might seem just a bit picky. But it stresses that the null is much more likely to be chosen as you start with the statement being true.

### 27. What If Our Sample Is Large-WoTCeSTL1eM.en

When conducting a hypothesis test, there are really two components that you have control over. First, you should be asking yourself, is my sample representative of my population of interest? Are there ways to assure that everyone in my population is accurately represented in my sample? If your sample isn't representative, then your conclusions are likely to be incorrect. Second, you should know the impact of your sample size, and the role that it plays on your results. As your sample size increases, even the smallest differences between the two groups will appear as notable and statistically significant. With the really large sample sizes that are becoming more frequent, as a part of the data world, we are seeing a large change in techniques away from hypothesis tests for exactly this reason. Imagine we are interested in which of two coffee types will sell better on the shelf, and on average, maybe type one sells better than type two. However, there are hundreds, thousands, maybe even millions of individuals who still prefer a different type. A hypothesis test just tells us that, on average, type one will sell more than type two. But with large sample sizes, we should do better than this. Discussing averages leaves out an entire part of the population who preferred type two, or maybe did not care for either of these types, but like a different type. But with large sample sizes, we can do better than this. We can do better than hypothesis testing. And this is what more and more people are finding out. Using machine learning, we can individualize an approach. So maybe, we sell twenty types of coffee and we know what type every member in our population wants. So these large sample sizes prove to be pretty detrimental to a hypothesis test, but they are spectacular for individual-aimed approaches using machine learning techniques. We will get a glimpse of two of these machine learning techniques in the last lessons of this class.

### 28. Multiple Testing Corrections-DuMgeHrkIF0.en

You've learned about the types of errors that are possible on hypothesis testing. And you've learned about how we can create a threshold for how often we allow these to happen. But consider if we run 20 of the exact same types of hypothesis tests. Even if the null is actually true, if we have a five percent type one error rate, we can expect one of these to have results where we choose the alternative. Often, researchers all over the world are performing very similar studies. So when one researcher comes up with significant results, how can we be sure that they are one of these type one errors? Well, the hard part is we can't really know in many cases. And that's a problem. But there are a few methods that statisticians have come up with to assist with this problem. One of the most conservative and common approaches is known as the Bonferroni Correction. This simple correction says that if you're performing m-tests, you should divide your type one error rate by m to assure that you actually maintain the error rate here. So if you really want to have a five percent type one error rate and you're performing 10 hypothesis test, your new threshold is actually 0.5 percent for choosing the alternative hypothesis. This is just one popular correction method. But other methods include the Tukey correction. And in the biomedical field, they commonly use a method called Q-Values. More on these methods is provided in the instructor notes below.

### 29. How Do Confidence Intervals  Hypothesis Tests Compare-KEmsEViOoMA.en

So you saw earlier that hypothesis tests and confidence intervals are pretty similar. But you might be surprised just how similar these two techniques are. It turns out that if you are to do a hypothesis test that contains a not equal in alternative hypothesis, your conclusions are identical to a confidence interval, as long as 100 minus your confidence interval level is the same as the type one error rate in your hypothesis test. That is, if you build a 95 percent confidence interval, this is the same as doing a two-sided hypothesis test with a five percent type one error rate, and building a 99 percent confidence interval will provide the same results as a hypothesis test with a one percent type one error rate. With this in mind, many academic journals are moving away from hypothesis tests as they find the results are often misinterpreted. Instead, more and more confidence intervals and other statistics like effect size or machine learning techniques are being used. There is more information on effect sizes as well as a link to a free course in the Udacity Nanodegree, link in instructor notes below.

### 32. Hypothesis Testing Conclusion-nQFchD4XPPs.en

You now have learned how to set up a null and alternative hypothesis. You have determined type one and type two errors. And you can calculate which hypothesis you should choose based on error threshold. You also have learned about some of the dangers of the conclusions that you might make when having a really large sample size, or if you were to perform more than one hypothesis tests. Finally, you learned how confidence intervals and hypothesis tests are highly related to one another. The ideas associated with hypothesis tests are central to A/B testing and making database business decisions. In the next lesson, you will take a hands-on approach to applying these ideas.



## Part 06-Module 01-Lesson 13_Case Study AB tests

### 01. Case Study Introduction-J5uvdPxHIfs.en

Welcome to this case study on AB test. In this lesson, you all apply what you've learned in the previous lessons, to help a company decide whether to launch two new features on their web site. You'll do this by analyzing results from a widely practiced experiment called AB test. The work you've done so far on confidence intervals and hypothesis testing, will really come in

### 02. AB Testing-EcWvhbIjT9o.en

When companies want to test new features or versions of a web page, they often use a method called A/B testing. The way this works is that one set of users called the control group is shown the old version of a page while another set of users called the experiment group is shown the new version of a page. Based on how both groups respond, we can determine if the new version is better and should be launched. This is actually just an application of hypothesis testing where the null hypothesis would be that the new version is no better or even worse than the old version and the alternative, would be that the new version is better than the old version. A ton of companies use A/B testing to try out changes in features, layouts and even colors to increase a metric that measures interest from their users. For example, you could perform an A/B test to see if a new look for your site's landing page increases the likelihood of a visit or subscribing to your mailing list. Or you could perform an A/B test to see if offering personalized recommendations on a shopping site increases purchases. One thing to be aware of is that although A/B testing can be good to test if a new feature or version of a feature is better on a page, it's not useful for everything. For example, A/B testing can tell you the best way to rank a set of products on a site but it can't tell you that the site would really benefit from having two additional products added to the site. It's also not always great for testing whole new experiences for existing users due to factors like change aversion and novelty effect. Existing users may give an unfair advantage to the older version simply because they're unhappy with a change even if it's ultimately for the better. Alternatively, they may give an unfair advantage to the new version because they're excited or drawn to the change even if it isn't for the better. In addition to these, there are many other factors that can bias the results of an A/B test. You'll learn more about those in this lesson.

### 04. Business Example-Wzz7omSDfEk.en

In this case study, you'll be analyzing AB test results for an online education company called Audacity, that offers courses on finance. Let's first see what a typical user flow might look like on Audacity site. New users would land on the home page, and if they're interested, they would click to explore courses. As they browse the course list, a course may catch their eye, and they click on it to learn more. Once they're on the course overview page, they may decide to enroll. After enrolling, they'd hopefully complete the course. In this flow, commonly called the customer funnel, users lose interest, and leave at different stages of the funnel, and few make it to the end. Of course, this is a simplistic model, and isn't how all user experiences would play out. But it does capture pretty well the main steps from an initial exposure to course completion in order of decreasing probability. To increase student engagement, Audacity wants to perform AB tests to try out changes, at different points in this funnel. In this case study, we'll analyze test results for two particular changes they have in mind, and then make a recommendation on whether they should launch each change.

### 05. Experiment I-JLKAdT2JESk.en

The first change Audacity wants to try is on their home page. They hope that this new, more engaging design will increase the number of users that explore their courses. That is, move on to the second stage of the funnel. Now that we know the change we want to make, we need to choose a metric that measures that change. To measure how many people move on to the next stage, we can track how many people click on the Explore Courses button on the home page with the new design and the old design. However, using just the number of users doesn't make sense if more total users view the page in one version of the experiment. More total clicks could occur in one version even if there is a greater percentage of clicks in the other version. So instead, we could use the fraction of page visitors who clicked. That is the number of clicks on The View Courses button divided by the number of page views to the home page. This metric is commonly called click-through rate or CTR. You could take this metric one step further and make it the number of unique visitors who clicked at least once, divided by the number of unique visitors to view the page. Now that we have our metric, let's set up our known alternative hypothesis. Our alternative hypothesis is what we want to prove to be true. In this case, the new home page design has a higher click-through rate than the old home page design. And the new hypothesis is what we assumed to be true before analyzing any data, which is that the new home page design has a click-through rate that is less than or equal to that of our old home page design. As you've seen before, we can rearrange our hypothesis to look like this.

### 07. Metric - Click Through Rate-EpfoKAwV_Eg.en

As you saw in the last section, this dataset includes view and click actions on the home page of Audacity's site, from users that were shown the control and experimental versions of the A/B test. Our task is to analyze these actions to see if there was a significant difference in performance for the two versions. To do this, let's first compute the click-through rate for each group. Let's start with the control group. We can extract all the actions from the control group like this. Now, to compute the click-through rate, we'll divide the number of unique users who actually click the Explore courses button by the total number of unique users who viewed the page. This gives us a click-through rate of about 28 percent. Let's do the same thing for the experiment group. Again, we'll take all the click actions, get the unique number of users, and divide that number by the number of unique users who viewed the page. That gives us a click-through rate of about 31 percent. So in this sample, the experiment group's click-through rate was higher than the control group's click-through rate by about 3 percent. Now that we know the observed difference in this sample, we have to see if this difference is significant and not just due to chance. Let's bootstrap the sample to simulate the sampling distribution for the difference in proportions. Let's take a look at our sampling distribution. If you remember from the previous lesson, we can compute the p-value for our statistic which is the observed difference in proportions by simulating the distribution under the null hypothesis and then finding the probability that our statistic came from this distribution. To simulate from the null, we'll create a normal distribution centered at zero, with the same standard deviation as our sampling distribution we simulated here. We could see the null distribution here. And this is where our observed statistic falls. We can't find the p-value like this as these are all the null values that are more extreme than our statistic in favor of our alternative. With a p-value of approximately a half of percent, the difference in click-through rates for the control and experiment groups does appear to be significant. We can reject the null hypothesis, and based on these results, it looks like Audacity should launch the new version of the home page.

### 09. Experiment II-fq4eO7CybA4.en

The second change Audacity wants to try is on their course overview page. They created a new description for one of their courses to dedicate larger portions to connecting concepts in the course to career skills, and less on the details of each concept. They hope that this change may encourage more users to complete the course. For this experiment, instead of choosing just a single metric, we're going to analyze multiple. We'll track the enrollment rate, the average reading time on the course page, the average time spent in the classroom, and the course completion rate. First, we'll analyze these individually and then, we'll see how they all come together.

### 11. Metric - Average Reading Duration-w6Y9ZxHDEbw.en

In addition to computing the enrollment rate, we can also compute the average reading durations with this dataset. The two analyses so far were comparing proportions. With this metric, we'll be analyzing the difference in means. This analysis will be quite similar. Since we're comparing reading durations, we only care about view action. So let's filter by that first. And let's only count each unique user once by finding their average reading duration if they visited the site more than once. Well, also group by group, just so we keep track of that information. This isn't necessary, but resetting the index is nice just so we keep the ID and group as column names. And it also let's us continue working in a data frame instead of a multi index series. Now, we can find the average reading durations for each group like this. On average, it looks like users in the experiment group spent 15 more seconds on the course overview page than those in the control group. To see if this difference is significant, let's simulate the sampling distribution for the difference in mean reading durations with bootstrapping. Here's what the sampling distribution looks like. Now to find the P value, let's simulate the distribution under the null and find the probability that our observed statistic came from this distribution. We'll create the distribution centered at zero and they're having the same spread as our sampling distribution. Here's our null distribution. And here is where I observed statistic falls. Our statistic definitely doesn't look like it came from this null distribution. Looks like the difference we observed is significant.

### 14. Analyzing Multiple Metrics-DtZghKNa7Ak.en

As you've seen in previous lessons, the more things that you test, the more likely you are to observe significant differences just by chance. This happens when we run evaluations from multiple metrics at the same time. The probability of any false positive increases as you increase the number of metrics. Luckily, this is something that we can fix.

### 16. Drawing Conclusions-s-4ghG9vrGQ.en

How do we make a recommendation when three out of our four metrics had a significant difference for testing each metric individually, but insignificant differences when we use the Bonferroni correction? The Bonferroni method is a conservative one, and since we expect the metrics to be correlated, this would be best handled by a more sophisticated method that ideally takes this correlation into account. In the instructor notes, there is a list of possible other methods that are less conservative. Should you choose one of these methods, you should ensure that the assumptions of the method are truly met in your scenario. Choosing a poorly suited test that simply provides the results that you want is not only bad practice but will likely lead to misguided decisions that harm your company's performance in the long run.

### 18. Conclusion-qmGjRpMVBz8.en

Great job completing this case study. To summarize, you learned about the uses and values of AB testing, defining metrics that measure changes in your experiments, analyzing results with confidence intervals and hypothesis testing, handling multiple metrics, and common difficulties associated with AB testing. Congratulations again on completing this case study.

## Part 07-Module 01-Lesson 01_Linear Regression

### 01. Welcome To Linear Regression-zxZkTkM34BY.en

Hi I'm Louis. Welcome to the linear regression section of this Nanodegree. The two main families of algorithms and predictive machine learning are classification and regression. Classification answers questions of the form yes-no. For example, is this email spam or not, or is the patient sick or not. Regression answers questions of the form how much. For example, how much does this house cost? Or how many seconds do expect someone to watch this video? In this lesson we'll learn how to answer this last type of questions. First, we will learn linear regression, then some ways to improve it, and finally some ways to generalize it to non-linear cases. Are you ready? Let's go!

### 02. DLND REG 01 Quiz Housing Prices V2-8CSBiVKu35Q.en

So let's say we're studying the housing market and our task is to predict the price of a house given its size. So we have a small house that costs $70,000 and a big house that costs $160,000. We'd like to estimate the price of these medium-sized house over here. So how do we do it? Well, first we put them in a grid where the x-axis represents the size of the house in square feet and the y-axis represents the price of the house. And to help us out, we have collected some previous data in the form of these blue dots. These are other houses that we've looked at and we've recorded their prices with respect to their size. And here we can see the small house is priced at $70,000 and the big one at $160,000. Now it's time for a small quiz. What do you think is the best estimate for the price of the medium house given this data? Would it be $80,000, $120,000 or $190,000? Submit your answer.

### 03. Solution  Housing Prices-uhdTulw9-Nc.en

Well to help us out, we can see that these points can form a line. And we can draw the line that best fits this data. Now on this line, we can see that our best guess for the price of the house is this point here over the line which corresponds to $120000. So if you said $120000, that is correct. This method is known as linear regression. You can think of linear regression as a painter who would look at your data and draw the best fitting line through it. And you may ask, "How do we find this line?" Well, that's what the rest of the section will be about.

### 04. Fitting A Line-gkdoknEEcaI.en

So here's a trick that will help us fit a line through a set of points. Let's say that these are points and we start by drawing some random line. We're going to ask every point what it wants for the model to be better and then listen to them. So let's go one by one, let's take this point over here and ask it what would it want the line to do? Well, the point would want the line to move closer. So, we listen to the point and move closer. Now, if we ask all the points they will tell us the same thing, so we move closer to all of them. Then we'll listen to them again and we take another step that gets closer to all of them, now it's trickier because we have a point underneath that pulls it down but that's still okay. Basically, the idea is that we take a few steps that make us go closer to all the points and that's it, that's linear regression. In the next few videos, we'll see this in a bit more detail.

### 05. Moving A Line-8EIHFyL2Log.en

So, let's have a little refresher on how we move lines by changing the parameters. So, if we have a line of equation y equals w_1x plus w_2, where w_1 and w_2 are constants, it looks like this, w_1 is the slope and w_2 is the y-intercept which is where the line intersects the y-axis. Now, what happens if we increase w_1? Well, we increase the slope so that means the line rotates like this. If we decrease w_1, then we're decreasing the slope so the line rotates like this. Now, what happens if we increase the y-intercept w_2? Then the line moves in a parallel way up like this. And if we decrease w_2, then the line moves like that to the bottom. That's really all we're going to need for the tricks that are coming.

### 06. Absolute Trick-DJWjBAqSkZw.en

So, here's our first trick that we will align closer to a point that we're going to use in a linear regression. It's called the absolute trick and it works like this, we start with a point, and a line, and the idea is that the point wants the line to come closer to it. So, let's put some numbers here, the point has coordinates p comma q, where p is a horizontal coordinate and q is the vertical coordinate. The line has equation y equals w_1x plus w_2. In here w_1 is the slope and w_2 is the y-intercept. So, an easy way to move the line closer to the point pq is to just add one to the y-intercept, and then there is, the line moves up. Now let's add something to the slope two to make the line rotating the direction of the point. This is going to look a little strange but it's going to make sense very soon. So, this distance over here is p because it's the horizontal distance from the y-axis to the point. So let's just add that, let's add p to the slope, and now our new slope is w_1 plus p, and that rotates the line in this direction. Now our new equation is y equals w_1 plus p times x plus w_2 plus one, and that's pretty much what the absolute trick is about. Notice some subtleties though, notice that we moved the line a little too much, and we actually went over the point and kept going, we don't want this, in general in machine learning we never want to take big steps like this. Instead what we want to do is take tiny steps. So, in order to take a tiny step we'll just do the exact same thing we did except we multiply everything by a small number. So let's take a small number called the learning rate, let's say alpha, and instead of adding one to the y-intercept and p to the slope, we will add alpha times one to the y-intercept, and alpha times p to the slope. Now the line moves up by a little bit and rotates a little bit, so we don't have that risk of going too far. Our new equation is going to be w_1 plus p times alpha times x plus w_2 plus alpha. So our new slope is w_1 plus p times alpha and our new y-intercept is w_2 plus alpha. So we're doing better but there's still a little subtlety. What happens if the point is not on top of the line but underneath the line? Well, same thing except now instead of adding we just subtract to get our new equation w_1 minus b times alpha times x plus w_2 minus alpha. The reason is if we subtract alpha to the y-intercept the line moves down instead of up and if we subtract p times alpha to the slope the line rotates in this direction instead. Something more interesting is this, and that will explain the reason p is there. If the point is not on the right of the y-axis but on the left of it, than we still add one to the y-intercept because we need the line to move up, but the fact that now we're adding p to the slope and p is now a negative number means that our line now rotates in this direction, so that's a reason for p to be there. Another reason for p to be there is this, check this out. If this distance is small then p is small, so we're adding a small number to the slope. Now, if the distance is large then p is large so we're adding a large number to the slope. It makes sense that if the point is really close to the y-axis we want to increase the slope by a little bit. Whereas if it's far want to move it by a lot more. So, let's do an example to make this more clear. Let's say we have the point 5 comma 15 and the line y equals 2_x plus three, so this means the distance from the point to the y-axis is five, now let's say our learning rate is 0.1. So, we're adding 0.1 times one to the y-intercept and that moves the line up by a little bit. Also we are taking five multiplying it by 0.1 and adding that to the slope which makes the line move in this direction, this means our new equation is y equals 2.5 x plus 3.1. Now, check out what happens if the point is in the left, we're still adding 0.1 to the y-intercept to move the line up, but now to the slope we're going to add the product of 0.1 and minus five, that's minus 0.5. This means our new equation is going to be 1.5 x plus 3.1. As you can see the slope moved in a different direction which made it go closer to the point. So that's it, that's the absolute trick, and we're going to use it extensively in linear regression.

### 07. Square Trick-AGZEq-yQgRM.en

So here's another trick that will help us move a point closer to a line, and it's very similar to the absolute trick but it has a little bit of extra gravy. It's based on this premise. If we have a point that is close to a line, then this distance is small and we want to move the line very little. But if the point is far from the line, then want to move the line a lot more. The absolute trick that we learned previously does not have this property because if we remember the absolute trick adds alpha to the y-intercept w_2 and p times alpha to the slope w_1. This has nothing to do with how far the point is from the line, since P is just a horizontal distance. So let's just add the vertical distance into this formula. Let's look at this vertical distance between the point and the line. The point over the line has coordinates say,( p,q'). This distance is then q minus q' because q' is the value at the line and q is the value of the y coordinate at the point. So what we do here is very simple. We just take this q minus q' and multiply it into what we're adding to both the y-intercept and to the slope. This will again make the line go up by a bit and rotate in this direction except that now if the point is far from or close to the line, the amount the line moves will be affected and here's our new equation with a factor of q minus q'. Notice that here we get something for free. If the point is underneath the line instead of over the line, then q minus q' is actually a negative value and if this is the case then we're subtracting something from the slope thus rotating the line in this direction instead, still towards the point. So this trick also takes care of points that are under the line and we don't have to have two rules like we had on the absolute trick. We just have one same rule for both. Again, let's clarify this with an example. In this example over here, we have the point (5,15) and notice that this distance is going to be two because the line goes through the (5,13) as two times five plus three is 13. Now to change things a bit, we'll use zero point 0.01 as the learning rate. So the absolute trick would be adding 0.01 to the y-intercept and 0.05 to the slope. But now as our neutral access, we multiply the two numbers two adding to the slope and the y-intercept by two; which means we're adding 0.12 to the slope and 0.02 to the y-intercept. So the equation of our new line is y equals 2.1_x plus 3.02. Notice that that line is going to be closer to the point. So that's it. That's the square trick.

### 08. Gradient Descent-4s4x9h6AN5Y.en

so now that we've learned the absolute trick and the square trick, and how they're used in linear regression, we still want to have some intuition on how these things get figured out. These tricks still seem a little too magical, and we'd like to find their origin so let's do this in a much more formal way. Let's say we have our points on our plan is to develop an algorithm which will find the line that best fits this set of points. And the algorithm works like this, first it will draw a random line, and it'll calculate the error. The error is some measure of how far the points are from the line, in this drawing it looks like it's the sum of these distances but it could be any measure that tells us how far we are from the points. Now we're going to move the line around and see if we can decrease this error. We move in this direction and we see that the error kind of increases so that's not the way to go. So we move in the other direction and see that the error decreased, so we pick this one and stay there. Now we'll repeat the process many times over and over every time descending the error a bit until we get to the perfect line. So to minimize this error, we're going to use something called gradient descent. So let me talk a bit about gradient descent, the way it works is we're standing here on top of a mountain. This is called Mt rainierror as it measures how big our error is, and wanted descend from this mountain in order to descend from this mountain we need to minimize our height. And on the left, we have a problem of fitting the line to the data, which we can do by minimizing the error or the distance from the line to the points. So descending from the mountain is equivalent to getting the line closer to the points. Now, if we wanted to descend from the mountain we would look at the directions where we can walk down and find the one that makes us descend the most. And let's say this is the direction, so we descend a bit in this direction, this is equivalent to getting the line a little bit closer to the points. So now our height is smaller because we're closer to the points since our distance to them is smaller. And again and again we look at what makes us descend the most from the mountain and let's say we get here. Now we're at a point where we're descended from the mountain and on the right we found the line that is very close to our points. Thus, we've solved our problem and that is gradient descent. In a more mathematical way what happens is the following, we have a plot and here's a plot in two dimensions allowing a reality that plot will be in higher dimensions. We have our weights on the x-axis and our error on the y-axis. And we have an error function that looks like this, we're standing over here and the way to descend is to actually take the derivative or gradient of the error function with respect to the weights. This gradient is going to point to a direction where the function increases the most. Therefore, the negative of this gradient is going to point down in the direction where the function decreases the most. So what we do is we take a step in the direction of the negative of that gradient, this means we are taking our weights wi and changing them to wi minus the derivative of the error with respect to wi. In real life we'll be multiplying this derivative by the learning rate since we want to make small steps. This means the error function is decreasing and we're closer to the minimum. If we do this several times we get to either a minimum or a pretty good value where the error is small. Once we get to the point that we've reached a pretty good solution for our linear regression problem, and that's what gradient descent is all about

### 09. Mean Absolute Error-vLKiY0Ehors.en

In the last video, we'll learned how to decrease an error function by walking along the negative of its gradient. Now in this video, we're going to learn formulas for these error functions. The two most common error functions for linear regression are the mean absolute error, and the mean squared error. First, we'll learn the mean absolute error. So, here's a point and a line, the point has coordinates x, y, and the line is called Y-hat since it's the prediction. So, our prediction for this point is going to be the point in the line with the same x coordinate as our point, that is the point x, y hat. This means the vertical distance from the point to the line is y minus y hat, and that's what we'll be calling the error. Notice that this is not the actual distance to the line since this would be a perpendicular segment, but it's the vertical distance from the point to the line which is the distance between the point and its prediction. Now, our total error is going to be the sum of all these distances for all the points in our dataset. And sometimes we'll use this as our error, but in this case we'll use the average or the mean absolute error, which is the sum of all the errors divided by m, the number of points in our dataset. Using the sum or the average won't change our algorithms, since that would only scale our error by a constant, namely m. Notice something here which is that we have an absolute value around the y minus y hat, the reason is that if the point is on top of the line, the distance is y minus y hat, but if it's under the line then it's y hat minus y. We want the error to always be positive, otherwise negative errors will cancel with positive errors. Therefore, we take the absolute value of y minus y hat. So, our mean absolute error is the average of all absolute errors or in other words, the sum of all these absolute values divided by the number of points, which is m. We're going to plot these error over here in a graph. Obviously as we mentioned before, the graph has more dimensions but this is a two-dimensional simplification of that graph. As we descend in this graph using gradient descent, we get a better and better line until we find the best possible fit with the smallest possible mean absolute error.

### 10. Mean Squared Error-MRyxmZDngI4.en

Now in this video we'll learn the Mean Squared Error. The Mean Squared Error is very similar to the Mean Absolute Error. Again, here we have our point and our prediction, but now instead of taking the distance we're actually going to draw a square with this segment as its side. So the areas precisely y minus y hat squared. Notice that this is always non-negative, so we don't need to worry about absolute values. And our mean squared error is going to be the average of all these series of squares, and we're going to have this extra factor of one half for convenience later. So in summary the area's one half times the average of the sum of all y minus y hat squared. Again, we can take the sum and call it the total square error, but we take the average and this won't make a difference in the algorithm. Notice, if the point is over the line or underneath the line the square is always going to be a non-negative number, because the square of a real number is always going to be non-negative. The one half is going to be there for convenience because later we'll be taking the derivative of this error. Again, we can multiply this error by any constant and the process of minimizing it will be the exact same thing, so this one half does not affect anything. So here's a pictorial representation of the error. Here we have our points, our line, and the error is the average of the areas of all these squares. Here's our graph of the error. As we descend from this mountain we get to the place where the error is the smallest possible, and that's the same as minimizing the average of the areas of the squares.

### 11. Minimizing Error Functions-RbT2TXN_6tY.en

So far, we've learned two algorithms that will fit a line through a set of points. One is using any of the tricks namely the absolute and the square trick, and the other one is minimizing any of the error functions namely the mean absolute error and the mean squared error. The interesting thing is that these two are actually the exact same thing. What I'm saying is that when we minimize the mean absolute error, we're using a gradient descent step and that gradient descent step is the exact same thing as the absolute trick. Likewise, when we minimize the squared error, the gradient descent step is the exact same thing as the square trick. So let's see why, let's start with the mean squared error. Here's a point with coordinates (x, y) and here's our line with equation y-hat equals w_1 times x plus w_2. So y-hat is our prediction and the line predicts the y coordinate of this point that prediction gives us a point on the line that matches the x coordinate of the point (x,y). So it's the point (x,y-hat). Now, the error for this point is 1.5 times y minus y-hat squared, and the mean squared error for this set of points is the average of all these errors. But since average is a linear function, then whatever we do here applies to the entire error. Now, we know that the gradient descent step uses these two derivatives, namely the derivative with respect to the slope w_1 and the derivative with respect to the y-intercept w_2. If we calculate the derivatives, and you can see the calculation in detail in the instructor notes, we get negative times y minus y-hat times x for the one respect to the slope and negative times y minus y-hat for the one with respect to the y-intercept w_2. And notice that the length of this red segment is precisely (y minus y-hat) and the length of this green segment is precisely x. And if you remember correctly, the square trick told us that we have to upgrade the slope by adding y minus y-hat times x times the learning rate alpha, and upgrade the y-intercept by adding y minus y-hat times the learning rate alpha. But that is precisely what this gradient descent step is doing. If you like, feel free to pause the video or actually write it down in a little piece of paper to verify that is the exact same calculation. So this is why the gradient descent step utilize when we minimize the mean squared error is the same as the square trick. We can do the same thing with the absolute trick. The procedure is very similar except we have to be careful about the sign. This is our error, the absolute value of y minus y-hat and the derivatives of the error with respect to w_1 and w_2 are plus or minus x and plus or minus one based on the point is on top or underneath the line. Since the distance is x, then you can also check that this is precisely what the gradient descent step does when we minimize the mean absolute error. So that's it, that's why minimizing these errors with gradient descent is the exact same thing that using the absolute and the square tricks.

### 14. Absolute Vs Squared Error-csvdjaqt1GM.en

So here's a question, what is better the mean absolute error or the mean squared error? Well, there's no real answer to this. Both are used for a lot of different purposes, but here's one property that actually tells them apart. So here's a set of data and we're going to try to fit it by minimizing the mean absolute error. So let's say we have line A, line B, and line C. And the question is, which one of these lines gives us a smaller mean absolute error? Enter your answer below.

### 14. DLND REG 12 Absolute Vs Squared Error 2 V1 (1)-7El1OH17Oi4.en

Well that was a tricky quiz. If you said the same, then you were correct because as you can see, moving this line up and down actually keeps the mean absolute error the same. You can convince yourself by looking at the picture and checking that as you move a line up and down, you're adding a segment to two of the errors and removing a segment of equal length to the other two errors. Now, let's repeat the quiz but with the mean squared error. The question is, which one of these three lines A, B, and C would give us a smaller mean squared error? Enter your answer below.

### 14. DLND REG 13 Absolute Vs Squared Error 3 V1 (1)-bIVGf_dDkrY.en

So this time there was a solution and the solution is B and here's the reason which is more subtle. Our mean squared error is actually a quadratic function and quadratic functions have a minimum at the point in the middle. Over here, we can see that the error for line A would be around here, the error for line C would be around here, and the one for line B would be somewhere around here. So we can see that the minimum mean square error is given by line B. Here's a diagram of the errors and you can check with your eyes or even drawing a small example and calculating the areas; that line B would be giving you the smallest mean square error.

### 16. Higher Dimensions--UvpQV1qmiE.en

So in the previous example, we had a one column input and one column output. The input was the size of the house and the output was the price. So we had a two-dimensional problem. Our prediction for the price would be a line and the equation would just be a constant times size plus another constant. What if we had more columns in the input, for example size and school quality? Well, now we have a three dimensional graph because we have two dimensions for the input and one for the output. So now our points don't live in the plane, but they look like points flying in 3-dimensional space. What we do here is we'll feed a plane through them instead of fitting a line, and our equation won't be a constant times one variable plus another constant. It's going to be a constant times school quality plus another constant times size plus a third constant. That's what happens when we're in three dimensions. So what happens if we're in n dimensions? So in this case we have n minus one columns in the input and one in the output. So, for example the inputs are size, school quality, number of rooms, et cetera. Well, now we have the same thing except our data lives in n-dimensional space. So for our input, we have n minus one variables namely; x_1, x_2 up to x_n minus one and for the output of the prediction, we only have one variable y hat. Our prediction would be an n minus one dimensional hyperplane living in n dimensions. Since it's hard to picture n-dimensions just think of a linear equation in n variables, such as y hat equals w1x1 plus w2x2 plus all the way to w_n minus one x_n minus one plus w_n and that's how we do predictions for higher dimensions. In order to find the weights w_1 up to w_n the algorithm is exactly the same thing for two variables. We can either do the absolute or square root tricks, or we can calculate the mean absolute or square errors, and minimize using gradient descent.

### 18. Closed Form Solution-G3fRVgLa5gI.en

So here's an interesting observation; in order to minimize the mean squared error, we do not actually need to use gradient descent or the tricks. We can actually do this in a closed mathematical form. Let me show you. Here's our data x_1, y_1 all the way to x_m, y_m; and in this case, m is five. And the areas of the squares represent our squared error. So our input is x_1 up to x_m and our labels are y_1 up to y_m, and our predictions are of the form y_i hat equals w_1 x_i plus w_2, where w_1 is a slope of the line and w_2 is the y-intercept. And the mean squared error is given by this formula over here. Notice that I've written the error as a function of w_1 and w_2, since given any w_1 and w_2 we can calculate the predictions and the error based on these values of w_1 and w_2. Now, as we know from calculus, in order to minimize this error, we need to take the derivatives with respect to the two input variables w_1 and w_2 and set them both equal to zero. We calculate the derivatives and you can see the full calculation in the instructor notes and we get these two formulas. Now, we just need to solve for w_1 and w_2 for these two equations to be zero. So what do we have now? We have a system of two equations and two unknowns, we can easily solve this using linear algebra. So now the question is, why don't we do this all the time? Why do we have to go through many gradient descent steps instead of just solving a system of equations and unknowns? Well, think about this. If you didn't have only two dimensions in the input but you had n, then you would have n equations with n unknowns, and solving a system of n equations with n unknowns is very expensive because if n is big, then at some point of our solution, we have to invert an n by n matrix. Inverting a huge matrix is something that takes a lot of time and a lot of computing power. So this is simply not feasible. So instead this is why we use gradient descent. It will not give us the exact answer necessarily but it will get us pretty close to the best answer which will give us a solution that fits our data pretty well. But if we had infinite computing power, we would just solve this system and solve linear regression in one step.

### 21. Polynomial Regression-DBhWG-PagEQ.en

So what happens if we have data that looks like this where a line won't really do a good job fitting in? Maybe would like to have a curve or some polynomial. Maybe something along the lines of 2x cubed minus 8x squared, et cetera. This can be solved using a very similar algorithm than linear regression. All we have to do is instead of considering lines, we consider higher degree polynomials. This would give us more weights to solve our problem. For example, this problem here we'll make a solve for four weights; w_1, w_2, w_3, w_4. But the algorithm is the same thing. We just take the mean absolute or squared error and take the derivative with respect to the four variables and use gradient descent to modify these four weights in order to minimize the error. These algorithm is known as polynomial regression.

### 22. Regularization-PyFNIcsNma0.en

The following concept is one that works both for regression and classification. So in this video, we'll explain it using a classification problem. But as you will see, all the arguments here work with regression algorithms as well. The concept is called regularization, it's a very useful technique to improve our models and make sure they don't over fit. So let's look at some data, here's the data, and let's make two copies of it, and let's look at two models that classify this data. The first one is a line and the second one is a higher degree polynomial curve. So, the question is which one is better? Well, they both have their pros and cons, right? The one on the left makes a couple of mistakes. As you can see, there is a red point and a blue point in the wrong sides but it is much simpler. The one on the right makes zero mistakes but it's actually a bit more complicated. So let's say we want the one on the left because the one on the right over fits and it just doesn't generalize well. So the problem is that when we train the model, the one on the right will appear more likely and the reason is the following. When we're training the model, the model takes an error and minimizes it. So, the model on the left has a small error since it own misclassifies two points but it's an error nonetheless. The model in the right has a really small error since it does not misclassifies any of the points. So, if we train a model to minimize error, it will build a boundary like the one on the right, not the one on the left. So the question is, how do we pick the one in the left? Well, here's an idea. Let's look at the equation and let's say the equation of the line on the left is something like 3x_1 plus 4x_2 plus five equals zero. The equation of the polynomial is something more complex with high degree terms like x_1 squared x_1 x_2 x_2 cube, et cetera. If we look at the equation in the left, it's much simpler than the equation on the right. In particular, there are less coefficients, only three, four, five whereas the right one has many more. So, if we find a way to in commend the error by some function of these numbers, that would be very helpful because in some way the complexity of the model will be added into the error. So a complex model will have a larger error and then a simple model. So, let's do that and I'll show you the details later but the idea is that we take this three and four and notice that we're forgetting about the constant term, and there's a reason for that. But if we take this three and four and say add them to the error, we get a slightly bigger error. But what if we take all these coefficients and add them to the error here, now we get a huge error. Now, we can see that the modeling the left is better because it has a smaller combined error, so again, what we did is we took the complexity in the model into account, when we calculated the error and in that way, a simpler model has an edge over the complicated model. Simpler models have a tendency to generalize better so, that's what we want. So now, let me be more detailed on how to take the complexity of a model and turn into part of the error. In summary, will take this highlighted coefficients and somehow add them to the error, this method is called L1 regularization and it's very simple, here's how it works. What L1 regularization does, it takes the coefficient and just adds the absolute values of them to the error. So in this case, we're adding absolute value of two which is two plus absolute value of minus two, which is two again, et cetera and we see that they add to 21. In the linear case, we see that we're adding is the absolute value of three and four which is seven, so a seven is much less than 21, we can see that the complicated model gives us a much higher error. That's L1 regularization and the one is attached to the absolute value. L2 regularization is very similar and what we do here is instead of adding the absolute values, we add the squares of the coefficients. So for the complicated case, we get two squared plus minus two squared, et cetera which gives us 85. For the linear case, we get three squared plus four squared which is 25, which is much smaller than 85. So again, we see that the complex model gets punished a lot more than the simple model. But now the question is, what if we punish the complicated model too little or what if we punish it too much? Maybe some models, like a model to send a rocket to the moon or a medical model, have very little room for error and we're okay with some complexity, or maybe other models like a video recommendation model, or model recommending potential friends on a social network have more room for experimenting and need to be simpler and faster to run a big data. So we're okay with having some error. So it seems that for every case, we have to tune how much we want to punish complexity in each model. This can be fixed with a parameter and this parameter is called lambda. What we do with lambda, is we multiply the complexity part of the error as follows. Let's look at the two models again and let's remember that the yellow part of the error comes from the misclassified points and the green part comes from the complexity of the model, namely the coefficients in the polynomial. Let's say, we have a small lambda, so we take the green error and multiplied by small lambda which gives us something small. Therefore, the right model still wins because the complexity part of the error is small and it won't swing the balance. But if we have a large value for lambda, then we're multiplying the complexity part of the error by a lot. Which punishes the complex model more and then the simple model wins. So in summary, this is what happens, if we have a large lambda then we're punishing complexity by a large amount and we're picking a simpler model. Whereas if we have a small lambda, then we're punishing complexity by a small amount, so we're okay with having more complex models. Now the question is, which one to use L1 or L2 regularization? So here's a cheat sheet with some benefits for each one. L1 regularization is actually computationally inefficient even though it seems simpler because it has no squares, but actually those absolute values are hard to differentiate. Whereas, an L2 regularization squares have very nice derivatives. So, these are easy to deal with computation. The only times where L1 regularization is faster than L2 regularization is when the data is sparse. So let's say if you have a thousand columns of data but only 10 are relevant and the rest are mostly zeros, then L1 is faster, L2 is better for non-sparse outputs which is when the data is more equally distributed among the columns. L1 has one huge benefit which is that, it gives us feature selection. So let's say, we have again, data in a thousand columns but really only 10 of them matters and the rest are mostly noise. So, L1 will detect this and will make the relevant columns into zeroes. L2 on the other hand won't do this and it just take the columns and treat them similarly. So that's it, that's regularization.

### 23. Conclusion-pyeojf0NniQ.en

Well that was it. In this lesson, you have learned linear regression and some of its generalizations. You have also gone hands-on and implemented the gradient descent algorithm for linear regression. You are now ready for the upcoming project in which you'll be implementing a regression neural network to analyze real data. Great job.



## Part 07-Module 01-Lesson 02_Naive Bayes

### 01. Naive Bayes Intro V2-vNOiQXghgRY.en

Hello again and welcome to the Naive Bayes section. Naive Bayes is a more probabilistic algorithm which is based on playing with the concept of conditional probability. This algorithm has great benefits such as being easy to implement and very fast to train. We'll be studying one of very interesting applications, natural language processing. In the lectures and in the lab, we'll use it to analyze text, and emails, and speeches.

### 02. SL NB 01 Guess The Person V1 V1-tAOAjI-7ins.en

We'll start with an example. Let's say we're in an office and there are two people, Alex and Brenda, and they're both there the same amount of time. When they were in the office and we see someone passing by really fast, we can't tell who it is, but we'd like to take a guess. So far, with all we know, all we can infer is that since they're both in the office the same amount of time, the probability of the person being Alex is 50 percent and the probability of the person being Brenda is also 50 percent. But now, let's try to use more information so we can make a better guess of who the person is. When we saw the person running by, we notice that they were wearing a red sweater. So, we'll use that piece of information. We've known Alex and Brenda for a while, and actually we've noticed that Alex wears a red sweater two days a week, and Brenda wears a red sweater three days a week. We don't know which days, but we are sure of this fact. Also, when we say week we mean workweek, so five days, although at the end this won't matter much. So now what we'll do, is we'll use this piece of information to help us make a better guess. First off, since Alex wears a red sweater less than Brenda, It's easy to imagine that it's a bit less likely that the person we saw is Alex than that it is Brenda. But exactly how likely? Well, let's say that if we saw a person pass by five times, it would make sense to think that two of this times it was Alex, since he wears a red sweater twice a week. And the other three times it was Brenda, since she wears a red sweater three times a week. Therefore, from here we can infer that the probabilities are 40 and 60. We've used the formation about the color of the sweater to obtain better probabilities about who was the person who passed by. This is Bayes' theorem and we'll learn in more in detail in the next few videos. The initial guess we had, the 50/50 guess, is called the prior, since it's all we could infer prior to the new information about the red sweater. The final guess we have, the 60/40 guess is called the posterior, since we've inferred it after the new information has arrived.

### 03. SL NB 02 Known And Inferred V1 V2-DrYfZXiDLQI.en

In the last video, we saw an example of Bayes theorem. But here's the main idea fit and it's a very powerful theorem. What it does is it switches from what we know to what we infer. What we know in this case is the probability that Alex wears red and the probability that Brenda wears red. And what we infer is the opposite, is the probability that someone wearing red is Alex or that someone wearing red is Brenda. In other words, initially we know the probability of an event A. And to give us more information, we introduce a new event R which is related to A. We know the probability of R given A. What Bayes' theorem does is from these two, it infers the probability of A given R. This is the new probability of A once we know that the event R has occurred

### 04. SL NB 03 Guess The Person Now V1 V2-pQgO1KF90yU.en

Bayes Theorem can get a little more complex. Let's take a look at a small example and what we'll do here is we'll mess a bit with the prior probability. So again, we have Alex and Brenda in the office, and we saw someone pass by quickly and we don't know who the person is. So let's say we look more carefully at their schedules and we realized that Alex actually works from the office most of the time. He comes by three days a week. And Brenda travels a lot for work, so, she actually comes to the office only one day a week. So initially, without knowing anything about the red sweater, all we know is that it's three times more likely to see Alex than to see Brenda. Therefore our prior probabilities are 0.75 for Alex and 0.25 for Brenda. And let's say that we have this happening throughout all the weeks, but now we use our extra knowledge which is that the person we saw had a red sweater. The rule is still as before, as Alex wears red twice a week and Brenda wears red three times a week. So, naively we would think that the real probabilities are not exactly 0.75 or 0.25 because Brenda wears a red sweater more than Alex, so they should be a little closer to each other. Let's calculate them. So, we'll do the following, let's think of the columns as weeks instead. So, now for each five-day work week, Alex wears red twice and Brenda three times. So, we colored the days they wore red. Now, since we know the person wore red, we forget about the times that they didn't. So we have nine times someone wore red. Six of them are Alex and three of them are Brenda. Therefore, among nine times we saw someone wearing red, two-thirds of the times it with Alex and one third of the time it was Brenda. Thus, our posterior probabilities are two-thirds or 0.67 for Alex and one third or 0.33 for Brenda. So it looks like we did a little bit of magic. Let's do this again in a more mathematical way. We saw a person and initially all we know is that it's Alex with a 75% probability and Brenda with a 25% probability since Alex comes to the office three times a week and Brenda once a week. But now new information comes to light which is that the person is wearing a red sweater and the data says that Alex wears red two times a week. So now we look at Alex. What is the probability that he's wearing red? Since a work week has five days and the probability of him wearing red is two-fifths or 0.4. And the probability of him not wearing red is the complement, so 0.6. Same thing with Brenda, since she wears red three a week, then the probability of her wearing red today is 0.6 and the probability of her not wearing red is 0.4. Now, by the formula of conditional probability, the probability that these two will happen is the product of the two probabilities P of Alex, times P of red given Alex. Therefore, the probability of the person we saw is Alex and that they're wearing red is precisely 0.75 times 0.4. We multiply them and put the result here. We calculate the other probabilities in the same way, that probability of the person we saw is Alex and that he's not wearing red is 0.75 times 0.6. The probability of the person we saw is Brenda and that she's wearing red, is again the product of these probabilities, which is 0.25 times 0.6. And finally, the probability of the person we saw is Brenda and she's not wearing red is 0.25 times 0.4. And now here's where the Bayesian magic happens, are you ready? We have four possible scenarios and you can check that these four probabilities add to one. But we know one thing, that the person we saw was wearing red. Therefore, out of these four scenarios, only two are plausible, the two when the person is wearing red. So, we forget about the other two. Now, since our new universe consists of only these two scenarios, then the probability should be higher, but their ratio should still be the same with respect to each other. This means, we need to normalize them or equivalently, divide them by something so that they now add to one. The thing we should divide them by, is the sum of the two. So, our new probability of the person being Alex is the top one, namely, 0.75 times 0.4 divided by the sum of the two, namely, 0.75 times four, plus 0.25 zero times 0.6. This is precisely two-thirds or 0.67, and now we can see that the complement is the probability that the person is Brenda, which is one third or 0.33. If we take Brenda's probability and divide it by the sum of both probabilities we can see that we get one third as desired. And that's it, that is Bayes Theorem at its full potential.

### 05. SL NB 04 Bayes Theorem V1 V2-nVbPJmf53AI.en

So, let's look at a formal version of Bayes Theorem. Initially, we start with an event, and this event could be A or B. The probabilities for each are here, P of A, and P of B. Now, we observe a third event, and that event can either happen or not happen both for A and for B. R is going to help us find more exact probabilities for A and B in the following way. Let's say we can calculate the probability of R given A, and also, of R complement which is node R given A. And similarly for R given B, and R complement given B. Now, our set of scenarios are these four, R n A, R complement n A, R n B, and R complement n B. But since we know R occurred, then we know that the second and the fourth events are not possible. So, our new universe consists of the two events, R n A and R n B. We calculate the probability for A n R or equivalently A intersection R, and by the Law of Conditional Probability, this is P of A times B of R given A. Similarly, for B intersection R. Now, since these probabilities do not add to one, we just divide them both by their sum so that the new normalized probabilities now do add to one. Thus, we get the following formulas for P of A given R, and P of B given R. These are our new and improved probabilities for A and B after we know that R occurred. Again, P of A and P of B are called the prior probabilities which is, what we knew before we knew that R occurred. P of A given R and P of B given R, are posterior probabilities which is, what we inferred after we knew that R occurred. And here it is, the formula for Bayes Theorem..

### 06. SL NB 05 Q False Positives V1 V2-ngA6v09eP08.en

Now, let's look at an interesting application of Bayes Theorem. Let's say we're not feeling very well and we go to the doctor, the doctor says there's a terrible disease going on, I'll administer a test for you. Moreover, she says that the test has 99 percent accuracy. More specifically, she says that for every 100 patients that are sick, the test correctly diagnosis 99 of them and for every 100 patients that are healthy, the test correctly diagnosis 99 of them. If we want to be tactical, these are actually called sensitivity and specificity. Then while we're waiting for our test, we researched the Internet and find that on average, one of every 10,000 people suffers from the disease. The next day the doctor calls with terrible news. She says that we have tested positive for the disease, so now we're panicking. But before panicking, let's turn to math and actually calculate what is the probability of being sick. So here's a quiz. Given the two pieces of information that the test has 99 percent accuracy, and that one out of every 10,000 people have the disease, what do you think the probability is that we're sick? Is it from 0-20 percent, from 20-40, 40-60, 60-80, or 80-100? Take your guess and enter it below.

### 07. SL NB 06 S False Positives V1 V3-Bg6_Tvcv81A.en

Well, let's see. Let's use Bayes theorem to calculate it. We'll use the following notation, S will stand for sick, H will stand for healthy, and the plus sign will stand for testing positive. So since one out of every 10,000 people are sick, we get that P of S is 0.0001. Similarly, P of H is 0.9999. Since the test has 99 percent accuracy, both for sick and for healthy patients, we see that P of plus, given S is 0.99, the probability that the sick patient will get correctly diagnosed. And that P of plus given H is 0.01, the probability that the healthy patient will get incorrectly diagnosed as sick. So plugging that into the new formula, we get the probability of being diagnosed as positive when you're sick is exactly 0.0098, which is less than 1 percent. Really? Less than 1 percent? When the test has 99 percent accuracy? That's strange, but I guess that's the answer to the quiz. So, less than 1 percent falls in this category of 0 to 20 percent. I'm still puzzled though, why less than 1 percent If the test is correct 99 percent of the time. Well, let's explore. Let's go back to the tree of possibilities. Let's say we start with 1 million patients, and they have two options, healthy and sick. Now, since 1 out of every 10,000 patients is sick, then from this group of 1 million patients, 100 will be sick and the remaining 999,900 will be healthy. Now let's remember that for every 100 patients, 99 get correctly diagnosed and one gets incorrectly diagnosed, this happens both for sick and for healthy patients. So, let's see how many of these patients will get diagnosed positively or negatively. Out of the 100 sick ones, 99 will be correctly diagnosed as positive and one will be incorrectly diagnosed as negative. Now, out of the healthy ones, 1 percent or 9,999 will be incorrectly diagnosed as positive and the remaining 99 percent or 989,901 will be correctly diagnosed as negative. Now let's really examine these four groups. The first group is the sick people who we will send for more test or treatment. The second is the unlucky sick people that will be sent home with no treatment. The third is a slightly confused healthy people who will be sent for more tests. And the fourth group or the majority is the people who are healthy and were correctly diagnosed healthy and sent home. But now, here's the thing, we know we tested positively, so we must be among one of these two groups, the sick people who tested positively or the healthy people who tested positively. One group is much larger, it has 9,999 people, whereas the other one has only 99 people. The probability that we're in this group is much larger than that we're in this group. As a matter of fact, the probability that we are in the small group is 99 divided by the sum, 99 plus 9,999, which is, you guessed it, 0.0098, which is smaller than 1 percent, this is the probability of being sick if you are diagnosed as positive. But why is the group of healthy people who tested positively so much larger than the group of sick people who tested positively? The reason is because, even though the test only fails 1 percent of the time, that 1 percent is much, much larger than the one out of 10,000 rate of sickness among the population. In other words, in a group of 10,000 healthy people, 1 percent or a 100 of them will get misdiagnosed as sick. On the other hand, in a group of 10,000 people, around one will be sick, this is much less. So if you know you've tested positively, you are still more likely to be among the 100 errors than among the ones sick. And how much more likely? Around 100 times, and that's why our probability of being sick while being diagnosed positively is around 1 percent. This phenomenon is called the False Positive, and it has been a nightmare for the medical world, the legal world and many others. Search False Positives on Google, and you'll see many cases in which people have been misdiagnosed, misjudged etc. So always be aware of false positives, they are very sneaky.

### 08. SL NB 07 Q Bayesian Learning 1 V1 V4-J4BmsKXPnkA.en

Now the question is, how do we use this wonderful Bayes theorem to do machine learning. And the answer is repeatedly. Let's look at this example, a spam email classifier. So let's say, we have some data in the form of a bunch of emails. Some of them are spam and some of them are not spam, which we call ham. Spam are, "Win money now!" "Make cash easy!" et cetera. And the ham are, "How are you?" "There you are!" et cetera. And now, what we'll do is, a new email comes in say, "easy money" and we want to check if it's spam or ham. So, we take it word by word. Of course, we can be more effective if we took into account the order of the words, but for this classifier, we won't. It's surprising how good it can be even if it doesn't take into account the order of the words. So let's study the first word say, "easy." We can see that the word "easy" appears once among the three spam emails and once among the five ham emails. And the word "money" appears twice among the three spam emails and once among the five ham emails. So, let's start with calculating some preliminary probabilities as an exercise. Given the data we have, what is the probability of an email containing the word "easy" given that it is spam? Here are some options. And let's also calculate it for the other word. Again given our data, what's the probability of an email being spam given that it contains the word "money"? Here are the options. Enter your answer below.

### 09. SL NB 08 S Bayesian Learning 2 V1 V6-3rIYZgCXVXY.en

So let's see. We have three spam emails and one of them contains the word 'easy,' which means the probability of an email containing the word 'easy' given that it's spam is one-third. Since two out of the three three spam emails containing the word 'money,' then the probability of an email containing the word money given that it's spam is two-thirds. And similarly, since there are five ham emails and one of them contains the word 'easy,' then the probability of an email containing the word 'easy' given that it is ham is one-fifth. And same thing for the word 'money.' And the main gist of Bayesian learning is the following, we go from what's known, which is P of 'easy' given spam and P of 'money' given spam, to what's inferred, which is P of spam given that it contains the word 'easy,' which is one-half, since there are two emails containing the word 'easy' and only one of them is spam. And P of spam given that it contains the word 'money,' which is two-thirds since there are three emails containing the word 'money' and two of them are spam.

### 10. SL NB 09 Bayesian Learning 3 V1 V4-u-Hj4RsJn1o.en

So, let's do this calculation a bit more in detail. Since we have eight emails in total and three of them are spam and five of them are non-spam or ham, then our prior probabilities are three over eight for spam and five over eight for ham. So, onto calculate the posteriors. Say we have a spam email, since there are three of them and one contains the word easy and two don't. Then, the probability for containing the word easy is one-third, and for not containing it is two-thirds, if you're spam. And as we had calculated before, the probability of containing the easy if your ham is one-fifth and of not containing it if your ham is four-fifths. Now, by the rule of conditional probability, probability of the email spam containing the word easy is the product of these two, three over eight times one-third, which one-eighth. In a similar way, we calculate the probability of being spam and not containing the word easy, which is one-fourth. And probabilities of being ham containing the word easy is one-eighth, and not containing it is one-half. Now, this is where we apply Bayes' rule. We know that the email contains the word easy, so our entire universe consists of only these two cases: when the is spam or ham. Those two have the same probability, one-eighth of happening. So, once we normalize the probabilities, they both turn into 50 percent. Thus, our two posterior probabilities are 50 percent. For ham emails, we can do the same procedure. Our prior are three over eight and five over eight as before. Our probabilities of containing the word money and not containing it are two-thirds and one-third for the spam emails, and one-fifth and four-fifths for the ham emails. Our products of probabilities are then one-quarter, one-quarter, one-eighth, and one-half. But since the email contains the word money, then we only care about these two. Since one-fourth is twice as much as one-eighth, when we normalize them we get two-thirds or 66.7 percent for spam, and one-third or 33.3 percent for ham. These are the posteriors.

### 11. MLND SL NB Naive Bayes Algorithm-CQBMB9jwcp8.en

Now, here's where the word naive comes in Naive Bayes. We're going to make a pretty naive assumption here. Let's look at the probability of two events happening together, so P of A and B. We can also read this a P of A intersection B. And we're going to say that this is the product of P of A and P of B. Now, this only happens when the two events are independent. If they're not, then this is not true. For example, if A is the event of it being hot outside and B is the event of it being cold outside, then they both have a positive probability. But now, what's the probability of both events happening at the same time? This will be zero, since it can't be hot and cold at the same time. So, this formula doesn't follow because the events of being hot and being cold are dependent on each other. But in a Naive Bayes, we will assume that our probabilities are independent. This, as we said, is a false and naive assumption, but in practice, it works very well and it makes our algorithm very fast. Another formula I will use is a formula for conditional probability. These are two ways of writing P of A intersection B. And this is the basis for our base theorem. But the trick we'll use here is to forget about P of B. And now, we don't have these being equal. But we have P of A given B to be proportional to P of B given A times P of A. This will work very well because in the practice, P of B will cancel out, so the fact that these two are proportional is very useful. And now, here's what we want. We have an email that contains the words easy and money, and we want to know if it is spam. So we want this, the probability of the email being spam given that it contains the words easy and money. We'll start by using a conditional probability rule that we just reviewed to write it as a product of the probability that the email contains the words easy and money given that it is spam, times the probability of the email being spam. In this formula, A represents being spam and B represents containing the words easy and money. Now, we are ready to use our naive assumption. This first factor over here is a probability of the email containing the words easy and money given that it is spam. We can write it as a probability of the email containing the word easy given that it is spam, times the probability of the email containing the word money given that it is spam. Again, huge naive assumptions as these may be dependent. It could be that containing the word easy actually makes it more likely that the email contains the word money. But that's okay. In many cases, this assumption won't affect the results and it will make our calculations much easier. And this is the heart of the Naive Bayes algorithm. And now, we do the same thing for hand emails. We have both probabilities written as a product of factors. But what are these factors? Well, we've calculated them before based on our data. The first one P of containing the word easy given that it is spam, is one-third since there are three spam emails and one of them contains the word easy. For P of containing money given spam, that's two-thirds since there are three spam emails and two of them contain the word money. And P of spam is very simple. It's three over eight since there are eight emails and only three of them are spam. We do a similar calculation for the bottom one and get one-fifth, one-fifth, and five over eight. Now we multiply them and we get that the probability of spam given that it contains the word easy and money is proportional to one over 12. And for ham, it's proportional to one over 40. Now remember that these values are not the actual probabilities, they are proportional to the actual probabilities. So what do we do to get the actual probabilities? Here's the magic. We know that an email has to be either spam or ham, so these two should add to one. So we need to normalize them, namely, multiply them both by the same factor so that they are still proportional to one over 12 and one over 40, but they add to one. Let's try that in a quiz. Can you find two numbers that add to one and that they are in the same proportion to each other as one over 12 and one over 40? Enter your answer below.

### 12. MLND SL NB Solution Naive Bayes Algorithm-QDj3xzjuYmo.en

So the way to do this is to actually divide each one by the sum of both. This will make sure that they add to one. For the first one, we have one over 12 divided by one over 12 plus one over 40, which is 10 divided by 13. And for the second one, we have one over 40 divided by one over 12 plus one over 40, which is three over 13. So there we go. The answers are 10 over 13 for spam and three over 13 for ham. So, for this particular email, we conclude that it is very likely to be spam. Now, what happens in general? Well, let's say we have a bunch of words that we use as features to tell if the email is spam or not. Say, easy, money,cheap, et cetera. Our first step is to flip the event and the conditional to get this, then we make the naive assumption to split this into a product of simple factors that we can quickly calculate by looking at our data. We do this both for spam and ham, and we get some values that don't add to one. As a final step, we normalize to get our final probabilities of our email being spam or ham. And that's it. That's how the Naive Bayes algorithm works.



## Part 07-Module 01-Lesson 03_Clustering

### 02. Unsupervised Learning-Mx9f99bRB3Q.en

So Katie, this is going to be a unit on unsupervised learning. &gt;&gt; Unsupervised learning is something that's very important, because most of the time, the data that you get in the real world doesn't have little flags attached that tell you the correct answer. So what are you to do as a machine learner in that case? You turn to unsupervised techniques to still figure something out about that data. &gt;&gt; Okay, let's talk about them. Given a dataset without labels over all the data points are of the same class. There are sometimes still things you can do to extract useful information. Like this dataset over here, where I would say this dataset is structured in a way that is useful to recognize for a machine learning algorithm. &gt;&gt; When we look at this by eye, it looks like there's clumps or clusters in the data. And if we could identify those clumps or clusters, we could maybe say something about a new, unknown data point and what its neighbors might be like. &gt;&gt; Or here's a second example of data. Maybe the data looks just like this. There's something we can say here as well. &gt;&gt; Right. So all the data in this example looks like it lives on some kind of line or some complicated shape that you seem to be drawing in there right now. &gt;&gt; Yeah. And it's, it's, it's used to be a two-dimensional space, with x and y over here. But some of it we can reduce it to a one-dimensional line. So that's called what? &gt;&gt; That's called dimensionality reduction, usually. &gt;&gt; Dimensionality reduction. So we learned about, a little bit about clustering. &gt;&gt; Clustering is what we'll learn in this lesson. &gt;&gt; And you can see here an example of something also called unsupervised learning of dimensionality reduction. &gt;&gt; Which we will get in a future lesson. &gt;&gt; So these kind of things where you find structure in the data without labels, they're called unsupervised learning. And we're now gong to dive into the wonderful, wonderful magical land of unsupervised learning. &gt;&gt; Sounds great, let's dive in.

### 03. Clustering Movies-g8PKffm8IRY.en

So here's an example that should make it intuitively clear the clustering sometimes make sense. So take Katie and me, we both have a movie collection at home. And just imagine that both of us look at each other's movies, and all movies, and Katie gets to rank them from really, really bad to great. And I to get to rank the same movies from bad to great. Now it so turns out that Katie and I have very different tastes. Maybe some movies that I love, like all my James Bond movies, but Katie doesn't like as much. And there's others or these chick flicks, that Katie loves, and I don't. So somewhat exaggerated. It mind end up, that our movies fall into different classes, depending on who likes which movies. So say, say you're Netflix, and and you look at both my queue and Katie's queue, and you graph it like that. Then you can conclude, wow, there's two different classes of movies. Without knowing anything else about movies, you would say, here's class A and class B. And they're very different in characteristics. And the reason why Netflix might want to know this is next time Katie comes in, you want to kind of propose a movie that fits into class B and on to class A. Otherwise, she's very unlikely to watch this movie. And conversely, for me, you want a reach into class A versus class B. In fact, if you were to look at those movies, you might find that old style westerns are right over here. And modern chick flicks might be sitting over here. Who knows? But that's an example of clustering. Because here, there's no target labels given. Then they move down for you if class A and B existed. But after looking at the data, you could, through clustering, deduce as two different classes, and you could even look at the movie titles to understand what these classes are all about.

### 04. How Many Clusters-8Ygq5dRV0Kk.en

Okay, and I would argue it's 2. There's a cluster over here. And a cluster over here. And the cluster centers respectively lie right over here and somewhere over here. So that's the place we would like to find to characterize the data.

### 04. How Many Clusters-R6oIvdBtsZw.en

The perhaps the most basic algorithm for clustering, and by far the most used is called K-MEANS. And I'm going to work with you through the algorithm with many, many quizzes for you. Here is our data space. And suppose we are given this type of data. The first question is intuitively, how many clusters do you see? Truth telling, there is not a unique answer, it could be seven it could be one, but give me the answer that seems to make the most sense

### 05. Match Points with Clusters-lS5DfbsWH34.en

In k-means, you randomly draw cluster centers and say our first initial guess is, say, over here and over here. These are obviously not the correct cluster centers. You're not done yet. But k-means now operates in two steps. Step number is assign and step number two is optimize. So let's talk about the assignment. For classes in number one, I want you to click on exactly those of the red points that you believe are closer to center one than center two.

### 05. Match Points with Clusters-wJV1cRjmIYY.en

And the answer is this guy is closer these guys over her are closer. And the way to see this is you can make a line between the cluster centers and then draw an equidistant and orthogonal line and that line separate the space into a half space that's closer to center number one, which is the one over here, and a half space that's closer to center number two, which is the space over here.

### 06. Optimizing Centers (Rubber Bands)-TN1rQMrx65c.en

So after the assign step, you can now see these little blue lines over here that marry data points to cluster centers. And now we're going to think of what this rubber bands. They're rubber bands that like to be as short as possible. In the optimize step, we're not allowed, now allowed to move the green cluster center to a point where the total rubber band is minimized.

### 06. Optimizing Centers (Rubber Bands)-nNR4hjhhGBc.en

So now we know that these four points correspond to the present class center one that was randomly chosen. And these three points over here correspond to class center in the middle. That's the assignment step. obviously that's not good enough. Now we have to optimize. And what we are optimizing is, you are minimizing the total quadratic distance. Of our cluster center to the points. We're now free to move our cluster center. I think of these little blue lines over here as rubber bands, and that we're trying to find the state of minimum energy for the rubber bands, where the total quadratic error is minimized. And for the top one, I'm going to give you three positions. They're all approximate, but pick the one that looks best in terms of minimization. And that's a not a trivial question at all. So one could be right over here, one could be at and one could be right over here. Which one do you think best minimize, or is minimize of the three positions, the total quadratic length of these rubber bands?

### 07. Moving Centers 2-FY0DXe0lfrI.en

And then argue this is the one that minimizes it. In fact, a center somewhere over here minimizes the total rubber bands in the bottom case. And we can argue where exactly it falls, but you get the principle

### 07. Moving Centers 2-uC1Xwc7warg.en

Same exercise for the optimization step for the center below. I give you a couple of hypotheses, four in total. Pick the one that minimizes the total bubble length should be easy now

### 08. Match Points (again)-5j6VZr8sHo8.en

And this example is now easy. You can see that all the four over here fit with the green one. In fact, the separating line will be somewhere here and, in fact, after the next iteration of optimize, with the assignment of those point, four points of this cluster center and those three points of this cluster center. You can see that this cluster center will move straight into the center of those four points. And this cluster center will move to the center of those three points. Have we truly achieved our result? We now have assumed it's two clusters. But our algorithm of iteratively assigning and optimizing has moved the cluster center straight into what we would argue is actually the correct centroid for those two clusters over here. That is called the k-Means algorithm.

### 08. Match Points (again)-9J3IwQFXveI.en

So, let's do it again now. With these new cluster centers, pick the one up here and click on all of the seven data points that you now believe will be assigned for the cluster center on the left

### 09. Handoff to Katie-knrPsGtpyQY.en

So you learn about k-means. Katie is going to give you one more example of how to apply k-means in practice.

### 10. K-Means Cluster Visualization-ZMfwPUrOFsE.en

And I hope you said three, it's pretty obvious that there should be three centroids here. So let's add three, one, two, three. So they're all starting out right next to each other, but we'll see how as the algorithm progresses, they end up in the right place.

### 10. K-Means Cluster Visualization-iCTPBcowJRY.en

Now I want to show you a visualization tool that I found online that I think does a really great job of helping you see what k-means clustering does. And that should give you a good intuition for how it works. So I'd like to give a special shout out to Naftali Harris, who wrote this visualization and very kindly agreed to let us use it. I'll put a link to this website in the instructor notes that you can go and play around with it on your own. So it starts out by asking me how to pick the initial centroids of my clusters. I'll start out with Randomly right now. What kind of data would I like to use? There are a number of different things here, and I encourage you to play around with them. A Gaussian Mixture has been really similar to one of the simple examples we've done so far. So Gaussian mixture data looks like this. These are all the points that we have to classify. The first question for you is, how many centroids do you think is the correct number of centroids on this data?

### 11. K-Means Clustering Visualization 2-fQXXa-CAoS0.en

One of the things that's immediately apparent once I start assigning my centroids, with these colored regions, is how all the points are going to be associated with one of the centroids, with one of the clusters. So you can see that the blue is probably already in reasonably good shape. I would say that we got a little bit lucky in where the, the initial centroid was placed. It looks like it's pretty close to the, the center of this blob of data. With the red and the green it looks like they're sitting kind of right on top of each other in the same cluster. So, let's watch as K-means starts to sort out this situation and get all the clusters properly allocated. So, I hit Go. The first thing that it does is it tells me explicitly which cluster each one of these points will fall into. So you see, we have a few blue that fall into the wrong cluster over here. And then, of course, the red and the green. So this is the association step is all the points are being associated with the nearest centroid. And then the next thing that I'll do is I'm going to update the centroid. So now, this is going to move the centroids to the, the mean of all of the associated points. So in particular, I, I expect this green point to be pulled over to the right by the fact that we have so many points over here. So let's update. Now this is starting to look much better. If we were to just leave everything as is, you can see how the clustering was before. So now all these points that use to be green are now about to become red. And likewise with a few blue points over here. You can see how even just in one step from this bad initial condition, we've already started to capture the structure in the data pretty well. So I'm going to reassign the points. Iterate through this again to reassign each point to the nearest centroid. And now things are starting to look very, very consistent. There's probably just one, one or two more iterations before we have the centroid's right at the middle of the clusters so I update and reassign points. No points have changed so this is the final clustering that would be assigned by k-means clustering. So in three or four steps, using this algorithm, I assigned every point to a cluster and it worked in a really beautiful way for this example.

### 12. K-Means Clustering Visualization 3-WfwX3B4d8_I.en

Now I'm going to show you another set of data that won't work out quite so perfectly, but you can see how k-means clustering is still. And the type of data that I'll use in this example is uniform points. This is what uniform points look like. It's just scattered everywhere. So I wouldn't look at this and say there's clear clusters in here that I want to pick out, but I might still want to be able to describe that, say, these points over here are all more similar to each other than these points over there. And k-means clustering could be one way of mathematically describing that, that fact about the data. So I don't a priori have a number of centroids that I know I want to use here, so I'll use two. Seems like a reasonable number. One, two. And then let's see what happens in this case. Few points are going to be reassigned. Move the centroids. If you can see that there's a few more little adjustments here. But in the end, it basically just ends up splitting the data along this axis. If I try this again, depending on the exact initial conditions that I have and the exact details of how these points are allocated, I can come up with something that looks a little bit different. So you can see here that I ended up splitting the data vertically rather than horizontally. And the way you should think about this is the initial placement of the centroids is usually pretty random and very important. And so depending on what exactly the initial conditions are, you can get clustering in the end that looks totally different. Now, this might seem like a big problem, but there is one pretty powerful way to solve it. So let's talk about that.

### 13. Sklearn-3zHUAXcoZ7c.en

Now that I've explained the theory of k-means clustering to you, I'm going to show you how to use the scikit-learn implementation to deploy it in your own studies. So I start over here at Google, and I find that there's a whole page on clustering in scikit-learn. The first thing that I notice when I get to this page is that there are many types of clustering, besides just k-means clustering. So all of these different columns right here are different types of clustering. We won't go into all of these, instead I want to use this page to navigate to the k-means documentation that you can get a little bit of a better idea of how this is handled in scikit-learn. So here's a list of all of the different clustering methods that I have. And here the first item on the list we see is k-means, and some summary information about the algorithm. And so one of the parameters that you have to define for k-means is the number of clusters. Remember, we had to say at the outset how many clusters we want to look for and this is one of the things that can be most challenging actually about using k-means is deciding how many clusters you want to try. Then he gives us some information about the scalability, which basically tells us how the algorithm performs as you start to have lots and lots of data, or lots of clusters. A use case, which gives us a little bit of information that this is good for general purpose when you have clusters that have even number of points in them and so on. And, last, that the way that k-means clustering works is based on the distances between the points. So, very consistent with what we've seen so far. Let's dig in a little bit deeper. Now we're at the k-means documentation page. And there are three parameters in particular that I want to call your attention do. First and most important one is n_clusters. The default value for n_clusters is eight. But of course we know that the number of clusters in the algorithm is something that you need to set on your own based on what you think makes sense. This might even be a parameter that you play around with. So you should always be thinking about whether you actually want to use this default value, or if you want to change it to something else. I can guarantee you that you're mostly going to want to change it to something else. The second parameter that I want to call your attention to is max_iter=300. Remember that when we're running k-means clustering we have an iteration that we go through as we're finding the clusters, where we assign each point to a centroid and then we move the centroid. Then we assign the points again. We move the centroids again. And each one of those assign and move, assign and move steps is an iteration of the algorithm. And so max_iter actually says how many iterations of the algorithm do you want it to go through. 300 will usually be a very reasonable value for you. In fact most of the time I would guess that it's going to terminate before it gets to this maximum number. But if you want to have a finer level of control over the algorithm and how many times it goes through that iteration process this is the parameter that you want. And then the last parameter that I'll mention, another one that's very important. Is the number of different initializations that you give it. Remember we said that k-means clustering has this challenge, that depending on exactly what the initial conditions are, you can sometimes end up with different clusterings. And so then you want to repeat the algorithm several times so that any one of those clusterings might be wrong, but in general, the ensemble of all the clusterings will give you something that makes sense. That's what this parameter controls. It's basically how many times does it initialize the algorithm, how many times does it come up with clusters. You can see that by default it goes through at ten times. If you think for some reason that your clustering might be particularly prone to bad initializations or challenging initializations, then this is the parameter that you want to change. Maybe bump the number of initializations up to a higher number. But again, just to reiterate, of all those parameters, number of clusters is definitely the one that's most important. And that you should be playing around with and thinking really hard about the most.

### 14. Some challenges of k-means-e2CdlG5P4WA.en

Now this wraps up what we're going to talk about in terms of the k-means algorithm. What I'll have you do is practice much more in the coding aspects of this in the mini project. But before we do that, here are few thoughts on things that k-means is very valuable for and a few places where you need to be careful if you're going to try to use it.

### 15. Limitations of K-Means-4Fkfu37el_k.en

So now we look at the limits of what k-means can or cannot do, and you're going to try to break it. And specifically, talk about local minima and to do this, I want to ask you a question that you can think about and see if you get the answer right. Suppose you use a fixed number of cluster centers, two or three or four. Will the output for any fixed training set, always be the same? So given a fixed data set, given a fixed number of cluster centers, when you run k-means will you always arrive at the same result? Take your best guess.

### 15. Limitations of K-Means-nvLhUSSUhiY.en

And the answer is no, as I will illustrate to you. K-means is what's called a hill climbing algorithm, and as a result it's very dependent on where you put your initial cluster centers.

### 16. Counterintuitive Clusters-StmEUgT1XSY.en

And the answer is positive, and I prove it to you. Suppose you put one cluster center right between those two points over here and the other two somewhere in here. It doesn't even have an error. In your assignment step, you will find that pretty much everything left of this line would be allocated to the left cluster center. And as a result, this is the point where the total rubber band distance is minimized. So this cluster is very stable. These two guys over here, however, separate between themselves the data on the right. And they will fight for the same data points and end up somewhere partitioning the cloud on the right side. And that is a stable solution because in the assignment step, nothing changes. This guy will still correspond to all the guys over here, and these guys will correspond to the guys over here. That's called a local minimum. And it really depends on the initialization of the cluster centers. If you had chosen these three cluster centers as your initial guesses, you would never move away from it. Thus, make sure it's really important in clustering to be aware of the fact it's a local hill climbing algorithm. And it can give you suboptimal solutions that, if you divide them again, it gives you a better solution. Obviously, in this case with three cluster centers, you want them over here, over here and just one on the right side over here.

### 16. Counterintuitive Clusters-aveIz1JYeAg.en

So let's make another data set. In this case, you're going to pick three cluster centers and, then, conveniently, we'll draw three clusters onto my diagram. Obviously, for three cluster centers, you want a cluster to be here, right here, and right over here. So my question is, is it possible that all these data points over here are represented by one cluster, and these guys over here by two separate clusters. Given what you know about k-means, do you think it can happen that all these points here fall into one cluster and those two fall into two clusters as one what's called a local minimum for clustering.

### 17. Counterintuitive Clusters 2-HyjBus7S2gY.en

And I would say the answer's yes. You could make it so that the cluster centers sit right on top of each other, and the separation line looks like this. And all the top points are associated to the top cluster center, and all the bottom points are associated to the bottom cluster center. Granted, it's unlikely, to have init-, ,initialization like this, but if it happens, then the algorithm would believe this is one cluster. And this is another cluster. If we re-run it and you initialize differently. Say one of the cluster centers sits over here. Then a separation line will fall like that. And the classes would automatically resolve themselves. It's unlikely, but there exists a bad local minimum, even in the example I showed you over here. Now as a rule of thumb, the more cluster centers you have, the more local minima you find. But they exist, as a result, you are forced to run the algorithm multiple times.

### 17. Counterintuitive Clusters 2-xSQTzAeeoEc.en

Let me give another example and ask you a quiz. Suppose we have data just like this over here. Do you think there could be a local minimum if you initialize this data set with two cluster centers? Is there a stable solution where which the two cluster would not end up one over here and one over here? Or put differently, is there, does there exist a bad local minimum? Yes or no?



## Part 07-Module 01-Lesson 04_Decision Trees

### 01. MLND SL DT 00 Intro V2-l34ijtQhVNk.en

Hello, and welcome to the Decision Trees section. Let me introduce the concept of decision trees by playing this fun game on the Internet. It's called the Akinator. And the way it works is the genie will ask you questions about some character, and based on these questions, it'll guess who it is. The questions, as you can see, will get more and more educated as the genie narrowed down who the person is. This is the exact same thing decisions trees do. They ask you questions and questions about the data until they narrow the information down well enough to make a prediction. So let's play it, and I'll choose one of my favorite character from history, the great mathematician Hypatia. And here we have her Wikipedia page to help us out, so let's start answering questions. First question, is your character an adult man? Nope. Is your character older than 18? Yes, way older than 18. Has your character ever been pregnant? Well, it says there were no kids known, so I'm going to go for no. Is your character a YouTuber? Well, she would have been a wonderful Youtuber, but she was born way before Youtube was a thing, so no. Has your character recorded any albums? Again, no. Does your character have a cell phone? Nope, way before cell phones. Has your character really existed? Of course. Is your character a citizen of the United States? Nope, she was Egyptian. Has your character ever been married? Well, Wikipedia is not sure, so I'm going to go for a 'don't know'. Has your character been dead for more than a hundred years? Yes, way more than a hundred years. Is your character in the Bible? No. Is your character an orphan? I don't think so. Her dad was known, so no. Is your character European? Well, she was in the Roman Empire but she was born in Egypt, so I'm going to go for no. Is your character from Eastern Europe? Again, no. Oh, we're getting there. Is your character Egyptian? Yes. Is your character obsessed with waffles? No. Is your character a woman? Yes. Did your character know Cleopatra? Well, they didn't live in the same period of time, so no. Was your character a Pharaoh? No, she wasn't. Does your character live in Utah? No. Is your character a queen? No, she was a great mathematician but not a queen. Was your character a murderer? Yes, she was brutally murdered. Is your character a princess? No. Can your character cast spells? No. Is your character bad? No, she was very good. Oh, and the genie did it. He found the Hypatia of Alexandria. Good job, genie. The way decision trees work is very similar to these examples. So, let's dive deeper into them and learn how they work and how they get built.

### 02. MLND SL DT 01 Recommending Apps 1 MAIN V3-uI_yNrqqKVg.en

So, let's start with an example. Let's say we're in charge of writing the recommendation engine for the App Store or for Google play. Our task is to recommend to people the app they're most likely to download, and we should do this based on previous data. Our previous data is this table with six people each in a row, and the columns are their gender, male or female, their occupation, work or study, and the app they downloaded. The options for the app are Pokemon Go, WhatsApp, and Snapchat. So, the model we'll create will take the first two columns and guess the third one. So let's start with some small quizzes to test our intuition with this data. The first quiz is the following: if we have a woman who works at an office, what app should we recommend to her, Pokemon Go, WhatsApp, or Snapchat? The second quiz is the following: if we have a man who works at a factory, what app should we recommend to him? And the third one says, if we have a girl who's in high school, what app do we recommend to her? Enter your answers below.

### 03. MLND SL DT 02 Recommending Apps 2 MAIN V3-KSrIYqKZwCA.en

Well, let's see. A woman who works at an office. In the table, there's two women who work and both downloaded WhatsApp. So, let's say, it's safe to assume that recommending WhatsApp to this new person is the best idea. Now, for the man who works at a factory, we'll see that there's another man in the table who works and he downloaded Snapchat. So, we'll proceed to recommend Snapchat to the new person. And finally, the girl that is in high school. Well, we see that in the table, there are three students and they all downloaded Pokemon Go. So, we'll go ahead and recommend Pokemon Go to this new person. But now, that's how a human thinks. For the computer, it's harder to repeat this procedure. However, the computer can ask a slightly more technical question like this one: between gender and occupation, which one seems more decisive for predicting what app will the users download? This question sounds slightly ambiguous, but if you think about it, the answer does make sense. Let's give it a try.

### 04. Recommending Apps-nEvW8B1HNq4.en

Okay. Well, let's see what happens if we split them by gender. If we split them by gender, we see that the women downloaded WhatsApp and Pokemon Go, while the men downloaded Snapchat and Pokemon Go. This tells us a bit, but not much. On the other hand, if split by occupation, we can see that the students all downloaded Pokemon Go, whereas, the people who work downloaded other apps. This is a good piece of information since from now on, whenever a student comes in, we'll recommend them Pokemon Go. Thus, occupation is a better feature here for predicting what app will the users download. So, we can go ahead and make that decision. We'll do it by creating a node here that says, "To everybody that goes to school, we'll recommend Pokemon Go. And for the ones that work, let's see." If we forget about the students, now will look at the people who work. And now, it turns out that the gender split will help us. Because the women downloaded WhatsApp, and the men downloaded Snapchat. So, let's do that. Let's add another node here. The new node says "If you work, then I'll ask for your gender. And if the gender is female, we'll recommend WhatsApp. And if the gender is male, we'll recommend Snapchat." And we're done. So quick summary. If we got a student, well recommend them Pokemon Go. If we get someone who works, we'll ask for the gender. If it's a woman, we'll recommend her WhatsApp. And if it's a man, we'll recommend him Snapchat. Now, what we need to figure out is, how do we get the computer to measure the two features, and figure out that occupation is a better feature to split by? We will learn that later.

### 05. MLND SL DT 04 Q Student Admissions V3 MAIN V1-MOa335cQGI4.en

Now, in the last example, we constructed a tree with categorical features, namely gender and occupation, but we can also create a tree with continuous features. Let's go to this example, which you may see in other parts of this class. The example is an Admissions Office which takes two pieces of data from the students, their score in a test and their grades, and the better they do on them, the more likely that they'll be accepted at a university. So in here, the blue points are accepted and the red points are rejected, and our model will have the task of determining a rule for future students to be accepted or rejected. So, let's ask the same question as before. Between grades and tests, which one determines student acceptance better? And in order to help you out, let me translate this sentence into graphical terms. Since test is the horizontal axis and grades is the vertical axis, then the question becomes, between a horizontal and a vertical line, which one would cut the data in a better way, namely separating the red and the blue points as much as possible? Give this one a try.

### 06. Student Admissions-TdgBi6LtOB8.en

Well, let's try both. The best horizontal line would be somewhere around here. It does an okay job, but it doesn't really separate the points that well, at least a lot of blue points in the red area and vice versa. So, what happens if we try a vertical line? Well, it seems that the best cut is here around five. That does a pretty good job and only leaves five red points on the blue side and five blue points on the red side. So, let's go for that one, and our answer is vertical line. This means that the best feature to separate this data is test, and the best threshold is five. Therefore, we can add our first node to the decision tree and this node asks, "Is your test greater than or equal to five or is it less than five?" And now, we can do even more. We can try dividing each of the two halves with a horizontal line, which is the equivalent of saying, "Okay. I've seen your tests. Now, let's see how you did in the grades." The left half can be cut with a vertical line over here at height seven. This means, if your test score is less than five, then you need seven or more in the grades to get accepted, otherwise, you get rejected. The right half can be separated with a vertical line at height two. This means, if your test score is greater than or equal to five, then you need to have two or more in your grades in order to get accepted, otherwise, you get rejected. So, we've built our Decision Tree in a similar way as before, except now at each node, we don't have a yes/no question, but we have a threshold which would cut the values in two.

### 07. Entropy-piLpj1V1HEk.en

Now in order to go further with Decision Trees, we need to learn an important concept called entropy. Entropy comes from physics and to explain it, we'll use the example of the three states of water. These are solid, which is ice, liquid, and gas, which is water vapor. Let's think of the particles inside ice, water, and vapor. Ice is pretty rigid in that its particles don't have many places to go. They mostly stay where they are. Water is a little less rigid in which a particle has a few places to move around. And water vapor is in the other end of the spectrum. A particle has many possibilities of where to go and can move around a lot. So, entropy measures precisely this, how much freedom does a particle have to move around? Thus, the entropy of ice is low, the entropy of liquid water is medium, and the entropy of water vapor is high. The notion of entropy can also work in probability. Let's look at these three configurations of balls inside buckets. The first bucket has four red balls. The second one has three red and one blue, and the third one has two red and two blue. And let's say balls from each color are completely indistinguishable. So we could say that entropy is given by how much balls are allowed to move around if we put them in a line. We can see that the first bucket is very rigid. No matter how we organize the balls, we always get the same state, so it has low entropy. In the second one, we can reorganize the balls in four ways, so it has medium entropy. For the third one, we have six ways of reorganizing the balls, so it has high entropy. This is not the exact definition of entropy, but it gives us an idea, that the more rigid the set is or the more homogeneous, the less entropy you'll have, and vice versa. Another way to see entropy is in terms of knowledge. If we were to pick a random ball from each of the buckets, how much do we know about the color of this ball? In the first bucket, we know for sure that the ball is red, so we have high knowledge. In the second bucket, it's very likely to be red and not very likely to be blue. So if we bet that it's red, we'll be right most of the time. So we have medium knowledge of the color of the ball. In the third bucket, we know much less since it's equally likely to be blue or red. So here, we have low knowledge. And it turns out that knowledge and entropy are opposites. The more knowledge one has, the less entropy, and vice versa. Thus, we conclude that the first bucket has low entropy, the second one has medium entropy, and the third one has high entropy. Now when I say opposites, I don't mean additive inverse or multiplicative inverses. I only mean it in the colloquial sense of the word. When one of them is big, then the other one is small, and vice versa. Over the next few videos, we'll cook up a formula for entropy, namely, one that gives us low, medium, and high values for these buckets.

### 08. Entropy Formula-iZiSYrOKvpo.en

In order to cook up a formula for entropy, we'll consider the following game. In this game, we'll start with a configuration of balls; say red, red, red, and blue, and we'll put them inside the bucket. Now, what we do is we pick four balls out of this bucket with repetition and we try to get the initial configuration; red, red, red, blue and if we do, we win. Let's say we pick our first ball and it's red. We record the color and put it back. Then, we pick a second ball and it's red. So, we again record the color and put it back. Then, we pick a third ball, say red and record the color and put it back. Notice that we pick the same ball again. This is completely okay. Finally, we pick a fourth ball. Now, we got blue, we record it, and put it back. We recorded red, red, red, blue. So, we win a lot of money. If say we got the colors red, blue, blue, red, then we will win no money at all. That's the game. Now, the quiz question is, which one of the three buckets is the best to play the game with and which one is the worst? Enter your answer below.

### 09. MLND SL DT 08 Entropy Formula 2 MAIN V2-6GHg70hrSJw.en

Well, it seems that the first bucket is the best one, because no matter what we do, we'll always pick red, red, red, red so we'll win every time. We can see that although it's not very easy to win in any of the other two, it's easier to pick red, red, red, blue in the second one, and much harder to get red, red, blue, blue in the third one. Thus, the answers are: the best bucket is the first one, the next one is okay, and the third one is the worst. But by how much more specifically? Let's ask the following question. What is the probability of winning in each of these games? So, let's start by the easy one. How likely is it to winning this game? Well, to get the first ball to be red, the probability is actually one. Same thing for the second one, the third one, and the fourth one. Since we put the ball back after recording each color, then these events are completely independent. So, the probability that they all occur is the product of the four probabilities. This means the probability is one, which matches our intuition that no matter what we do, we'll always pick red, red, red, red. Now let's go to the red, red, red, blue case. What is the probability that the first ball we'd pick is red? Well, it's three over four or 0.75 since there are three red balls, and four in total. Same thing for the second, and the third balls. Now what's the probability of the fourth ball we pick his blue? Well now it's one over four since there's only one blue ball among four. Therefore again, since the events are independent the probability of the four of them happening is the product of the four probabilities, which is 0.75 times 0.75 times 0.75 times 0.25. This is 0.105 or around ten 10 percent probability of winning here. And for the last one, well the chances here of getting a red ball are 50 percent, since there are two red balls and two blue balls. And the chance of getting a blue ball are the same. Thus, the chance of these balls being red, red, blue, and blue is the product, which is 0.0625 or roughly six percent for winning in this game. We summarize these results in the table over here, where the first column has a probability of a ball being red, the second one off the ball being blue, and the last one we highlight the probability of winning. Now products are confusing mainly for two reasons. The first one is that if we have, say a a thousand balls. Now, we take the product of a thousand numbers all between zero and one. This could be very tiny. The other reason is that a small change in one of the factors could drastically alter their product. We want something more manageable. And what's better than products? Let's ask our friend here. Yes, he is right. Sums are better than products. And now we just need to turn the products into sums. Would any of the following functions be able to help us? The options are sine, cosine, logarithm, or exponential. Enter your answer below.

### 10. Entropy Formula-w73JTBVeyjE.en

Correct logarithm is the answer since it satisfies that beautiful identity that says, the logarithm of a product is the sum of the logarithms. Thus, our product numbers becomes a sum of the logarithms of the numbers. In this case, we get minus 3.245. Now in this class, we'll be using log as a logarithm base two, and the reason is information theory. So here's our summary. We have our three configuration of red and blue balls. Now we have the probability of the ball being red and blue, and the product of them according to the sequence, which is the probability of winning the game. In the next column, we'll take the logarithm base two. But since the numbers are less than one, the logarithm is negative. Thus, if we take the negative of the whole equation, we're now dealing with positive numbers. And in the last column, we'll just divide by four because what we'll use as definition of entropy is the average of the negatives of the logarithms of the probabilities of picking the balls in a way that we win the game. Thus, for the first bucket, we get 0 entropy, for the second one we get 0.81, and for the third one we get one. This is going to be our formula for entropy. In the slightly more general case of five red balls and three blue balls, we get the following. The negative of the sum of five times the logarithm of the probability of picking a red ball, which is five over eight, and three times the logarithm of the probability of picking a blue ball, which is three over eight. We can see that this is a large number since this set has a lot of entropy. In the more general case with m red balls and n blue balls, this is the formula. As the probability of picking a red ball is m divided by m plus n, and for a blue ball it's n divided by n plus n. And this is the general formula for entropy when the balls can be of two colors.

### 12. MLND SL DT 10 Q Information Gain MAIN V1-tVLOLPEtLFw.en

Okay, so now, we'll use what we know about entropy and information gain to build Decision Trees. Let's say, we have our data in the form of these red and blue points and we want to split it into two. So, I'll show you three ways of splitting it, and here are the three ways. Now, let's have a small quiz: In which of these do you think we have gained more information about our data and in which one did we gain less information? Enter your answers below.

### 13. Information Gain-k9iZL53PAmw.en

Correct. The first way to split them does not help us at all. We ended up with very similar sets of red and blue points and we learn nothing about the data. The second way did a decent job. We managed to get most of the blue ones on one side and most of the red ones on the other side. So, now we know a bit more about it. And the third one did fantastic. It managed to send all the blue points on one side and all the red ones on the other side. Now we know a lot about our data. Actually, we'll learn how to calculate something called the information gain, and it will be zero for the first splitting, 0.28 for the second, and one for the third one. And the formula for information gain is very simple. It's just the change in entropy. Let me be more specific. For every node in the Decision Tree, we can calculate the entropy of the data in the parent node. And then, we calculate the entropies of the two children. The information gain is a difference between the entropy of the parent and the average entropy of the children. So, in our first example, you can calculate the entropy of the parent and its one. The entropy of each of the children is 0.72. So the average of the entropy of the children is actually 0.72. Thus, the change in entropy is one minus 0.72, which is 0.28. For this example, now, we can calculate the entropies of the children which are both one. Thus, the change in entropy is one minus one which is zero. This is a terrible splitting because they gave us zero information. And finally, in this example, the entropy of the children are both zero, since as we saw, a set where the points are the same color has zero entropy. Thus, the information gain is one minus zero which is one. This split gave us the largest information gain and as you can see, it is the best split because it managed to perfectly cut the data into the blue points and red points. So, here's our summary. The three cuts with the values of information gain. If the tree had to choose, it would go for the third cut, which is the one that gives it the largest amount of information gain.

### 14. Maximizing Information Gain-3FgJOpKfdY8.en

Okay, so now, let's go ahead and build a Decision Tree. Our algorithm will be very simple. Look at the possible splits that each column gives, calculate the information gain, and pick the largest one. So, let's calculate the entropy of the parent, which is this data. We'll calculate the entropy of the column of the labels. So there are three Pokemon Go's, two WhatsApp, and one Snapchat. The entropy is negative three over six, logarithm base two of three over six, minus two over six, logarithm base two of two over six, minus one over six, logarithm base two of one over six. This gives us 1.46. Now, if we split them by gender, we get two sets, one with one Pokemon Go and two WhatsApp, and one with one Snapchat and two Pokemon Go. The entropies for these sets are both 0.92. Thus, the average entropy of the children of this node is 0.92, and the information gain is 1.46 minus 0.92, which is zero point 54. Now, if we split by occupation, we get one set of three Pokemon Go's, and one of two WhatsApps, and one Snapchat. The first set has entropy zero, and the other has entropy 0.92. Therefore, the average of these is 0.46, and the information gain is 1.46 minus 0.46, which is one. So to summarize, splitting by the gender column gave us an information gain of 0.54, and splitting by the occupation column gave us an information gain of one. The algorithm says, pick the column with the highest information gain, which is occupation. So we split by occupation. We'll get two sets. One is very nice, since everybody downloaded Pokemon Go, and in the other one, we can still do better. We can split based on the gender column. Since now, we get two very nice sets, one where everybody downloaded WhatsApp, and the other one where everybody downloaded Snapchat. And we're done, here's our Decision Tree. If we want to do this for continuous features instead of discrete features, we can still do it. We'll let you think about the details. So basically, the idea is to think of all the possible vertical and horizontal cuts, and seeing which one maximizes the entropy, and then iterating over and over as we build a Decision Tree. Here, we can see that our first cut is vertical at the value five. Our next cut will be a horizontal cut at the height seven. And our final cut will be horizontal at height two. And finally, we have our Decision Tree that cuts our data in two.

### 15. MLND SL DT 13 Random Forests MAIN V1-n5DhXhcYKcw.en

Now, here's a potential problem with Decision Trees. Let's say we have a humungous table with lots and lots of columns. So, we create our Decision Tree and let's say it looks like this. This is not a realistic tree, though, just an example. And we end up with answers like the following. If a client is male between 15 and 25 in the US, on Android, in school, likes tennis, pizza, but does not like long walks on the beach, then they're likely to download Pokemon Go. This is not good. This almost looks like the tree just memorized the data. It's overfitting. Decision Trees tend to overfit a lot. In the continuous case, this can also happen and it looks like this. The Decision Tree has many nodes which end up giving us a complicated boundary that pretty much borders every point with a small square. This is also overfitting as it doesn't generalize well to the data. So, how do we solve this? In the simplest possible way. Take a look at this. Let's take our data and say, pick some of the columns randomly. Build a Decision Tree in those columns. Now, pick some other columns randomly and build a Decision Tree in those, and do it again. And now, just let the trees vote. When we have a new data point, say this person over here, we just let all the trees make a prediction and pick the one that appears the most. For example, these trees decided that this person will download Snapchat, WhatsApp, and WhatsApp. So, the ensemble of trees will recommend WhatsApp. Since we used a bunch of trees on randomly picked columns, this is called a random forest. There are better ways to pick the columns than randomly and we'll see this in the ensemble methods section of this Nanodegree.

## Part 08-Module 01-Lesson 01_Introduction to Neural Networks

### 02. Introduction-tn-CrUTkCUc.en

So let's start with two questions, what is deep learning, and what is it used for? The answer to the second question is pretty much everywhere. Recent applications include things such as beating humans in games such as Go, or even jeopardy, detecting spam in emails, forecasting stock prices, recognizing images in a picture, and even diagnosing illnesses sometimes with more precision than doctors. And of course, one of the most celebrated applications of deep learning is in self-driving cars. And what is at the heart of deep learning? This wonderful object called neural networks. Neural networks vaguely mimic the process of how the brain operates, with neurons that fire bits of information. It sounds pretty scary, right? As a matter of fact, the first time I heard of a neural network, this is the image that came into my head, some scary robot with artificial brain. But then, I got to learn a bit more about neural networks and I realized that there are actually a lot scarier than that. This is how a neural network looks. As a matter of fact, this one here is a deep neural network. Has lots of nodes, lots of edges, lots of layers, information coming through the nodes and leaving, it's quite complicated. But after looking at neural networks for a while, I realized that they're actually a lot simpler than that. When I think of a neural network, this is actually the image that comes to my mind. There is a child playing in the sand, with some red and blue shells and we are the child. Can you draw a line that separates the red and the blue shells? And the child draws this line. That's it. That's what a neural network does. Given some data in the form of blue or red points, the neural network will look for the best line that separates them. And if the data is a bit more complicated like this one over here, then we'll need a more complicated algorithm. Here, a deep neural network will do the job and find a more complex boundary that separates the points. So with that image in mind, let's dive in and learn about neural networks.

### 03. Exemplo de classificao-Dh625piH7Z0.en

So, let's start with one classification example. Let's say we are the admissions office at a university and our job is to accept or reject students. So, in order to evaluate students, we have two pieces of information, the results of a test and their grades in school. So, let's take a look at some sample students. We'll start with Student 1 who got 9 out of 10 in the test and 8 out of 10 in the grades. That student did quite well and got accepted. Then we have Student 2 who got 3 out of 10 in the test and 4 out of 10 in the grades, and that student got rejected. And now, we have a new Student 3 who got 7 out of 10 in the test and 6 out of 10 in the grades, and we're wondering if the student gets accepted or not. So, our first way to find this out is to plot students in a graph with the horizontal axis corresponding to the score on the test and the vertical axis corresponding to the grades, and the students would fit here. The students who got three and four gets located in the point with coordinates (3,4), and the student who got nine and eight gets located in the point with coordinates (9,8). And now we'll do what we do in most of our algorithms, which is to look at the previous data. This is how the previous data looks. These are all the previous students who got accepted or rejected. The blue points correspond to students that got accepted, and the red points to students that got rejected. So we can see in this diagram that the students would did well in the test and grades are more likely to get accepted, and the students who did poorly in both are more likely to get rejected. So let's start with a quiz. The quiz says, does the Student 3 get accepted or rejected? What do you think? Enter your answer below.

### 04.  2 -46PywnGa_cQ.en

Correct. Well, it seems that this data can be nicely separated by a line which is this line over here, and it seems that most students over the line get accepted and most students under the line get rejected. So this line is going to be our model. The model makes a couple of mistakes since there are a few blue points that are under the line and a few red points over the line. But we're not going to care about those. I will say that it's safe to predict that if a point is over the line the student gets accepted and if it's under the line then the student gets rejected. So based on this model we'll look at the new student that we see that they are over here at the point 7:6 which is above the line. So we can assume with some confidence that the student gets accepted. So if you answered yes, that's the correct answer. And now a question arises. The question is, how do we find this line? So we can kind of eyeball it. But the computer can't. We'll dedicate the rest of the session to show you algorithms that will find this line, not only for this example, but for much more general and complicated cases.

### 05. Linear Boundaries-X-uMlsBi07k.en

So, first let's add some math. We're going to label the horizontal axis corresponding to the test by the variable x1, and the vertical axis corresponding to the grades by the variable x2. So this boundary line that separates the blue and the red points is going to have a linear equation. The one drawn has equation 2x1+x2-18=0. What does this mean? This means that our method for accepting or rejecting students simply says the following: take this equation as our score, the score is 2xtest+grades-18. Now when the student comes in, we check their score. If their score is a positive number, then we accept the student and if the score is a negative number then we reject the student. This is called a prediction. We can say by convention that if the score is 0, we'll accept a student although this won't matter much at the end. And that's it. That linear equation is our model. In the more general case, our boundary will be an equation of the following wx1+w2x2+b=0. We'll abbreviate this equation in vector notation as wx+b=0, where w is the vector w1w2 and x is the vector x1x2. And we simply take the product of the two vectors. We'll refer to x as the input, to w as the weights and b as the bias. Now, for a student coordinates x1x2, we'll denote a label as Y and the label is what we're trying to predict. So if the student gets accepted, namely the point is blue, then the label is Y+1. And if the student gets rejected, namely the point is red and then the label is Y=0. Thus, each point is in the form x1x2Y or Y is 1 for the blue points and 0 for the red points. And finally, our prediction is going to be called Y-hat and it will be what the algorithm predicts that the label will be. In this case, Y-hat is one of the algorithm predicts that the student gets accepted, which means the point lies over the line. And, Y-hat is 0 if the algorithm predicts that this didn't get rejected, which means the point is under the line. In math terms, this means that the prediction Y-hat is 1 if wx+b is greater than or equal to zero and 0 if wx+b is less than 0. So, to summarize, the points above the line have Y hat=1 and the points below the line have Y-hat=0. And, the blue points have Y=1 and the red points have Y=0. And, the goal of the algorithm is to have Y-hat resembling Y as closely as possible, which is exactly equivalent to finding the boundary line that keeps most of the blue points above it and most of the red points below it.

### 06. 09 Higher Dimensions-eBHunImDmWw.en

Now, you may be wondering what happens if we have more data columns so not just testing grades, but maybe something else like the ranking of the student in the class. How do we fit three columns of data? Well the only difference is that now, we won't be working in two dimensions, we'll be working in three. So now, we have three axis: x_1 for the test, x_2 for the grades and x_3 for the class ranking. And our data will look like this, like a bunch of blue and red points flying around in 3D. On our equation won't be a line in two dimension, but a plane in three dimensions with a similar equation as before. Now, the equation would be w_1_x_1 plus w_2_x_2 plus w_3_x_3 plus b equals zero, which will separate this space into two regions. This equation can still be abbreviated by Wx plus b equals zero, except our vectors will now have three entries instead of two. And our prediction will still be y head equals one if Wx plus b is greater than or equal to zero, and zero if Wx plus b is less than zero. And what if we have many columns like say n of them? Well, it's the same thing. Now, our data just leaps in n-dimensional space. Now, I have trouble picturing things in more than three dimensions. But if we can imagine that the points are just things with n coordinates called x_1, x_2, x_3 all the way up to x_n with our labels being y, then our boundaries just an n minus one dimensional hyperplane, which is a high dimensional equivalent of a line in 2D or a plane in 3D. And the equation of this n minus one dimensional hyperplane is going to be w_1_x_1 plus w_2_x_2 plus all the way to w_n_x_n plus b equals zero, which we can still abbreviate to Wx plus b equals zero, where our vectors now have n entries. And our prediction is still the same as before. It is y head equals one if Wx plus b is greater than or equal to zero and y head equals zero if Wx plus b is less than zero.

### 07. DL 06 Perceptron Definition Fix V2-hImSxZyRiOw.en

So let's recap. We have our data which is all these students. The blue ones have been accepted and the red ones have been rejected. And we have our model which consists of the equation two times test plus grades minus 18, which gives rise to this boundary which the point where the score is zero and a prediction. The prediction says that the student gets accepted of the score is positive or zero, and rejected if the score is negative. So now we'll introduce the notion of a preceptron, which is the building block of neural networks, and it's just an encoding of our equation into a small graph. The way we've build it is the following. Here we have our data and our boundary line and we fit it inside a node. And now we add small nodes for the inputs which, in this case, they are the test and the grades. Here we can see an example where test equals seven and grades equals six. And what the perceptron does is it blocks the points seven, six and checks if the point is in the positive or negative area. If the point is in the positive area, then it returns a yes. And if it is in the negative area, it returns and no. So let's recall that our equation is score equals two times test plus one times grade minus 18, and that our prediction consists of accepting the student if the score is positive or zero, and rejecting them if the score is negative. These weights two, one, and minus 18, are what define the linear equation, and so we'll use them as labels in the graph. The two and the one will label the edges coming from X1 and X 2 respectively, and the bias unit minus 18 will label the node. Thus, when we see a node with these labels, we can think of the linear equation they generate. Another way to grab this node is to consider the bias as part of the input. Now since W1 gets multiplied by X1 and W2 by X2, It's natural to think that B gets multiplied by a one. So we'll have the B labeling and and edge coming from a one. Then what the node does is it multiplies the values coming from the incoming nodes by the values and the corresponding edges. Then it adds them and finally, it checks if the result is greater that are equal to zero. If it is, then the node returns a yes or a value of one, and if it isn't then the node returns a no or a value of zero. We'll be using both notations throughout this class although the second one will be used more often. In the general case, this is how the nodes look. We will have our node over here then end inputs coming in with values X1 up to Xn and one, and edges with weights W1 up to Wn, and B corresponding to the bias unit. And then the node calculates the linear equation Wx plus B, which is a summation from I equals one to n, of WIXI plus B. This node then checks if the value is zero or bigger, and if it is, then the node returns a value of one for yes and if not, then it returns a value of zero for no. Note that we're using an implicit function, here, which is called a step function. What the step function does is it returns a one if the input is positive or zero, and a zero if the input is negative. So in reality, these perceptrons can be seen as a combination of nodes, where the first node calculates a linear equation and the inputs on the weights, and the second node applies the step function to the result. These can be graphed as follows: the summation sign represents a linear function in the first node, and the drawing represents a step function in the second node. In the future, we will use different step functions. So this is why it's useful to specify it in the node. So as we've seen there are two ways to represent perceptions. The one on the left has a bias unit coming from an input node with a value of one, and the one in the right has the bias inside the node.

### 08. -zAkzOZntK6Y.en

So you may be wondering why are these objects called neural networks. Well, the reason why they're called neural networks is because perceptions kind of look like neurons in the brain. In the left we have a perception with four inputs. The number is one, zero, four, and minus two. And what the perception does, it calculates some equations on the input and decides to return a one or a zero. In a similar way neurons in the brain take inputs coming from the dendrites. These inputs are nervous impulses. So what the neuron does is it does something with the nervous impulses and then it decides if it outputs a nervous impulse or not through the axon. The way we'll create neural networks later in this lesson is by concatenating these perceptions so we'll be mimicking the way the brain connects neurons by taking the output from one and turning it into the input for another one.

### 10. 07 Perceptron Algorithm Trick-lif_qPmXvWA.en

Now, let me show you a trick that will make a line go closer to a point. Let's say we have our linear equation for example, 3x1 + 4x2 -10. And that linear equation gives us a line which is the points where the equation is zero and two regions. The positive region drawn in blue where 3x1 + 4x2 - 10 is positive, and the negative region drawn in red with 3x1 + 4x2 - 10 is negative. So here we have our lonely misclassified point, the 0.4, 5 which is a red point in the blue area, and the point has to come closer. So how do we get that point to come closer to the line? Well, the idea is we're going to take the four and five and use them to modify the equation of the line in order to get the line to move closer to the point. So here are parameters of the line 3, 4 and -10 and the coordinates of the point are 4 and 5, and let's also add a one here for the bias unit. So what we'll do is subtract these numbers from the parameters of the line to get 3 - 4, 4 - 5, and -10 -1. The new line will have parameters -1, -1, -11. And this line will move drastically towards the point, possibly even going over it and placing it in the correct area. Now, since we have a lot of other points, we don't want to make any drastic moves since we may accidentally misclassify all our other points. We want the line to make a small move towards that point and for this, we need to take small steps towards the point. So here's where we introduce the learning rate, the learning rate is a small number for example, 0.1 and what we'll do is instead of subtracting four, five and one from the coordinates of the line, we'll multiply these numbers by 0.1 and then subtract them from the equation of the line. This means we'll be subtracting 0.4, 0.5, and 0.1 from the equation of the line. Obtaining a new equation of 2.6x1 + 3.5x 2 - 10.1 = 0. This new line will actually move closer to the point. In the same way, if we have a blue point in the red area, for example, the point 1,1 is a positively labeled point in the negative area. This point is also misclassified and it says, come closer. So what do we do here is the same thing, except now instead of subtracting the coordinates to the parameters of the line, we add them. Again, we multiply by the learning rate in order to make small steps. So here we take the coordinates of the point 1,1 and put an extra one for the constant term and now, we multiply them by the learning rates 0.1. Now, we add them to the parameters of the line and we get a new line with equation 3.1x1 + 4.1x2 - 9.9. And magic, this line is closer to the point. So that's the trick we're going to use repeatedly for the Perceptron Algorithm.

### 10. DL 10 S  Perceptron Algorithm-fATmrG2hQzI.en

Well, consider this. If you're in the wrong area, you would like the line to go over you, in order to be in the right area. Thus, the points just come closer! So the line can move towards it and eventually classify it correctly.

### 10. Perceptron Algorithm--zhTROHtscQ.en

So we had a question we're trying to answer and the question is, how do we find this line that separates the blue points from the red points in the best possible way? Let's answer this question by first looking at a small example with three blue points and three red points. And we're going to describe an algorithm that will find the line that splits these points properly. So the computer doesn't know where to start. It might as well start at a random place by picking a random linear equation. This equation will define a line and a positive and negative area given in blue and red respectively. What we're going to do is to look at how badly this line is doing and then move it around to try to get better and better. Now the question is, how do we find how badly this line is doing? So let's ask all the points. Here we have four points that are correctly classified. They are these two blue points in the blue area and these two red points in the red area. And these points are correctly classified, so they say, "I'm good." And then we have these two points that are incorrectly classified. That's this red point in the blue area and this blue point in the red area. We want to get as much information from them so we want them to tell us something so that we can improve this line. So what is it that they can tell us? So here we have a misclassified point, this red point in the blue area. Now think about this. If you were this point, what would you tell the line to do? Would you like it to come closer to you or farther from you? That's our quiz. Will the misclassified point want the line to come closer to it or farther from it?

### 11. Perceptron Agorithm Pseudocode-p8Q3yu9YqYk.en

Now, we finally have all the tools for describing the perceptron algorithm. We start with the random equation, which will determine some line, and two regions, the positive and the negative region. Now, we'll move this line around to get a better and better fit. So, we ask all the points how they're doing. The four correctly classified points say, "I'm good." And the two incorrectly classified points say, "Come closer." So, let's listen to the point in the right, and apply the trick to make the line closer to this point. So, here it is. Now, this point is good. Now, let's listen to the point in the left. The points says, "Come closer." We apply the trick, and now the line goes closer to it, and it actually goes over it classifying correctly. Now, every point is correctly classified and happy. So, let's actually write the pseudocode for this perceptron algorithm. We start with random weights, w1 up to wn and b. This gives us the question wx plus b, the line, and the positive and negative areas. Now, for every misclassified point with coordinates x1 up to xn, we do the following. If the prediction was zero, which means the point is a positive point in the negative area, then we'll update the weights as follows: for i equals 1 to n, we change wi, to wi plus alpha times xi, where alpha is the learning rate. In this case, we're using 0.1. Sometimes, we use 0.01 etc. It depends. Then we also change the bi as unit to b plus alpha. That moves the line closer to the misclassified point. Now, if the prediction was one, which means a point is a negative point in the positive area, then we'll update the weights in a similar way, except we subtract instead of adding. This means for i equals 1, change wi, to wi minus alpha xi, and change the bi as unit b to b minus alpha. And now, the line moves closer to our misclassified point. And now, we just repeat this step until we get no errors, or until we have a number of error that is small. Or simply we can just say, do the step a thousand times and stop. We'll see what are our options later in the class.

### 12. Non-Linear Regions-B8UrWnHh1Wc.en

Okay, so let's look more carefully at this model for accepting and rejecting students. Let's say we have this student four, who got nine in the test, but only one on the grades. According to our model this student gets accepted since it's placed over here in the positive region of this line. But let's say we don't want that since we'll say, "If your grades were terrible, no matter what you got on the test, you won't get accepted". So our data should look more like this instead. This model is much more realistic but now we have a problem which is the data can no longer be separated by just a line. So what is the next thing after a line? Maybe a circle. A circle would work. Maybe two lines. That could work, too. Or maybe a curve like this. That would also work. So let's go with that. Let's go with the curve. Now, unfortunately, the perceptron algorithm won't work for us this time. We'll have to come up with something more complex and actually the solution will be, we need to redefine our perceptron algorithm for a line in a way that it'll generalize to other types of curves.

### 13. Error Functions-YfUUunxWIJw.en

So the way we'll solve our problems from now on is with the help of an error function. An error function is simply something that tells us how far we are from the solution. For example, if I'm here and my goal is to get to this plant, an error function will just tell me the distance from the plant. My approach would then be to look around myself, check in which direction I can take a step to get closer to the plant, take that step and then repeat. Here the error is simply the distance from the plant.

### 14. Error Functions-jfKShxGAbok.en

Here is obvious realization of the error function. We're standing on top a mountain, Mount Errorest and I want to descend but it's not that easy because it's cloudy and the mountain is very big, so we can't really see the big picture. What we'll do to go down is we'll look around us and we consider all the possible directions in which we can walk. Then we pick a direction that makes us descend the most. Let's say it's this one over here. So we take a step in that direction. Thus, we've decreased the height. Once we take the step and we start the process again and again always decreasing the height until we go all the way down the mountain, minimizing the height. In this case the key metric that we use to solve the problem is the height. We'll call the height the error. The error is what's telling us how badly we're doing at the moment and how far we are from an ideal solution. And if we constantly take steps to decrease the error then we'll eventually solve our problem, descending from Mt. Errorest. Some of you may be thinking, wait, that doesn't necessarily solve the problem. What if I get stuck in a valley, a local minimum, but that's not the bottom of the mountain. This happens a lot in machine learning and we'll see different ways to solve it later in this Nanodegree. It's also worth noting that many times a local minimum will give us a pretty good solution to a problem. This method, which we'll study in more detail later, is called gradient descent. So let's try that approach to solve a problem. What would be a good error function here? What would be a good way to tell the computer how badly it's doing? Well, here's our line with our positive and negative area. And the question is how do we tell the computer how far it is from a perfect solution? Well, maybe we can count the number mistakes. There are two mistakes here. So that's our height. That's our error. So just as we did to descend from the mountain, we look around all the directions in which we can move the line in order to decrease our error. So let's say we move in this direction. We'll decrease the number of errors to one and then if we're moving in that direction, we'll decrease the number of errors to zero. And then we're done, right? Well, almost. There's a small problem with that approach. In our algorithms we'll be taking very small steps and the reason for that is calculus, because our tiny steps will be calculated by derivatives. So what happens if we take very small steps here? We start with two errors and then move a tiny amount and we're still at two errors. Then move a tiny amount again and we're still two errors. Another tiny amount and we're still at two and again and again. So not much we can do here. This is equivalent to using gradient descent to try to descend from an Aztec pyramid with flat steps. If we're standing here in the second floor, for the two errors and we look around ourselves, we'll always see two errors and we'll get confused and not know what to do. On the other hand in Mt. Errorest we can detect very small variations in height and we can figure out in what direction it can decrease the most. In math terms this means that in order for us to do gradient descent our error function can not be discrete, it should be continuous. Mt. Errorest is continuous since small variations in our position will translate to small variations in the height but the Aztec pyramid does not since the high jumps from two to one and then from one to zero. As a matter of fact, our error function needs to be differentiable, but we'll see that later. So, what we need to do here is to construct an error function that is continuous and we'll do this as follows. So here are six points with four of them correctly classified, that's two blue and two red, and two of them incorrectly classified, that is this red point at the very left and this blue point at the very right. The error function is going to assign a large penalty to the two incorrectly classified points and small penalties to the four correctly classified points. Here we are representing the size of the point as the penalty. The penalty is roughly the distance from the boundary when the point is misclassified and almost zero when the point is correctly classified. We'll learn the formula for the error later in the class. So, now we obtain the total error by adding all the errors from the corresponding points. Here we have a large number so it is two misclassified points add a large amount to the error. And the idea now is to move the line around in order to decrease these error. But now we can do it because we can make very tiny changes to the parameters of the line which will amount to very tiny changes in the error function. So, if you move the line, say, in this direction, we can see that some errors decrease, some slightly increase, but in general when we consider the sum, the sum gets smaller and we can see that because we've now correctly classified the two points that were misclassified before. So once we are able to build an error function with this property, we can now use gradient descent to solve our problem. So here's the full picture. Here we are at the summit of Mt. Errorest. We're quite high up because our error is large. As you can see the error is the height which is the sum of the blue and red areas. We explore around to see what direction brings us down the most, or equivalently, what direction can we move the line to reduce the error the most, and we take a step in that direction. So in the mountain we go down one step and in the graph we've reduced the error a bit by correctly classifying one of the points. And now we do it again. We calculate the error, we look around ourselves to see in what direction we descend the most, we take a step in that direction and that brings us down the mountain. So on the left we have reduced the height and successfully descended from the mountain and on the right we have reduced the error to its minimum possible value and successfully classified our points. Now the question is, how do we define this error function? That's what we'll do next.

### 15. Discrete vs Continuous-rdP-RPDFkl0.en

In the last section we pointed out the difference between a discrete and a continuous error function and discovered that in order for us to use gradient descent we need a continuous error function. In order to do this we also need to move from discrete predictions to continuous predictions. Let me show you what I mean by that.

### 15. Discrete vs. Continuous-Rm2KxFaPiJg.en

The prediction is basically the answer we get from the algorithm. A discreet answer will be of the form yes, no. Whereas a continued answer will be a number, normally between zero and one which we'll consider a probability. In the running example, here we have our students where blue is accepted and red is rejected. And the discrete algorithm will tell us if a student is accepted or rejected by typing a zero for rejected students and a one for accepted students. On the other hand, the farther our point is from the black line, the more drastic these probabilities are. Points that are well into the blue area get very high probabilities, such as this point with an 85% probability of being blue. And points that are well into the red region are given very low probabilities, such as this point on the bottom that is given a 20% probability of being blue. The points over the line are all given a 50% probability of being blue. As you can see the probability is a function of the distance from the line. The way we move from discrete predictions to continuous, is to simply change your activation function from the step function in the left, to the sigmoid function on the right. The sigmoid function is simply a function which for large positive numbers will give us values very close to one. For large negative numbers will give us values very close to zero. And for numbers that are close to zero, it'll give you values that are close to point five. The formula is sigmoid effects equals (x) = 1/(1 + exp(-x)) So, before our model consisted of a line with a positive region and a negative region. Now it consists of an entire probability space or for each point in the plane we are given the probability that the label of the point is one for the blue points, and zero for the red points. For example, for this point the probability of being blue is 50% and of being red is 50%. For this point, the probabilities are 40% for being blue, and 60% for being red. For this one over here it's 30% for blue, and 70% for red. And for this point all over here is 80% for being blue and 25 percent for being red. The way we obtain this probability space is very simple. We just combine the linear function WX + b with the sigmoid function. So in the left we have the lines that represent the points for which WX + b is zero, one, two, minus one, minus two, etc. And once we apply the sigmoid function to each of these values in the plane, we then obtain numbers from zero to one for each point. These numbers are just the probabilities of the point being blue. The probability of the point being blue is a prediction of the model Y hat to sigmoid of W x plus b. Here we can see the lines for which the prediction is point five, point six, point seven, point four, point three, et cetera. As you can see, as we get more into the blue area, (Wx + b) gets closer and closer to one. And as we move into the red area, (Wx + b) gets closer and closer to zero. When we're over the main line, W x plus b is zero, which means sigmoid of W s plus b is exactly zero point five. So here on the left we have our old perceptron with the activation function as a step function. And on the right we have our new perceptron, where the activation function is the sigmoid function. What our new perceptron does, it takes the inputs, multiplies them by the weights in the edges and adds the results, then applies the sigmoid function. So instead of returning one and zero like before it returns values between zero and one such as 0.99 or 0.67 etc. Before it used to say the student got accepted or not, and now it says the probability of the student got accepted is this much.

### 16. DL 18 Q Softmax V2-RC_A9Tu99y4.en

Let's switch to a different example for a moment. Let's say we have a model that will predict if you receive a gift or not. So, the model use predictions in the following way. It says, the probability that you get a gift is 0.8, which automatically implies that the probability that you don't receive a gift is 0.2. And what does the model do? What the model does is take some inputs. For example, is it your birthday or have it been good all year? And based on those inputs, it calculates a linear model which would be the score. Then, the probability that you get the gift or not is simply the sigmoid function applied to that score. Now, what if you had more options than just getting a gift or not a gift? Let's say we have a model that just tell us what animal we just saw, and the options are a duck, a beaver and a walrus. We want a model that tells an answer along the lines of, the probability of a duck is 0.67, the probability of a beaver is 0.24, and the probability of a walrus is 0.09. Notice that the probabilities need to add to one. Let's say we have a linear model based on some inputs. The inputs could be, does it have a beak or not? Number of teeth. Number of feathers. Hair, no hair. Does it live in the water? Does it fly? Etc. We calculate linear function based on those inputs, and let's say we get some scores. So, the duck gets a score of two, and the beaver gets a score of one, and the walrus gets a score of zero. And now the question is, how do we turn these scores into probabilities? The first thing we need to satisfy with probabilities is as we said, they need to add to one. So the two, the one, and the zero do not add to one. The second thing we need to satisfy is, since the duck had a higher score than the beaver and the beaver had a higher score than the walrus, then we want the probability of the duck to be higher than the probability of the beaver, and the probability of the beaver to be higher than the probability of the walrus. Here's a simple way of doing it. Let's take each score and divide it by the sum of all the scores. The two becomes two divided by two plus one plus zero, the one becomes one divided by two plus one plus zero, and the zero becomes zero divided by two plus one plus zero. This kind of works because the probabilities we obtain are two thirds for the duck, one third for the beaver, and zero for the walrus. That works but there's a little problem. Let's think about it. What could this problem be? The problem is the following. What happens if our scores are negative? This is completely plausible since the scores are linear function which could give negative values. What if we had, say, scores of 1, 0 and (-1)? Then, one of the probabilities would turn into one divided by one plus zero plus minus one which is zero, and we know very well that we cannot divide by zero. This unfortunately won't work, but the idea is good. How can we turn this idea into one that works all the time even for negative numbers? Well, it's almost like we need to turn these scores into positive scores. How do we do this? Is there a function that can help us? This is the quiz. Let's look at some options. There's sine, cosine, logarithm, and exponential. Quiz. Which one of these functions will turn every number into a positive number? Enter your answer below.

### 16. DL 18 S Softmax-n8S-v_LCTms.en

So, if you said exponential, you are correct. Because this is a function that returns a positive number for every input. E to the X is always a positive number. So, what we're going to do is exactly what we did before, except, applying it to the X to the scores. So, instead of 2,1, 0, we have E to the 2, E to the 1 and E to the 0. So, that 2 becomes E to the 2 divided by E to the two plus E to the 1 plus E to the 0. And, similarly for 1 and 0. So, the probabilities we obtain now are as 0.67, 0.24 and 0.09. This clearly add to 1. And, also notice that since the exponential function is increasing, then the duck has a higher probability than the beaver. And this one has a higher probability than the walrus. This function is called the Softmax function and it's defined formally like this. Let's say we have N classes and a linear model that gives us the following scores. Z1, Z2, up to ZN. Each score for each of the classes. What we do to turn them into probabilities is to say the probability that the object is in class I is going to be E to the power of the ZI divided by the sum of E to the power of Z1 plus all the way to E to the power ZN. That's how we turn scores into probabilities. So, here's a question for you. When we had two classes, we applied the sigmoid function to the scores. Now, that we have more classes we apply the softmax function to the scores. The question is, is the softmax function for N equals to the same as the sigmoid function? I'll let you think about it. The answer is actually, yes, but it's not super trivial why. And, it's a nice thing to remember.

### 16. Quiz - Softmax-NNoezNnAMTY.en

So far we have models that give us an answer of yes/no or the probability of a label being positive or negative. What if we have more classes? What if we want our model to tell us if something is red, blue, yellow or dog, cat, bird? In this video I'll show you what to do.

### 17. One-Hot Encoding-AePvjhyvsBo.en

So, as we've seen so far, all our algorithms are numerical. This means we need to input numbers, such as a score in a test or the grades, but the input data will not always look like numbers. Sometimes it looks like this. Let's say the module receives as an input the fact that you got a gift or didn't get a gift. How do we turn that into numbers? Well, that's easy. If you've got a gift, we'll just say that the input variable is 1. And, if you didn't get a gift, we'll just say that the input variable is 0. But, what if we have more classes as before or, let's say, our classes are Duck, Beaver and Walrus? What variable do we input in the algorithm? Maybe, we can input a 0 or 1 and a 2, but that would not work because it would assume dependencies between the classes that we can't have. So, this is what we do. What we do is, we come up with one variable for each of the classes. So, our table becomes like this. That's one variable for Duck, one for Beaver and one for Walrus. And, each one has its corresponding column. Now, if the input is a duck then the variable for duck is 1 and the variables for beaver and walrus are 0. Similarly for the beaver and the walrus. We may have more columns of data but at least there are no unnecessary dependencies. This process is called The One-Hot Encoding and it will be used a lot for processing data.

### 18. Maximum Likelihood 1-1yJx-QtlvNI.en

So we're still in our quest for an algorithm that will help us pick the best model that separates our data. Well, since we're dealing with probabilities then let's use them in our favor. Let's say I'm a student and I have two models. One that tells me that my probability of getting accepted is 80% and one that tells me the probability is 55%. Which model looks more accurate? Well, if I got accepted then I'd say the better model is probably the one that says 80%. What if I didn't get accepted? Then the more accurate model is more likely the one that says 55 percent. But I'm just one person. What if it was me and a friend? Well, the best model would more likely be the one that gives the higher probabilities to the events that happened to us, whether it's acceptance or rejection. This sounds pretty intuitive. The method is called maximum likelihood. What we do is we pick the model that gives the existing labels the highest probability. Thus, by maximizing the probability, we can pick the best possible model.

### 18. Maximum Likelihood 2-6nUUeQ9AeUA.en

So let me be more specific. Let's look at the following four points: two blue and two red and two models that classify them, the one on the left and the one on the right. Quick. Which model looks better? You are correct. The model on the right is much better since it classifies the four points correctly whereas the model in the left gets two points correctly and two points incorrectly. But let's see why the model in the right is better from the probability perspective. And by that, we'll show you that the arrangement in the right is much more likely to happen than the one in the left. So let's recall that our prediction is  = (Wx+b) and that that is precisely the probability of a point being labeled positive which means blue. So for the points in the figure, let's say the model tells you that the probability of being blue are 0.9, 0.6, 0.3, and 0.2. Notice that the points in the blue region are much more likely to be blue and the points in the red region are much less likely to be blue. Now obviously, the probability of being red is one minus the probability of being blue. So in this case, the probability of some of the points being red are 0.1, 0.4, 0.7 and 0.8. Now what we want to do is we want to calculate the probability of the four points are of the colors that they actually are. This means the probability that the two red points are red and that the two blue points are blue. Now if we assume that the colors of the points are independent events then the probability for the whole arrangement is the product of the probabilities of the four points. This is equal to 0.1  0.6  0.7  0.2 = 0.0084. This is very small. It's less than 1%. What we mean by this is that if the model is given by these probability spaces, then the probability that the points are of these colors is 0.0084. Now let's do this for both models. As we saw the model on the left tells us that the probabilities of these points being of those colors is 0.0084. If we do the same thing for the model on the right. Let's say we get that the probabilities of the two points in the right being blue are 0.7 and 0.9 and of the two points in the left being red are 0.8 and 0.6. When we multiply these we get 0.3024 which is around 30%. This is much higher than 0.0084. Thus, we confirm that the model on the right is better because it makes the arrangement of the points much more likely to have those colors. So now, what we do is the following? We start from the bad modeling, calculate the probability that the points are those colors, multiply them and we obtain the total probability is 0.0084. Now if we just had a way to maximize this probability we can increase it all the way to 0.3024. Thus, our new goal becomes precisely that, to maximize this probability. This method, as we stated before, is called maximum likelihood.

### 19. Quiz - Cross 1--xxrisIvD0E.en

Well we're getting somewhere now. We've concluded that the probability is important. And that the better model will give us a better probability. Now the question is, how we maximize the probability. Also, if remember correctly we're talking about an error function and how minimizing this error function will take us to the best possible solution. Could these two things be connected? Could we obtain an error function from the probability? Could it be that maximizing the probability is equivalent to minimizing the error function? Maybe.

### 19. Quiz Cross Entropy-njq6bYrPqSU.en

So a quick recap. We have two models, the bad one on the left and the good one on the right. And the way to tell they're bad or good is to calculate the probability of each point being the color it is according to the model. Multiply these probabilities in order to obtain the probability of the whole arrangement and then check that the model on the right gives us a much higher probability than the model on the left. Now all we need to do is to maximize this probability. But probability is a product of numbers and products are hard. Maybe this product of four numbers doesn't look so scary. But what if we have thousands of datapoints? That would correspond to a product of thousands of numbers, all of them between zero and one. This product would be very tiny, something like 0.0000 something and we definitely want to stay away from those numbers. Also, if I have a product of thousands of numbers and I change one of them, the product will change drastically. In summary, we really want to stay away from products. And what's better than products? Well, let's ask our friend here. Products are bad, but sums are good. Let's do sums. So let's try to turn these products into sums. We need to find a function that will help us turn products into sums. What would this function be? It sounds like it's time for a quiz. Quiz. Which function will help us out here? Sine, cosine, logarithm or the exponential function? Enter your answer below.

### 20. Cross Entropy 1-iREoPUrpXvE.en

Correct. The answer is logarithm, because logarithm has this very nice identity that says that the logarithm of the product A times B is the sum of the logarithms of A and B. So this is what we do. We take our products and we take the logarithms, so now we get a sum of the logarithms of the factors. So the ln(0.6*0.2*0.1*0.7) is equal to ln(0.6) + ln(0.2) + ln(0.1) + ln(0.7) etc. Now from now until the end of class, we'll be taking the natural logarithm which is base e instead of 10. Nothing different happens with base 10. Everything works the same as everything gets scaled by the same factor. So it's just more for convention. We can calculate those values and get minus 0.51, minus 1.61, minus 0.23 etc. Notice that they are all negative numbers and that actually makes sense. This is because the logarithm of a number between 0 and 1 is always a negative number since the logarithm of one is zero. So it actually makes sense to think of the negative of the logarithm of the probabilities and we'll get positive numbers. So that's what we'll do. We'll take the negative of the logarithm of the probabilities. That sums up negatives of logarithms of the probabilities, we'll call the cross entropy which is a very important concept in the class. If we calculate the cross entropies, we see that the bad model on left has a cross entropy 4.8 which is high. Whereas the good model on the right has a cross entropy of 1.2 which is low. This actually happens all the time. A good model will give us a low cross entropy and a bad model will give us a high cross entropy. The reason for this is simply that a good model gives us a high probability and the negative of the logarithm of a large number is a small number and vice versa. This method is actually much more powerful than we think. If we calculate the probabilities and pair the points with the corresponding logarithms, we actually get an error for each point. So again, here we have probabilities for both models and the products of them. Now, we take the negative of the logarithms which gives us sum of logarithms and if we pair each logarithm with the point where it came from, we actually get a value for each point. And if we calculate the values, we get this. Check it out. If we look carefully at the values we can see that the points that are mis-classified has like values like 2.3 for this point or 1.6 one for this point, whereas the points that are correctly classified have small values. And the reason for this is again is that a correctly classified point will have a probability that as close to 1, which when we take the negative of the logarithm, we'll get a small value. Thus we can think of the negatives of these logarithms as errors at each point. Points that are correctly classified will have small errors and points that are mis-classified will have large errors. And now we've concluded that our cross entropy will tell us if a model is good or bad. So now our goal has changed from maximizing a probability to minimizing a cross entropy in order to get from the model in left to the model in the right. And that error function that we're looking for, that was precisely the cross entropy.

### 21. CrossEntropy V1-1BnhC6e0TFw.en

Let's look a bit closer into Cross-Entropy by switching to a different example. Let's say we have three doors. And no this is not the Monty Hall problem. We have the green door, the red door, and the blue door, and behind each door we could have a gift or not have a gift. And the probabilities of there being a gift behind each door is 0.8 for the first one, 0.7 for the second one, 0.1 for the third one. So for example behind the green door there is an 80 percent probability of there being a gift, and a 20 percent probability of there not being a gift. So we can put the information in this table where the probabilities of there being a gift are given in the top row, and the probabilities of there not being a gift are given in the bottom row. So let's say we want to make a bet on the outcomes. So we want to try to figure out what is the most likely scenario here. And for that we'll assume they're independent events. In this case, the most likely scenario is just obtained by picking the largest probability in each column. So for the first door is more likely to have a gift than not have a gift. So we'll say there's a gift behind the first door. For the second door, it's also more likely that there's a gift. So we'll say there's a gift behind the second door. And for the third door it's much more likely that there's no gift, so we'll say there's no gift behind the third door. And as the events are independent, the probability for this whole arrangement is the product of the three probabilities which is 0.8, times 0.7, times 0.9, which ends up being 0.504, which is roughly 50 percent. So let's look at all the possible scenarios in the table. Here's a table with all the possible scenarios for each door and there are eight scenarios since each door gives us two possibilities each, and there are three doors. So we do as before to obtain the probability of each arrangement by multiplying the three independent probabilities to get these numbers. You can check that these numbers add to one. And from last video we learned that the negative of the logarithm of the probabilities across entropy. So let's go ahead and calculate the cross-entropy. And notice that the events with high probability have low cross-entropy and the events with low probability have high cross-entropy. For example, the second row which has probability of 0.504 gives a small cross-entropy of 0.69, and the second to last row which is very very unlikely has a probability of 0.006 gives a cross entropy a 5.12. So let's actually calculate a formula for the cross-entropy. Here we have our three doors, and our sample scenario said that there is a gift behind the first and second doors, and no gift behind the third door. Recall that the probabilities of these events happening are 0.8 for a gift behind the first door, 0.7 for a gift behind the second door, and 0.9 for no gift behind the third door. So when we calculate the cross-entropy, we get the negative of the logarithm of the product, which is a sum of the negatives of the logarithms of the factors, which is negative logarithm of 0.8 minus logarithm of 0.7 minus logarithm 0.9. And in order to drive the formula we'll have some variables. So let's call P1 the probability that there's a gift behind the first door, P2 the probability there's a gift behind the second door, and P3 the probability there's a gift behind the third door. So this 0.8 here is P1, this 0.7 here is P2, and this 0.9 here is one minus P3. So it's a probability of there not being a gift is one minus the probability of there being a gift. Let's have another variable called Yi, which will be one of there's a present behind the ith door, and zero there's no present. So Yi is technically a number of presents behind the ith door. In this case Y1 equals one, Y2 equals one, and Y3 equals zero. So we can put all this together and derive a formula for the cross-entropy and it's this sum. Now let's look at the formula inside the summation. Noted that if there is a present behind the ith door, then Yi equals one. So the first term is logarithm of the Pi. And the second term is zero. Likewise, if there is no present behind the ith door, then Yi is zero. So this first term is zero. And this term is precisely logarithm of one minus Pi. Therefore, this formula really encompasses the sums of the negative of logarithms which is precisely the cross-entropy. So the cross-entropy really tells us when two vectors are similar or different. For example, if you calculate the cross entropy of the pair one one zero, and 0.8, 0.7, 0.1, we get 0.69. And that is low because one one zero is a similar vector to 0.8, 0.7, 0.1. Which means that the arrangement of gifts given by the first set of numbers is likely to happen based on the probabilities given by the second set of numbers. But on the other hand if we calculate the cross-entropy of the pairs zero zero one, and 0.8, 0.7, 0.1, that is 5.12 which is very high. This is because the arrangement of gifts being given by the first set of numbers is very unlikely to happen from the probabilities given by the second set of numbers.

### 21. Formula For Cross 1-qvr_ego_d6w.en

So this cross entropy, it looks like kind of a big deal. Cross entropy really says the following. If I have a bunch of events and a bunch of probabilities, how likely is it that those events happen based on the probabilities? If it's very likely, then we have a small cross entropy. If it's unlikely, then we have a large cross entropy. Let's elaborate.

### 22. DL 27 Multi-Class Cross Entropy 2 Fix-keDswcqkees.en

Now that was when we had two classes namely receiving a gift or not receiving a gift. What happens if we have more classes? Let's take a look. So we have a similar problem. We still have three doors. And this problem is still not the Monty Hall problem. Behind each door there can be an animal, and the animal can be of three types. It can be a duck, it can be a beaver, or it can be a walrus. So let's look at this table of probabilities. According to the first column on the table, behind the first door, the probability of finding a duck is 0.7, the probability of finding a beaver is 0.2, and the probability of finding a walrus is 0.1. Notice that the numbers in each column need to add to one because there is some animal behind door one. The numbers in the rows do not need to add to one as you can see. It could easly be that we have a duck behind every door and that's okay. So let's look at a sample scenario. Let's say we have our three doors, and behind the first door, there's a duck, behind the second door there's a walrus, and behind the third door there's also a walrus. Recall that the probabilities are again by the table. So a duck behind the first door is 0.7 likely, a walrus behind the second door is 0.3 likely, and a walrus behind the third door is 0.4 likely. So the probability of obtaining this three animals is the product of the probabilities of the three events since they are independent events, which in this case it's 0.084. And as we learn, that cross entropy here is given by the sums of the negatives of the logarithms of the probabilities. So the first one is negative logarithm of 0.7. The second one is negative logarithm of 0.3. And the third one is negative logarithm of 0.4. The Cross entropy's and the sum of these three which is actually 2.48. But we want a formula, so let's put some variables here. So P11 is the probability of finding a duck behind door one. P12 is the probability of finding a duck behind door two etc. And let's have the indicator variables Y1j D1 if there's a duck behind door J. Y2j B1 if there's a beaver behind door J, and Y3j B1 if there's a walrus behind door J. And these variables are zero otherwise. And so, the formula for the cross entropy is simply the negative of the summation from i_ equals_ one to n, up to summation from y_ equals_ j to m of Yij_ times_ the logarithm of Pij. In this case, m is a number of classes. This formula works because Yij being zero one, makes sure that we're only adding the logarithms of the probabilities of the events that actually have occurred. And voila, this is the formula for the cross entropy in more classes. Now I'm going to leave this equestion. Given that we have a formula for cross entropy for two classes and one for m classes. These formulas look different but are they the same for m_ equals_ two? Obviously the answer is yes, but it's a cool exercise to actually write them down and convince yourself that they are actually the same.

### 23. DL 29 Logistic Regression-Minimizing The Error Function-KayqiYijlzc.en

Okay. So now our goal is to minimize the error function and we'll do it as follows. We started some random weights, which will give us the predictions (Wx+b). As we saw, that also gives us a error function given by this formula. Remember that the summands are also error functions for each point. So each point will give us a larger function if it's mis-classified and a smaller one if it's correctly classified. And the way we're going to minimize this function, is to use gradient decent. So here's Mt. Errorest and this is us, and we're going to try to jiggle the line around to see how we can decrease the error function. Now, the error function is the height which is E(W,b), where W and b are the weights. Now what we'll do, is we'll use gradient decent in order to get to the bottom of the mountain at a much smaller height, which gives us a smaller error function E of W', b'. This will give rise to new weights, W' and b' which will give us a much better prediction. Namely,  (W'x+b').

### 23. Error Function-V5kkHldUlVU.en

So this is a good time for a quick recap of the last couple of lessons. Here we have two models. The bad model on the left and the good model on the right. And for each one of those we calculate the cross entropy which is the sum of the negatives of the logarithms off the probabilities of the points being their colors. And we conclude that the one on the right is better because a cross entropy is much smaller. So let's actually calculate the formula for the error function. Let's split into two cases. The first case being when y=1. So when the point is blue to begin with, the model tells us that the probability of being blue is the prediction y_hat. So for these two points the probabilities are 0.6 and 0.2. As we can see the point in the blue area has more probability of being blue than the point in the red area. And our error is simply the negative logarithm of this probability. So it's precisely minus logarithm of y_hat. In the figure it's minus logarithm of 0.6. and minus logarithm of 0.2. Now if y=0, so when the point is red, then we need to calculate the probability of the point being red. The probability of the point being red is one minus the probability of the point being blue which is precisely 1 minus the prediction y_hat. So the error is precisely the negative logarithm of this probability which is negative logarithm of 1 - y_hat. In this case we get negative logarithm 0.1 and negative logarithm 0.7. So we conclude that the error is a negative logarithm of y_hat if the point is blue. And negative logarithm of one - y_hat the point is red. We can summarize these two formulas into this one. Error = - (1-y)(ln( 1- y_hat)) - y ln(y_hat). Why does this formula work? Well because if the point is blue, then y=1 which means 1-y=0 which makes the first term 0 and the second term is simply logarithm of y_hat. Similarly, if the point is red then y=0. So the second term of the formula is 0 and the first one is logarithm of 1- y_hat. Now the formula for the error function is simply the sum over all the error functions of points which is precisely the summation here. That's going to be this 4.8 we have over here. Now by convention we'll actually consider the average, not the sum which is where we are dividing by n over here. This will turn the 4.8 into a 1.2. From now on we'll use this formula as our error function. And now since y_hat is given by the sigmoid of the linear function wx + b, then the total formula for the error is actually in terms of w and b which are the weights of the model. And it's simply the summation we see here. In this case y_i is just the label of the point x_superscript_i. So now that we've calculated it our goal is to minimize it. And that's what we'll do next. And just a small aside, what we did is for binary classification problems. If we have a multiclass classification problem then the error is now given by the multiclass entropy. This formula is given here where for every data point we take the product of the label times the logarithm of the prediction and then we average all these values. And again it's a nice exercise to convince yourself that the two are the same when there are just two classes.

### 24. Gradient Descent-rhVIF-nigrY.en

So let's study gradient descent in more mathematical detail. Our function is a function of the weights and it can be graph like this. It's got a mathematical structure so it's not Mt. Everest anymore, it's more of a mount Math-Er-Horn. So we're standing somewhere in Mount Math-Er-Horn and we need to go down. So now the inputs of the functions are W1 and W2 and the error function is given by E. Then the gradient of E is given by the vector sum of the partial derivatives of E with respect to W1 and W2. This gradient actually tells us the direction we want to move if we want to increase the error function the most. Thus, if we take the negative of the gradient, this will tell us how to decrease the error function the most. And this is precisely what we'll do. At the point we're standing, we'll take the negative of the gradient of the error function at that point. Then we take a step in that direction. Once we take a step, we'll be in a lower position. So we do it again, and again, and again, until we are able to get to the bottom of the mountain. So this is how we calculate the gradient. We start with our initial prediction Y had equals sigmoid of W Expo's B. And let's say this prediction is bad because the error is large since we're high up in the mountain. The prediction looks like this, Y had equal sigmoid of W 1 x 1 plus all the way to WnXn plus b. Now the error function is given by the formula we saw before. But what matters here is the gradient of the error function. The gradient of the error function is precisely the vector formed by the partial derivative of the error function with respect to the weights and the bias. Now, we take a step in the direction of the negative of the gradient. As before, we don't want to make any dramatic changes, so we'll introduce a smaller learning rate alpha. For example, 0.1. And we'll multiply the gradient by that number. Now taking the step is exactly the same thing as updating the weights and the bias as follows. The weight Wi will now become Wi prime. Given by Wi minus alpha times the partial derivative of the error, with respect to Wi. And the bias will now become b prime given by b minus alpha times partial derivative of the error with respect to b. Now this will take us to a prediction with a lower error function. So, we can conclude that the prediction we have now with weights W prime b prime, is better than the one we had before with weights W and b. This is precisely the gradient descent step.

### 25. Gradient Descent Algorithm-snxmBgi_GeU.en

And now we finally have the tools to write the pseudocode for the grading descent algorithm, and it goes like this. Step one, start with random weights w_one up to w_n and b which will give us a line, and not just a line, but the whole probability function given by sigmoid of w x plus b. Now for every point we'll calculate the error, and as we can see the error is high for misclassified points and small for correctly classified points. Now for every point with coordinates x_one up to x_n, we update w_i by adding the learning rate alpha times the partial derivative of the error function with respect to w_i. We also update b by adding alpha times the partial derivative of the error function with respect to be. This gives us new weights, w_i_prime and then new bias b_prime. Now we've already calculated these partial derivatives and we know that they are y_hat minus y times x_i for the derivative with respect to w_i and y_hat minus y for the derivative with respect to b. So that's how we'll update the weights. Now repeat this process until the error is small, or we can repeat it a fixed number of times. The number of times is called the epochs and we'll learn them later. Now this looks familiar, have we seen something like that before? Well, we look at the points and what each point is doing is it's adding a multiple of itself into the weights of the line in order to get the line to move closer towards it if it's misclassified. That's pretty much what the Perceptron algorithm is doing. So in the next video, we'll look at the similarities because it's a bit suspicious how similar they are.

### 28. Gradient Descent Vs Perceptron Algorithm-uL5LuRPivTA.en

So let's compare the Perceptron algorithm and the Gradient Descent algorithm. In the Gradient Descent algorithm, we take the weights and change them from Wi to Wi_ plus_ alpha_ times_ Y hat_ minus_ Y_ times_ Xi. In the Perceptron algorithm, not every point changes weights, only the misclassified ones. Here, if X is misclassified, we'll change the weights by adding Xi to Wi if the point label is positive, and subtracting if negative. Now the question is, are these two things the same? Well, let's remember that in that Perceptron algorithm, the labels are one and zero. And the predictions Y-hat are also one and zero. So, if the point is correct, classified, then Y_ minus_ Y-hat is zero because Y is equal to Y-hat. Now, if the point is labeled blue, then Y_ equals_ one. And if it's misclassified, then the prediction must be Y-hat_ equals_ zero. So Y-hat_ minus_ Y is minus one. Similarly, with the points labeled red, then Y_ equals_ zero and Y-hat_ equals_ one. So, Y-hat_ minus_ Y_ equals_ one. This may not be super clear right away. But if you stare at the screen for long enough, you'll realize that the right and the left are exactly the same thing. The only difference is that in the left, Y-hat can take any number between zero and one, whereas in the right, Y-hat can take only the values zero or one. It's pretty fascinating, isn't it? But let's study Gradient Descent even more carefully. Both in the Perceptron algorithm and the Gradient Descent algorithm, a point that is misclassified tells a line to come closer because eventually, it wants the line to surpass it so it can be in the correct side. Now, what happens if the point is correctly classified? Well, the Perceptron algorithm says do absolutely nothing. In the Gradient Descent algorithm, you are changing the weights. But what is it doing? Well, if we look carefully, what the point is telling the line, is to go farther away. And this makes sense, right? Because if you're correctly classified, say, if you're a blue point in the blue region, you'd like to be even more into the blue region, so your prediction is even closer to one, and your error is even smaller. Similarly, for a red point in the red region. So it makes sense that the point tells the line to go farther away. And that's precisely what the Gradient Descent algorithm does. The misclassified points asks the line to come closer and the correctly classified points asks the line to go farther away. The line listens to all the points and takes steps in such a way that it eventually arrives to a pretty good solution.

### 29. Continuous Perceptrons-07-JJ-aGEfM.en

So, this is just a small recap video that will get us ready for what's coming. Recall that if we have our data in the form of these points over here and the linear model like this one, for example, with equation 2x1 + 7x2 - 4 = 0, this will give rise to a probability function that looks like this. Where the points on the blue or positive region have more chance of being blue and the points in the red or negative region have more chance of being red. And this will give rise to this perception where we label the edges by the weights and the node by the bias. So, what the perception does, it takes to point (x1, x2), plots it in the graph and then it returns a probability that the point is blue. In this case, it returns a 0.9 and this mimics the neurons in the brain because they receive nervous impulses, do something inside and return a nervous impulse.

### 30. Non-Linear Data-F7ZiE8PQiSc.en

Now we've been dealing a lot with data sets that can be separated by a line, like this one over here. But as you can imagine the real world is much more complex than that. This is where neural networks can show their full potential. In the next few videos we'll see how to deal with more complicated data sets that require highly non-linear boundaries such as this one over here.

### 31. Non-Linear Models-HWuBKCZsCo8.en

So, let's go back to this example of where we saw some data that is not linearly separable. So a line can not divide these red and blue points and we looked at some solutions, and if you remember, the one we considered more seriously was this curve over here. So what I'll teach you now is to find this curve and it's very similar than before. We'll still use grading dissent. In a nutshell, what we're going to do is for these data which is not separable with a line, we're going to create a probability function where the points in the blue region are more likely to be blue and the points in the red region are more likely to be red. And this curve here that separates them is a set of points which are equally likely to be blue or red. Everything will be the same as before except this equation won't be linear and that's where neural networks come into play.

### 32. 29 Neural Network Architecture 2-FWN3Sw5fFoM.en

So in the previous session we learn that we can add to linear models to obtain a third model. As a matter of fact, we did even more. We can take a linear combination of two models. So, the first model times a constant plus the second model times a constant plus a bias and that gives us a non-linear model. That looks a lot like perceptrons where we can take a value times a constant plus another value times a constant plus a bias and get a new value. And that's no coincidence. That's actually the building block of Neural Networks. So, let's look at an example. Let's say, we have this linear model where the linear equation is 5x1 minus 2x2 plus 8. That's represented by this perceptron. And we have another linear model with equations 7x1 minus 3x2 minus 1 which is represented by this perceptron over here. Let's draw them nicely in here and let's use another perceptron to combine these two models using the Linear Equation, seven times the first model plus five times the second model minus six. And now the magic happens when we join these together and we get a Neural Network. We clean it up a bit and we obtain this. All the weights are there. The weights on the left, tell us what equations the linear models have. And the weights on the right, tell us what the linear combination is of the two models to obtain the curve non-linear model in the right. So, whenever you see a Neural Network like the one on the left, think of what could be the nonlinear boundary defined by the Neural Network. Now, note that this was drawn using the notation that puts a bias inside the node. This can also be drawn using the notation that keeps the bias as a separate node. Here, what we do is, in every layer we have a bias unit coming from a node with a one on it. So for example, the minus eight on the top node becomes an edge labelled minus eight coming from the bias node. We can see that this Neural Network uses a Sigmoid Activation Function and the Perceptrons.

### 32. Combinando modelos-Boy3zHVrWB4.en

Now I'm going to show you how to create these nonlinear models. What we're going to do is a very simple trick. We're going to combine two linear models into a nonlinear model as follows. Visually it looks like this. The two models over imposed creating the model on the right. It's almost like we're doing arithmetic on models. It's like saying "This line plus this line equals that curve." Let me show you how to do this mathematically. So a linear model as we know is a whole probability space. This means that for every point it gives us the probability of the point being blue. So, for example, this point over here is in the blue region so its probability of being blue is 0.7. The same point given by the second probability space is also in the blue region so it's probability of being blue is 0.8. Now the question is, how do we combine these two? Well, the simplest way to combine two numbers is to add them, right? So 0.8 plus 0.7 is 1.5. But now, this doesn't look like a probability anymore since it's bigger than one. And probabilities need to be between 0 and 1. So what can we do? How do we turn this number that is larger than 1 into something between 0 and 1? Well, we've been in this situation before and we have a pretty good tool that turns every number into something between 0 and 1. That's just a sigmoid function. So that's what we're going to do. We applied the sigmoid function to 1.5 to get the value 0.82 and that's the probability of this point being blue in the resulting probability space. So now we've managed to create a probability function for every single point in the plane and that's how we combined two models. We calculate the probability for one of them, the probability for the other, then add them and then we apply the sigmoid function. Now, what if we wanted to weight this sum? What, if say, we wanted the model in the top to have more of a saying the resulting probability than the second? So something like this where the resulting model looks a lot more like the one in the top then like the one in the bottom. Well, we can add weights. For example, we can say "I want seven times the first model plus the second one." Actually, I can add the weights since I want. For example, I can say "Seven times the first one plus five times the second one." And when I do get the combine the model is I take the first probability, multiply it by seven, then take the second one and multiply it by five and I can even add a bias if I want. Say, the bias is minus 6, then we add it to the whole equation. So we'll have seven times this plus five times this minus six, which gives us 2.9. We then apply the sigmoid function and that gives us 0.95. So it's almost like we had before, isn't it? Before we had a line that is a linear combination of the input values times the weight plus a bias. Now we have that this model is a linear combination of the two previous model times the weights plus some bias. So it's almost the same thing. It's almost like this curved model in the right. It's a linear combination of the two linear models before or we can even think of it as the line between the two models. This is no coincidence. This is at the heart of how neural networks get built. Of course, we can imagine that we can keep doing this always obtaining more new complex models out of linear combinations of the existing ones. And this is what we're going to do to build our neural networks.

### 32. Layers-pg99FkXYK0M.en

Neural networks have a certain special architecture with layers. The first layer is called the input layer, which contains the inputs, in this case, x1 and x2. The next layer is called the hidden layer, which is a set of linear models created with this first input layer. And then the final layer is called the output layer, where the linear models get combined to obtain a nonlinear model. You can have different architectures. For example, here's one with a larger hidden layer. Now we're combining three linear models to obtain the triangular boundary in the output layer. Now what happens if the input layer has more nodes? For example, this neural network has three nodes in its input layer. Well, that just means we're not living in two-dimensional space anymore. We're living in three-dimensional space, and now our hidden layer, the one with the linear models, just gives us a bunch of planes in three space, and the output layer bounds a nonlinear region in three space. In general, if we have n nodes in our input layer, then we're thinking of data living in n-dimensional space. Now what if our output layer has more nodes? Then we just have more outputs. In that case, we just have a multiclass classification model. So if our model is telling us if an image is a cat or dog or a bird, then we simply have each node in the output layer output a score for each one of the classes: one for the cat, one for the dog, and one for the bird. And finally, and here's where things get pretty cool, what if we have more layers? Then we have what's called a deep neural network. Now what happens here is our linear models combine to create nonlinear models and then these combine to create even more nonlinear models. In general, we can do this many times and obtain highly complex models with lots of hidden layers. This is where the magic of neural networks happens. Many of the models in real life, for self-driving cars or for game-playing agents, have many, many hidden layers. That neural network will just split the n-dimensional space with a highly nonlinear boundary, such as maybe the one on the right.

### 32. Multiclass Classification-uNTtvxwfox0.en

We briefly mentioned multi-class classification in the last video but let me be more specific. It seems that neural networks work really well when the problem consist on classifying two classes. For example, if the model predicts a probability of receiving a gift or not then the answer just comes as the output of the neural network. But what happens if we have more classes? Say, we want the model to tell us if an image is a duck, a beaver, or a walrus. Well, one thing we can do is create a neural network to predict if the image is a duck, then another neural network to predict if the image is a beaver, and a third neural network to predict if the image is a walrus. Then we can just use SoftMax or pick the answer that gives us the highest probability. But this seems like overkill, right? The first layers of the neural network should be enough to tell us things about the image and maybe just the last layer should tell us which animal it is. As a matter of fact, as you'll see in the CNN section, this is exactly the case. So what we need here is to add more nodes in the output layer and each one of the nodes will give us the probability that the image is each of the animals. Now, we take the scores and apply the SoftMax function that was previously defined to obtain well-defined probabilities. This is how we get neural networks to do multi-class classification.

### 33. DL 41 Feedforward FIX V2-hVCuvMGOfyY.en

So now that we have defined what neural networks are, we need to learn how to train them. Training them really means what parameters should they have on the edges in order to model our data well. So in order to learn how to train them, we need to look carefully at how they process the input to obtain an output. So let's look at our simplest neural network, a perceptron. This perceptron receives a data point of the form x1, x2 where the label is Y=1. This means that the point is blue. Now the perceptron is defined by a linear equation say w1, x1 plus w2, x2 plus B, where w1 and w2 are the weights in the edges and B is the bias in the note. Here, w1 is bigger than w2, so we'll denote that by drawing the edge labelled w1 much thicker than the edge labelled w2. Now, what the perceptron does is it plots the point x1, x2 and it outputs the probability that the point is blue. Here is the point is in the red area and then the output is a small number, since the point is not very likely to be blue. This process is known as feedforward. We can see that this is a bad model because the point is actually blue. Given that the third coordinate, the Y is one. Now if we have a more complicated neural network, then the process is the same. Here, we have thick edges corresponding to large weights and thin edges corresponding to small weights and the neural network plots the point in the top graph and also in the bottom graph and the outputs coming out will be a small number from the top model. The point lies in the red area which means it has a small probability of being blue and a large number from the second model, since the point lies in the blue area which means it has a large probability of being blue. Now, as the two models get combined into this nonlinear model and the output layer just plots the point and it tells the probability that the point is blue. As you can see, this is a bad model because it puts the point in the red area and the point is blue. Again, this process called feedforward and we'll look at it more carefully. Here, we have our neural network and the other notations so the bias is in the outside. Now we have a matrix of weights. The matrix w superscript one denoting the first layer and the entries are the weights w1, 1 up to w3, 2. Notice that the biases have now been written as w3, 1 and w3, 2 this is just for convenience. Now in the next layer, we also have a matrix this one is w superscript two for the second layer. This layer contains the weights that tell us how to combine the linear models in the first layer to obtain the nonlinear model in the second layer. Now what happens is some math. We have the input in the form x1, x2, 1 where the one comes from the bias unit. Now we multiply it by the matrix w1 to get these outputs. Then, we apply the sigmoid function to turn the outputs into values between zero and one. Then the vector format these values gets a one attatched for the bias unit and multiplied by the second matrix. This returns an output that now gets thrown into a sigmoid function to obtain the final output which is y-hat. Y-hat is the prediction or the probability that the point is labeled blue. So this is what neural networks do. They take the input vector and then apply a sequence of linear models and sigmoid functions. These maps when combined become a highly non-linear map. And the final formula is simply y-hat equals sigmoid of w2 combined with sigmoid of w1 applied to x. Just for redundance, we do this again on a multi-layer perceptron or neural network. To calculate our prediction y-hat, we start with the unit vector x, then we apply the first matrix and a sigmoid function to get the values in the second layer. Then, we apply the second matrix and another sigmoid function to get the values on the third layer and so on and so forth until we get our final prediction, y-hat. And this is the feedforward process that the neural networks use to obtain the prediction from the input vector.

### 33. DL 42 Neural Network Error Function (1)-SC1wEW7TtKs.en

So, our goal is to train our neural network. In order to do this, we have to define the error function. So, let's look again at what the error function was for perceptrons. So, here's our perceptron. In the left, we have our input vector with entries x_1 up to x_n, and one for the bias unit. And the edges with weights W_1 up to W_n, and b for the bias unit. Finally, we can see that this perceptor uses a sigmoid function. And the prediction is defined as y-hat equals sigmoid of Wx plus b. And as we saw, this function gives us a measure of the error of how badly each point is being classified. Roughly, this is a very small number if the point is correctly classified, and a measure of how far the point is from the line and the point is incorrectly classified. So, what are we going to do to define the error function in a multilayer perceptron? Well, as we saw, our prediction is simply a combination of matrix multiplications and sigmoid functions. But the error function can be the exact same thing, right? It can be the exact same formula, except now, y-hat is just a bit more complicated. And still, this function will tell us how badly a point gets misclassified. Except now, it's looking at a more complicated boundary.

### 34. Backpropagation V2-1SmY3TZTyUk.en

So now we're finally ready to get our hands into training a neural network. So let's quickly recall feedforward. We have our perceptron with a point coming in labeled positive. And our equation w1x1 + w2x2 + b, where w1 and w2 are the weights and b is the bias. Now, what the perceptron does is, it plots a point and returns a probability that the point is blue. Which in this case is small since the point is in the red area. Thus, this is a bad perceptron since it predicts that the point is red when the point is really blue. And now let's recall what we did in the gradient descent algorithm. We did this thing called Backpropagation. We went in the opposite direction. We asked the point, "What do you want the model to do for you?" And the point says, "Well, I am misclassified so I want this boundary to come closer to me." And we saw that the line got closer to it by updating the weights. Namely, in this case, let's say that it tells the weight w1 to go lower and the weight w2 to go higher. And this is just an illustration, it's not meant to be exact. So we obtain new weights, w1' and w2' which define a new line which is now closer to the point. So what we're doing is like descending from Mt. Errorest, right? The height is going to be the error function E(W) and we calculate the gradient of the error function which is exactly like asking the point what does is it want the model to do. And as we take the step down the direction of the negative of the gradient, we decrease the error to come down the mountain. This gives us a new error, E(W') and a new model W' with a smaller error, which means we get a new line closer to the point. We continue doing this process in order to minimize the error. So that was for a single perceptron. Now, what do we do for multi-layer perceptrons? Well, we still do the same process of reducing the error by descending from the mountain, except now, since the error function is more complicated then it's not Mt. Errorest, now it's Mt. Kilimanjerror. But same thing, we calculate the error function and its gradient. We then walk in the direction of the negative of the gradient in order to find a new model W' with a smaller error E(W') which will give us a better prediction. And we continue doing this process in order to minimize the error. So let's look again at what feedforward does in a multi-layer perceptron. The point comes in with coordinates (x1, x2) and label y = 1. It gets plotted in the linear models corresponding to the hidden layer. And then, as this layer gets combined the point gets plotted in the resulting non-linear model in the output layer. And the probability that the point is blue is obtained by the position of this point in the final model. Now, pay close attention because this is the key for training neural networks, it's Backpropagation. We'll do as before, we'll check the error. So this model is not good because it predicts that the point will be red when in reality the point is blue. So we'll ask the point, "What do you want this model to do in order for you to be better classified?" And the point says, "I kind of want this blue region to come closer to me." Now, what does it mean for the region to come closer to it? Well, let's look at the two linear models in the hidden layer. Which one of these two models is doing better? Well, it seems like the top one is badly misclassifying the point whereas the bottom one is classifying it correctly. So we kind of want to listen to the bottom one more and to the top one less. So what we want to do is to reduce the weight coming from the top model and increase the weight coming from the bottom model. So now our final model will look a lot more like the bottom model than like the top model. But we can do even more. We can actually go to the linear models and ask the point, "What can these models do to classify you better?" And the point will say, "Well, the top model is misclassifying me, so I kind of want this line to move closer to me. And the second model is correctly classifying me, so I want this line to move farther away from me." And so this change in the model will actually update the weights. Let's say, it'll increase these two and decrease these two. So now after we update all the weights we have better predictions at all the models in the hidden layer and also a better prediction at the model in the output layer. Notice that in this video we intentionally left the bias unit away for clarity. In reality, when you update the weights we're also updating the bias unit. If you're the kind of person who likes formality, don't worry, we'll calculate these gradients in detail soon.

### 34. Calculating The Gradient 1 -tVuZDbUrzzI.en

Okay. So, now we'll do the same thing as we did before, painting our weights in the neural network to better classify our points. But we're going to do it formally, so fasten your seat belts because math is coming. On your left, you have a single perceptron with the input vector, the weights and the bias and the sigmoid function inside the node. And on the right, we have a formula for the prediction, which is the sigmoid function of the linear function of the input. And below, we have a formula for the error, which is the average of all points of the blue term for the blue points and the red term for the red points. And in order to descend from Mount Errorest, we calculate the gradient. And the gradient is simply the vector formed by all the partial derivatives of the error function with respect to the weights w1 up to wn and and the bias b. They correspond to these edges over here, and what do we do in a multilayer perceptron? Well, this time it's a little more complicated but it's pretty much the same thing. We have our prediction, which is simply a composition of functions namely matrix multiplications and sigmoids. And the error function is pretty much the same, except the  is a bit more complicated. And the gradient is pretty much the same thing, it's just much, much longer. It's a huge vector where each entry is a partial derivative of the error with respect to each of the weights. And these just correspond to all the edges. If we want to write this more formally, we recall that the prediction is a composition of sigmoids and matrix multiplications, where these are the matrices and the gradient is just going to be formed by all these partial derivatives. Here, it looks like a matrix but in reality, it's just a long vector. And the gradient descent is going to do the following; we take each weight, w_i_j super k and we update it by adding a small number, the learning rate times the partial derivative of E with respect to that same weight. This is the gradient descent step, so it will give us new updated weight w_i_j super k prime. That step is going to give us a whole new model with new weights that will classify the point much better.

### 34. Chain Rule-YAhIBOnbt54.en

So before we start calculating derivatives, let's do a refresher on the chain rule which is the main technique we'll use to calculate them. The chain rule says, if you have a variable x on a function f that you apply to x to get f of x, which we're gonna call A, and then another function g, which you apply to f of x to get g of f of x, which we're gonna call B, the chain rule says, if you want to find the partial derivative of B with respect to x, that's just a partial derivative of B with respect to A times the partial derivative of A with respect to x. So it literally says, when composing functions, that derivatives just multiply, and that's gonna be super useful for us because feed forwarding is literally composing a bunch of functions, and back propagation is literally taking the derivative at each piece, and since taking the derivative of a composition is the same as multiplying the partial derivatives, then all we're gonna do is multiply a bunch of partial derivatives to get what we want. Pretty simple, right?

### 34. DL 46 Calculating The Gradient 2 V2 (2)-7lidiTGIlN4.en

So, let us go back to our neural network with our weights and our input. And recall that the weights with superscript 1 belong to the first layer, and the weights with superscript 2 belong to the second layer. Also, recall that the bias is not called b anymore. Now, it is called W31, W32 etc. for convenience, so that we can have everything in matrix notation. And now what happens with the input? So, let us do the feedforward process. In the first layer, we take the input and multiply it by the weights and that gives us h1, which is a linear function of the input and the weights. Same thing with h2, given by this formula over here. Now, in the second layer, we would take this h1 and h2 and the new bias, apply the sigmoid function, and then apply a linear function to them by multiplying them by the weights and adding them to get a value of h. And finally, in the third layer, we just take a sigmoid function of h to get our prediction or probability between 0 and 1, which is . And we can read this in more condensed notation by saying that the matrix corresponding to the first layer is W superscript 1, the matrix corresponding to the second layer is W superscript 2, and then the prediction we had is just going to be the sigmoid of W superscript 2 combined with the sigmoid of W superscript 1 applied to the input x and that is feedforward. Now, we are going to develop backpropagation, which is precisely the reverse of feedforward. So, we are going to calculate the derivative of this error function with respect to each of the weights in the labels by using the chain rule. So, let us recall that our error function is this formula over here, which is a function of the prediction . But, since the prediction is a function of all the weights wij, then the error function can be seen as the function on all the wij. Therefore, the gradient is simply the vector formed by all the partial derivatives of the error function E with respect to each of the weights. So, let us calculate one of these derivatives. Let us calculate derivative of E with respect to W11 superscript 1. So, since the prediction is simply a composition of functions and by the chain rule, we know that the derivative with respect to this is the product of all the partial derivatives. In this case, the derivative E with respect to W11 is the derivative of either respect to  times the derivative  with respect to h times the derivative h with respect to h1 times the derivative h1 with respect to W11. This may seem complicated, but the fact that we can calculate a derivative of such a complicated composition function by just multiplying 4 partial derivatives is remarkable. Now, we have already calculated the first one, the derivative of E with respect to . And if you remember, we got  minus y. So, let us calculate the other ones. Let us zoom in a bit and look at just one piece of our multi-layer perceptron. The inputs are some values h1 and h2, which are values coming in from before. And once we apply the sigmoid and a linear function on h1 and h2 and 1 corresponding to the biased unit, we get a result h. So, now what is the derivative of h with respect to h1? Well, h is a sum of three things and only one of them contains h1. So, the second and the third summon just give us a derivative of 0. The first summon gives us W11 superscript 2 because that is a constant, and that times the derivative of the sigmoid function with respect to h1. This is something that we calculated below in the instructor comments, which is that the sigmoid function has a beautiful derivative, namely the derivative of sigmoid of h is precisely sigmoid of h times 1 minus sigmoid of h. Again, you can see this development underneath in the instructor comments. You also have the chance to code this in the quiz because at the end of the day, we just code these formulas and then use them forever, and that is it. That is how you train a neural network.



## Part 09-Module 01-Lesson 01_Intro to Computer Vision

### 01. Welcome to Computer Vision-GgA3_-MMT_I.en

One of the most fundamental ways we learn about our world and environment is through our sense of sight. Our ability to see permeates our very being. It has come to define, not only how we see the physical world, but also how we see the abstract. We speak of a company or an entrepreneur having a vision; we may say, "I see what you mean", to a colleague or send on the,"I see what you did there, Jeff". Our ability to see, our vision, in a very real sense determines how we interact with the world. Computer Vision strives to give a similar ability to a machine and if you want to build an agent that uses vision to extract, analyze and understand useful information then you're in the right place. We have Cezanne Camacho, a resident expert in the subject to help you learn about Computer Vision and its fascinating applications. Hey everyone. I'm Cezanne and I'm excited to guide you through some important Computer Vision topics. Computer Vision is a fascinating set of techniques, inspired by mathematics, statistics and probability theory and in this lesson we'll be learning about the basics of Computer Vision and about the role it plays in artificial intelligence systems. Let's get started.

### 02. 02. What Is Vision-_99V1rUNFa4.en

So, what is vision? At its most basic, vision or visual perception is the act of observing patterns and objects through sight or visual input. But vision is so much more than that. It allows us to build a model of the physical world. For example, take a look around you and think about what you see, maybe you see a computer screen or a mobile phone. You may also see a desk or different items in a room. When I look around the studio I see a camera in front of me, and tables and chairs besides me. We use site to learn about our physical surroundings and where they are in relation to us. And with this information, we build a physical model of our world. This is how typical human vision works, but different creatures have evolved vision systems that are tailored to their specific needs. Bees for instance have compound eyes which are many tiny eyes bundled together. Their eyes have really low resolution so they're not so great at recognizing things from a distance, but they're very sensitive to motion which is essential while flying fast. In essence, there are a variety of vision systems each with their own strengths. This is something to keep in mind when you start building and designing systems to perform specific visual tasks in future lessons

### 03. 03. Role In AI Render-xm1TXnNe5Pw.en

Some of you are probably wondering how vision fits into artificial intelligence. Why do AI systems need to see? Why do they need vision? Well, the basis of any AI system is that it can: one, perceive its environment and two, take actions based on those perceptions. And as we noted previously vision is a central part of how we perceive the world. One example of an AI system we're working on Udacity is a self-driving car. The car sees the world through cameras and sensors and then uses that input to safely navigate roads by itself. Computer vision is used to analyze camera images and intelligently identify objects like other cars or pedestrians. It's also used to recognize where the car is on the road and in the world based on the car's surroundings. So, self-driving cars and many other AI systems need vision to build a physical model of the world just like humans do to identify physical surroundings and respond to them.

### 04. 04. Computer Vision Applications-aFJKp2NltCY.en

When we think of computer vision's role in AI, we often focus on smart object and behavior recognition. We've already talked about the self-driving car example, in which computer vision is used to recognize cars and pedestrians and also recognize their behavior, like whether they're moving fast or slow or even moving erratically. But there's a plethora of other far-reaching uses that one wouldn't necessarily think of when talking about computer vision. It's used in state-of-the-art medical technology to analyze medical images and identify points of interest. An AI system can even learn to recognize cancerous tissue and help with early detection and diagnostics by analyzing data about cancer and images of cancer cells. On a more mundane level, many of us have phones that use computer vision on a daily basis. Computer vision is used to identify and categorize personal images and video. My phone recognizes me when I take a selfie and even knows that some of my friends faces look like. Some systems are smart enough to recognize behavior and images and even caption it correctly. So, AI systems can learn to recognize a multitude of objects and behaviors using computer vision and by analyzing masses of images or video data. We've already spoken of AI systems using computer vision in a similar manner to the human visual system to recognize visual surroundings, but there's another essential use that humans have for visual data. We use it to gauge the emotional state of our fellow humans. That is, vision not only let's us recognize objects, but when we're communicating with another person, we use vision to look at a person's face and body language to help us identify their mental and emotional states. As we'll see next, AI systems can learn to do this too.

### 05. 05. Emotional Intelligence-D_LzJsJH5qk.en

Today I'm with Dr. Rana el Kaliouby, Co-founder and CEO at Affectiva, a company that uses computer vision to build systems that are emotionally intelligent. Rana, I've been talking to our students about the role vision plays at a basic level in AI systems by helping to recognize objects and behavior but your work is in recognizing and responding to human emotion. What do you think makes this such important work? When we think of human intelligence, we tend to think of cognitive intelligence or IQ, but having emotional intelligence, which is the ability to empathize and read emotions, is central to how we all interact with the world. So the question becomes, how can we build systems that are artificially emotionally intelligent? Right. And this is the challenge we work on at Affectiva. We use computer vision and machine learning to recognize emotions as they manifest on your face. Can you tell what I'm feeling right now? Maybe a combination of surprise, a little bit of horror? Yeah, definitely happy surprise. So Affectiva software can recognize emotion just like you can? Yes, by observing your facial features, so how your eyebrows and your mouth and your cheeks move. And once a computer app, you know, or other application can recognize emotions, it can respond to them in real time. And this creates a lot of opportunity for improving and personalizing how we and machines interact with each other.

### 06. Vision-based Emotion AI-7nKKWWn1sAc.en

So Rana, at Affectiva you build software that reads emotion from facial expressions. I was wondering if you could tell us a bit more about your background and how you got involved in this work. Sure. So I studied computer science as an undergraduate at the American University in Cairo and I was really interested in how computers change how we connect and communicate with one another. And then I got an opportunity to study at Cambridge University for my doctorate degree and that was really my first living abroad experience, and I realized I was spending more hours with my laptop than I did with any other human. Yet it was completely oblivious to my mental and emotional state. So that got me thinking. What if that computer could read and respond to my emotions just the way an awesome friend would? I think when I think of emotional intelligence, I don't automatically link it with artificial intelligence. So I was also wondering if you could talk about some current applications of this technology. Yeah. Artificial emotional intelligence, or what we're calling emotion AI, is the core AI and it cuts across many many industries. I'll give you two examples. One is social robots. Social robots are expected to build a rapport with its users and understand and respond to social and emotional skills. Autonomous vehicles is another area where emotion AI plays a really critical role. Understanding the sentiment inside the car and also deciding when to relinquish control back to a driver. And is that driver paying attention? Is that driver even awake? That's exciting to hear. These are problems we're working on teaching at Udacity too. I have one last question for you. What kind of advice would you give to a student who might be wanting to pursue a career in AI? My advice would be, get hands-on and get building. Great. So let's keep learning.

### 07. 08. Computer Vision Pipeline-64hFcqhnNow.en

Let me walk you through a sequence of steps that you need to analyze facial expressions and emotions. Other computer vision tasks have different desired outputs and corresponding algorithms, but they use a similar overall pipeline. First off, a computer receives visual input from an imaging device like a camera. This is typically captured as a sequence of images or frames. Each frame is then sent through some preprocessing steps that enhance the quality and detail of the image. You may also perform other transformations here such as changing from color to grayscale. Next, to these images are analyzed and our software recognizes certain facial features of interest like the curve of the mouth and shape of the eyes, and then data about these features is fed into a so-called trained model that from previously known data can recognize patterns in these facial expressions and finally identify a certain emotion. It does so with a certain probability that's reported back. Finally, having recognized an emotion, an application can then act on this and interact with a human in a way that takes their emotional state into account.

### 09. 09. Training a Model-m4GVfwVkj74.en

So, I've just described a computer vision pipeline that takes in a sequence of images and through a series of steps can recognize different facial expressions and emotions. But it still seems kind of mysterious. Can you talk a bit about how exactly a model like this can be trained to recognize different facial expressions? Sure. The process is similar to the pipeline you just described. We have 45 facial muscles that drive thousands of different expressions on our face. But let's take a specific example. Let's say we are training an algorithm to discriminate between a smile and a smirk. We collect tens of thousands of examples of people smiling  the more diverse, the better  and then tens of thousands of examples of people smirking. We feed those prerecorded images along with their labels to the system. The algorithm then looks for visual differences between the two expressions. For instance, when you smile, your teeth might show, but that's not the case with a smirk. So, you give the model lots of examples of smiles and smirks and other facial expressions until it learns to recognize them. It sounds like how a baby learns, by lots of examples. Exactly. And similar to how humans learn, at the beginning of the training phase, the model typically performs very badly, but then it monitors the errors it makes and uses those to improve the performance each time it sees more images. After many iterations, the model converges on the right set of parameters once the error rate becomes acceptable, and that's when we consider the model to be fully trained. Now, this is a very high-level view of how to train any machine learning model, and the details will vary based on the type of model you use and the training algorithm you choose. For instance, you can use a convolutional neural network trained using gradient descent. Next, let's see how this computer vision pipeline works in a real-time application.

### 10. AffdexMe Demo-dpFtXDqakvY.en

The best way to understand how emotion AI works is by example. Would you like to see a live demo? Sure. Why not? Everyone wants their computers to understand them better. All right, so here's the demo. Basically, what's happening here is that the algorithm is looking for faces and it's detecting your face by, you know, drawing this bounding box around your different facial features. And then it's tracking the movement of your facial features, like your eyebrows, your mouth, your nose over time. That's what these dots are for? Exactly, and it's mapping it into a probability score for each emotion. So let's give this a try. For instance, yeah, let's try smiling. Smile. We can see that the probability score of the joy classifier goes up. Let's try sadness. It's also mapping your most dominant emotion into an emoji, so you can experiment with, you know, eliciting a bunch of different emojis. Yeah, that's interesting. It also detects the presence of glasses and gender. So it's identified us both as female. And looks quite a lot like me too. Well, I'm excited for our students to try out this technology.

### 11. Emotion as a Service-2jAP3rP3USM.en

Thank you for sharing this with us. It sounds like you and your team have put in an incredible amount of effort to build this service. I'm wondering if we can use this to build apps of our own. Absolutely. At Affectiva we've made it really easy for you to incorporate emotion AI into your own applications and projects through a variety of STKs and APIs. Great. This way we can include emotion recognition software in a variety of applications. Next, it will be up to our students to get some practice with computer vision by using the software in a project of their own.



## Part 10-Module 01-Lesson 01_Intro to NLP

### 01. Intro Arpan-MW5MWOLj064.en

Our first section will be taught by Arpan Chakraborty. Arpan has a Ph.D. in Computer Science and for several years, has taught at Udacity and at Georgia Tech. Hi Arpan. Hi Luis. Hello everyone. I'm glad to get you started on your journey through Natural Language Processing. Everything in NLP starts with raw text typically, produced by humans like you and me, and Luis here. This text is first processed using some simple transformations such as, splitting it into individual words, reducing verbs to their root form, et cetera. You need to do this before performing any other analysis or training complex models. This stage may sound simple but you have to be careful about how you process your raw text. It may affect the results you obtain further down the line. Ready? Let's do it.

### 02. Welcome to NLP-g-AlFF61p0I.en

Welcome to Natural Language Processing. Language is an important medium for human communication. It allows us to convey information, express our ideas, and give instructions to others. Some philosophers argue that it enables us to form complex thoughts and reason about them. It may turn out to be a critical component of human intelligence. Now consider the various artificial systems we interact with every day, phones, cars, websites, coffee machines. It's natural to expect them to be able to process and understand human language, right? Yet, computers are still lagging behind. No doubt, we have made some incredible progress in the field of natural language processing, but there is still a long way to go. And that's what makes this an exciting and dynamic area of study. In this lesson you will not only get to know more about the applications and challenges in NLP, you will learn how to design an intelligent application that uses NLP techniques and deploy it on a scalable platform. Sounds fun? Let's get started.

### 03. Structured Languages-NsmqUIHlk6U.en

What makes it so hard for computers to understand us? One drawback of human languages, or feature depending on how you look at it, is the lack of a precisely defined structure. To understand how that makes things difficult let's first take a look at some languages that are more structured. Mathematics, for instance, uses a structured language. When I write y equals 2x plus 5 there is no ambiguity in what I want to convey. I'm saying that the variable y is related to the variable x as two times x plus five. Formal logic also uses a structure language. For example, consider the expression parent(x,y) and parent(x,z) implies sibling(y, z). This statement is asserting that if x is a parent of y and x is a parent of z, then y and z are siblings. A set of structure languages that may be more familiar to you are scripting and programming languages. Consider this SQL statement. SELECT name, email FROM users WHERE name LIKE A%. We are asking the database to return the names and e-mail addresses of all users whose names begin with an A. These languages are designed to be as unambiguous as possible and are suitable for computers to process.

### 04. Grammar-Jw3dA7xmoQ4.en

Structured languages are easy to parse and understand for computers because they are defined by a strict set of rules or grammar. There are standard forms of expressing such grammars and algorithms, that can parse properly formed statements to understand exactly what is meant. When a statement doesn't match the prescribed grammar, a typical computer doesn't try to guess the meaning, it simply gives up. Such violations of grammatical rules are reported as syntax errors.

### 05. Unstructured Text-OmwSdaec5vU.en

The languages we use to communicate with each other also have defined grammatical rules. And indeed, in some situations we use simple structured sentences but for the most part human discourse is complex and unstructured. Despite that, we seem to be really good at understanding each other and even ambiguities are welcome to a certain extent. So, what can computers do to make sense of unstructured text? Here are some preliminary ideas. Computers can do some level of processing with words and phrases, trying to identify key words, parts of speech, named entities, dates, quantities, etc. Using this information they can also try to parse sentences, at least ones that are relatively more structured. This can help extract the relevant parts of statements, questions, or instructions. At a higher level computers can analyze documents to find frequent and rare words, assess the overall tone or sentiment being expressed, and even cluster or group similar documents together. You can imagine that building on top of these ideas, computers can do a whole lot with unstructured text even if they cannot understand it like us.

### 07. Context-J-4pfu2w1C0.en

So what is stopping computers from becoming as capable as humans in understanding natural language? Part of the problem lies in the variability and complexity of our sentences. Consider this excerpt from a movie review. "I was lured to see this on the promise of a smart witty slice of old fashioned fun and intrigue. I was conned. " Although it starts with some potentially positive words it turns out to be a strongly negative review. Sentences like this might be somewhat entertaining for us but computers tend to make mistakes when trying to analyze them. But there is a bigger challenge that makes NLP harder than you think. Take a look at this sentence. "The sofa didn't fit through the door because it was too narrow." What does "it" refer to? Clearly "it" refers to the door. Now consider a slight variation of this sentence. "The sofa didn't fit through the door because it was too wide." What does "it" refer to in this case? Here it's the sofa. Think about it. To understand the proper meaning or semantics of the sentence you implicitly applied your knowledge about the physical world, that wide things don't fit through narrow things. You may have experienced a similar situation before. You can imagine that there are countless other scenarios in which some knowledge or context is indispensable for correctly understanding what is being said.

### 08. Natural Language Processing-UQBxJzoCp-I.en

Natural language processing is one of the fastest growing fields in the world. NLP Is making its way into a number of products and services that we use every day. Let's begin with an overview of how to design an end-to-end NLP pipeline. Not that kind of pipeline; a natural language processing pipeline, where you start with raw text, in whatever form it is available, process it, extract relevant features, and build models to accomplish various NLP tasks. Now that I think about it, that is kind of like refining crude oil. Anyways, you'll learn how these different stages in the pipeline depend on each other. You'll also learn how to make design decisions, how to choose existing libraries, and tools to perform each step.

### 09. NLP M1-L1 01 NLP Pipeline-vJx6oKlu_MM.en

Let's look at a common NLP pipeline. It consists of three stages, text processing, feature extraction and modeling. Each stage transforms text in some way and produces a result that the next stage needs. For example, the goal of text processing is to take raw input text, clean it, normalize it, and convert it into a form that is suitable for feature extraction. Similarly, the next stage needs to extract and produce feature representations that are appropriate for that type of model you're planning to use and the NLP task you're trying to accomplish. When you're building such a pipeline, your workflow may not be perfectly linear. Let's say, you spend some time implementing text processing functions, then make some simple feature extractors, and then design a baseline statistical model. But then, maybe you are not happy with the results. So you go back and rethink what features you need, and that in turn, can make you change your processing routines. Keep in mind that this is a very simplified view of natural language processing. Your application may require additional steps.

### 10. Text Processing-pqheVyctkNQ.en

Let's take a closer look at text processing. The first question that comes to mind is, why do we need to process text? Why can we not feed it in directly? To understand that, think about where we get this text to begin with. Websites are a common source of textual information. Here's a portion of a sample web page from Wikipedia and the corresponding HTML markup, which serves as our raw input. For the purpose of natural language processing, you would typically want to get rid of all or most of the HTML tags, and retain only plain text. You can also remove or set aside any URLs or other items not relevant to your task. The Web is probably the most common and fastest growing source of textual content. But you may also need to consume PDFs, Word documents or other file formats. Or your raw input may even come from a speech recognition system or from a book scan using OCR. Some knowledge of the source medium can help you properly handle the input. In the end, your goal is to extract plain text that is free of any source specific markers or constructs that are not relevant to your task. Once you have obtained plain text, some further processing may be necessary. For instance, capitalization doesn't usually change the meaning of a word. We can convert all the words to the same case so that they're not treated differently. Punctuation marks that we use to indicate pauses, etc. can also be removed. Some common words in a language often help provide structure, but don't add much meaning. For example, a, and, the, of, are, and so on. Sometimes it's best to remove them if that helps reduce the complexity of the procedures you want to apply later.

### 11. Feature Extraction-UgENzCmfFWE.en

Okay. We now have clean normalized text. Can we feed this into a statistical or a machine learning model? Not quite. Let's see why. Text data is represented on modern computers using an encoding such as ASCII or Unicode that maps every character to a number. Computer store and transmit these values as binary, zeros and ones. These numbers also have an implicit ordering. 65 is less than 66 which is less than 67. But does that mean A is less than B, and B is less and C? No. In fact, that would be an incorrect assumption to make and might mislead our natural language processing algorithms. Moreover, individual characters don't carry much meaning at all. It is words that we should be concerned with, but computers don't have a standard representation for words. Yes, internally they are just sequences of ASCII or Unicode values but they don't quite capture the meanings or relationships between words. Compare this with how an image is represented in computer memory. Each pixel value contains the relative intensity of light at that spot in the image. For a color image, we keep one value per primary color; red, green, and blue. These values carry relevant information. Two pixels with similar values are perceptually similar. Therefore, it makes sense to directly use pixel values in a numerical model. Yes, some feature engineering may be necessary such as edge detection or filtering, but pixels are a good starting point. So the question is, how do we come up with a similar representation for text data that we can use as features for modeling? The answer again depends on what kind of model you're using and what task you're trying to accomplish. If you want to use a graph based model to extract insights, you may want to represent your words as symbolic nodes with relationships between them like WordNet. For statistical models however, you need some sort of numerical representation. Even then, you have to think about the end goal. If you're trying to perform a document level task, such as spam detection or sentiment analysis, you may want to use a per document representations such as bag-of-words or doc2vec. If you want to work with individual words and phrases such as for text generation or machine translation, you'll need a word level representation such as word2vec or glove. There are many ways of representing textual information, and it is only through practice that you can learn what you need for each problem.

### 12. Modeling-P4w_2rkxBvE.en

The final stage in this process is what I like to call modeling. This includes designing a model, usually a statistical or a machine learning model, fitting its parameters to training data using an optimization procedure, and then using it to make predictions about unseen data. The nice thing about working with numerical features is that it allows you to utilize pretty much any machine learning model. This includes support vector machines, decision trees, neural networks, or any custom model of your choice. You could even combine multiple models to get better performance. How you utilize the model is up to you. You can deploy it as a web-based application, package it up into a handy mobile app, integrate it with other products, services, and so on. The possibilities are endless.